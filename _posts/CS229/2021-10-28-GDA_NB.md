---
title: Generative Learning algorithms
author: raojiyong
date: 2021-10-27 22:30:00 +0800
categories: ["2021","10"]
tags: [notes,ml]
math: true
mermaid: true
---

## Generative Learning algorithms

**discriminative** learning algorithms: Algorithms that try to learn $p(y\vert x)$ directly (such as logistic regression) or algorithms that try to learn mappings directly from the space of inputs $\mathcal{X}$ to the labels {0,1} (such as perceptron).

**generative** learning algorithms: instead try to model $p(x\vert y)$ (and $p(y)$). For instance, if $y$ indicates whether an example is a dog (0) and an elephant (1), then $p(x\vert y=0)$ models the distribution of dogs' features, and $p(x\vert y=1)$ models the distribution of elephants' features.

After modeling $p(y)$ (called the **class prior**) and $p(x\vert y)$ (called class-conditional probability / **likelihood**), our algorithm can then use Bayes rule to derive the posterior distribution on $y$ given $x$:


$$
p(y\vert x)=\frac{p(x\vert y)p(y)}{p(x)}
$$
Here, the denominator is given by $p(x)=p(x\vert y=1)p(y=1)+p(x\vert y=0)p(y=0)$. Actually, if were calculating $p(y\vert x)$ in order to make a prediction, then we don't actually need to calculate the denominator, since


$$
\begin{align}
\arg\max_y p(y\vert x)&=\arg\max_y\frac{p(x\vert y)p(y)}{p(x)}\\&=\arg\max_y p(x\vert y)p(y)\\
\end{align}
$$



### 1. Gaussian discriminant analysis

GDA. We assume that $p(x\vert y)$ is distributed according to a multivariate normal distribution.

#### 1.1 The multivariate normal distribution

In d-dimensions, also called the multivariate Gaussian distribution, is parameterized by a **mean vector** $\mu\in\mathbb{R}^d$ and a **covariance matrix** $\Sigma\in\mathbb{R}^{d\times d}$, where $\Sigma\ge 0$ is symmetric and positive semi-definite对称半正定 . Also written "$\mathcal{N}(\mu,\Sigma)$", its density is given by:


$$
p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}\vert \Sigma\vert^\frac12}\exp(-\frac12(x-\mu)^T\Sigma^{-1}(x-\mu))
$$


For a random variable $X\in\mathcal{N}(\mu,\Sigma)$, the mean is $\mu$ , the covariate matrix is $\Sigma$.

#### 1.2 The GDA model

The classification problem in which the input features $x$ are continuous-valued random variables, we can then use GDA model, which model $p(x\vert y)$ using a multivariate normal distribution. The model is:


$$
\begin{align}y&\sim\text{Bernoulli}(\phi)\\x\vert y=0&\sim\mathcal{N}(\mu_0,\Sigma)\\x\vert y=1&\sim\mathcal{N}(\mu_1,\Sigma)\end{align}
$$


也就是


$$
\begin{align}p(y)&=\phi^y(1-\phi)^{1-y}\\p(x\vert y=0)&=\frac{1}{(2\pi)^{d/2}\vert \Sigma\vert^\frac12}\exp(-\frac12(x-\mu_0)^T\Sigma^{-1}(x-\mu_0))\\p(x\vert y=1)&=\frac{1}{(2\pi)^{d/2}\vert \Sigma\vert^\frac12}\exp(-\frac12(x-\mu_1)^T\Sigma^{-1}(x-\mu_1))\end{align}
$$


连乘操作容易造成下溢，通常使用对数似然


$$
\begin{align}\ell&=\log\prod_{i=1}^np(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\&=\log\prod_{i=1}^np(x^{(i)}\vert y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)\\&=\sum_{i=1}^n-\frac d2\log{2\pi}-\frac12\log{\vert \Sigma\vert}-\frac12(x-\mu)^T\Sigma^{-1}(x-\mu)+y\log\phi+(1-y)\log{(1-\phi)}\end{align}
$$


最大化对数似然估计，求矩阵偏导=0，得：


$$
\begin{align}\phi&=\frac1n\sum_{i=1}^n1\lbrace{y^{(i)}=1}\rbrace\\\mu_0&=\frac{\sum_{i=1}^n1\lbrace{y^{(i)}=0}\rbrace^{(i)}}{\sum_{i=1}^n1\lbrace{y^{(i)}=0}\rbrace}\\\mu_1&=\frac{\sum_{i=1}^n1\lbrace{y^{(i)}=1} \rbrace x^{(i)}}{\sum_{i=1}^n1\lbrace{y^{(i)}=1}\rbrace}\\\Sigma&=\frac1n\sum_{i=1}^n(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T\end{align}
$$


参数已经得到，判断新样本 x, 求得$p(y=0\vert x)、p(y=1\vert x)$ 取概率值大的那一类。

GDA 的分界面



$(1-\phi)\exp((x-\mu_0)^T\Sigma^{-1}(x-\mu_0))=\phi\exp((x-\mu_1)^T\Sigma^{-1}(x-\mu_1))$



化简可得



$2x^T\Sigma^{-1}(\mu_1-\mu_0)=\mu_1^T\Sigma^{-1}\mu_1-\mu_0^T\Sigma^{-1}\mu_0+\log\phi-\log(1-\phi)$



#### 1.3 GDA and logistic regression

可以将 $p(y\vert x)$ 表示成


$$
p(y=1\vert x;\phi,\Sigma,\mu_0,\mu_1)=\frac{1}{1+\exp(-\theta^Tx)}
$$


where $\theta$ is some appropriate function of $\phi,\Sigma,\mu_0,\mu_1$.

We just argued that if $p(x\vert y)$ is multivariate gaussian (with shared $\Sigma$), then $p(y\vert x)$ necessarily follows a logistic function. The converse is false. This shows that GDA makes *stronger* modeling assumption, and is more data efficient when assumption are correct.

### 2. Naive Bayes

离散特征

Naive Bayes assumption: $x_i's$ are conditionally independent given $y$. 简单来说就是，给定条件 $y$ , $x_i$ 之间互相独立


$$
\begin{align}
p(x_1,\dots,x_n\vert y)&=p(x_1\vert y)p(x_2\vert y,x_1)\cdots p(x_n\vert y,x_1,x_2,\dots,x_{n-1})\\&=p(x_1\vert y)p(x_2\vert y)\cdots p(x_n\vert y)\\&=\prod_{j=1}^np(x_j\vert y)\\
\end{align}
$$



Model is parameterized by $\phi_{j\vert y=1}=p(x_j=1\vert y=1),\phi_{j\vert y=0}=p(x_j=1\vert y=0)$ and $\phi_y=p(y=1)$. As usual, given a training set $\lbrace(x^{(i)},y^{(i)});i=1,\dots,n\rbrace$ , where each $x^{(i)}$ is a vector, and each $y^{(i)}$ is in $\{0,1\}$.We assume that each vector x is in the set $\lbrace0,1\rbrace^d$ for some integer $d$ specifying the number of "feature" in the model. The joint likelihood of the data:


$$
\mathcal{L}(\phi_y,\phi_{j\vert y=0},\phi_{j\vert y=1})=\prod_{i=1}^np(x^{(i)},y^{(i)})
$$


log-likelihood function is


$$
\begin{align}
\ell&=\sum_{i=1}^n\log p(x^{(i)},y^{(i)})\\&=\sum_{i=1}^n\log\Bigl(\phi_{y^{(i)}}\prod_{j=1}^d\phi_{x_j^{^{(i)}}\vert y^{(i)}}\Bigr)\\&=\sum_{i=1}^n\log\phi_{y^{(i)}}+\sum_{i=1}^n\sum_{j=1}^d\log\phi_{x_j^{(i)}\vert y^{(i)}}
\end{align}
$$



maximizing this w.r.t. $\phi_y,\phi_{j\vert y=0},\phi_{j\vert y=1}$ gives the maximum likelihood estimates:


$$
\begin{align}
\phi_{j\vert y=1}&=\frac{\sum_{i=1}^n1\lbrace x_j^{(i)}=1\land y^{(i)}=1\rbrace}{\sum_{i=1}^n1\lbrace y^{(i)}=1\rbrace}\\\phi_{j\vert y=1}&=\frac{\sum_{i=1}^n1\lbrace x_j^{(i)}=1\land y^{(i)}=0\rbrace}{\sum_{i=1}^n1\lbrace y^{(i)}=0\rbrace}\\\phi_y&=\frac{\sum_{i=1}^n1\lbrace y^{(i)}=1\rbrace}{n}
\end{align}
$$


For interpretation,$\phi_y$ can be interpreted as the probability of seeing the label $y$. The value of  $\phi_{j\vert y=1}$ is just the fraction of the spam (y=1) emails in which word j does appear.

To make a prediction on a anew example with features $x$, we then simply calculate


$$
\begin{align}
p(y=1\vert x)&=\frac{p(x\vert y=1)p(y=1)}{p(x)}\\&=\frac{\Bigl(\prod_{j=1}^np(x_j\vert y=1)\Bigr)p(y=1)}{\Bigl(\prod_{j=1}^np(x_j\vert y=1)\Bigr)p(y=1)+\Bigl(\prod_{j=1}^np(x_j\vert y=0)\Bigr)p(y=0)}\\
\end{align}
$$


and pick whichever class has the higher posterior probability.

#### Laplace smoothing

multinomial random variable $z$ taking values in $\{1,\dots,k\}$.


$$
\phi_j=\frac{1+\sum_{i=1}^n1\lbrace z^{(i)}=j\rbrace}{k+n}
$$

---

<center class="half">
    <div>
    	<img src="https://raojiyong.github.io/imgs/petal_length.png" width="300"/>
    	<img src="https://raojiyong.github.io/imgs/petal_width.png" width="300"/>
    	<img src="https://raojiyong.github.io/imgs/sepal-length.png" width="300"/>
      <img src="https://raojiyong.github.io/imgs/sepal-length.png" width="300"/>
    </div>
    <br>
</center>




---



