---
title: Video
author: raojiyong
date: 2021-12-09 15:00:00 +0800
categories: ["2021","12"]
tags: [notes, video, tracking]
math: true
mermaid: true
---

## Video

Video = 2D + Time

A video is a **sequence** of images

4D tensor: $T\times3\times H\times W$

### Training on Clips

- **Raw video**: Long, high FPS
- **Training:** Train model to classify short **clips** with low FPS
- **Testing:** Run model on different clips, average predictions.

### Video classification: 

#### Single-Frame CNN

Simple idea: train normal 2D CNN to classify video frame independently. 

(Average predicted probs at test-time)

Often a **very** strong baseline for video classification.

#### Late Fusion

- with FC layers

  - **Intuition:** Get high-level appearance of each frame, and combine them.

  - Clip features: TDH'W'

- with pooling

  - The same intuition.

  - Clip features: D
  - **Problem:** Hard to compare low-level motion between frames.

#### Early Fusion

- **Intuition**: Compare frames with very first conv layer, after that normal 2D CNN.
- Input: $T\times3\times H\times W$ Reshape: $3T\times H\times W$. That means it concats all fo the frames along the cahnnel dimension, then fuse all of this temporal information using a single two-dimensional convolution operations.
- First 2D convolution collapses all temporal information: **Input:** $3T\times H\times W$, **Output:** $D\times H\times W$.
- Rest of the  network is standard 2D CNN.
- **Problem:** One layer of temporal processing may not be enough.

#### 3D CNN

- **Intuition**: Use 3D versions of convolution and pooling to slowly fuse temporal information over the course of the network.
- Each layer in the network is a 4D tensor: $D\times T\times H\times W$. Use 3D conv and 3D pooling operations.

#### Early Fusion vs Late Fusion vs 3D CNN

<center class="half">
    <div>
    	<img src="https://raojiyong.github.io/imgs/lateFusion.png" width="300"/>
    	<img src="https://raojiyong.github.io/imgs/FusionComp.png" width="300"/>
    </div>
    <br>
</center>
space information and temporal information are interchangeable.

next level:

different between processing motion and processing visual appearance

- measuring motion: optical flow
  - flow field and distortion field
  - for every pixel in the first frame, what is the vector displacement telling where that pixel is going to move in the second frame.

next level: because all of operations we've seen are very local

early fusion doesn't work well, 3D convolution is only looking at a tiny receptive field in time, optical flow is only looking between adjcent frames to compute the motion between a pair of frames.



modeling long-term temporal structure

