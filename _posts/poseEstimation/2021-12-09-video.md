---
title: Video
author: raojiyong
date: 2021-12-09 15:00:00 +0800
categories: ["2021","12"]
tags: [notes, video, tracking]
math: true
mermaid: true
---

## Video

Video = 2D + Time

A video is a **sequence** of images

4D tensor: $T\times3\times H\times W$

### Training on Clips

- **Raw video**: Long, high FPS
- **Training:** Train model to classify short **clips** with low FPS
- **Testing:** Run model on different clips, average predictions.

### Video classification: 

#### Single-Frame CNN

Simple idea: train normal 2D CNN to classify video frame independently. 

(Average predicted probs at test-time)

Often a **very** strong baseline for video classification.

#### Late Fusion

- with FC layers

  - **Intuition:** Get high-level appearance of each frame, and combine them.

  - Clip features: TDH'W'

- with pooling

  - The same intuition.

  - Clip features: D
  - **Problem:** Hard to compare low-level motion between frames.

#### Early Fusion

- **Intuition**: Compare frames with very first conv layer, after that normal 2D CNN.
- Input: $T\times3\times H\times W$ Reshape: $3T\times H\times W$
- First 2D convolution collapses all temporal information: **Input:** $3T\times H\times W$, **Output:** $D\times H\times W$.
- Rest of the  network is standard 2D CNN.
- **Problem:** One layer of temporal processing may not be enough.

#### 3D CNN

- **Intuition**: Use 3D versions of convolution and pooling to slowly fuse temporal information over the course of the network.
- Each layer in the network is a 4D tensor: $D\times T\times H\times W$. Use 3D conv and 3D pooling operations.

#### Early Fsuin vs Late Fusion vs 3D CNN

```html
<center class="half">
    <div>
    	<img src="https://raojiyong.github.io/imgs/lateFusion.png" width="300"/>
    	<img src="https://raojiyong.github.io/imgs/FusionComp.png" width="300"/>
    </div>
    <br>
</center>
```
