---
title: ML Bayes
author: raojiyong
date: 2021-07-25 15:00:00 +0800
categories: ["2021","07"]
tags: [notes,Machine Learning]
math: true
mermaid: true
---

### Bayesian decision theory

|                          | 定义                                                  | 最小化分类错误率               |
| ------------------------ | ----------------------------------------------------- | ------------------------------ |
| loss                     | $\lambda_{ij}$                                        | $[i=j]$                        |
| Expected loss            | $R(c_i\vert x)=\sum_{j=1}^N\lambda_{ij}P(c_j\vert x)$ | $1-P(c\vert x)$                |
| Bayes optimal classifier | $h^\ast(x)=\arg\min_{c\in Y}R(c\vert x)$              | $\arg\max_{c\in Y}P(c\vert x)$ |
| Decision loss            | $R(h)=E_x(R(h(x)\vert x))$                            | $P(h^\ast(x)\vert x)$          |
| Bayes risk               | $1-R(h^\ast)$                                         | $1-P(h^\ast(x)\vert x)$        |

### 贝叶斯定理

$$
P(c\vert x)=\frac{P(x,c)}{P(x)}=\frac{P(c)P(x\vert c)}{\int p(c)P(x\vert c)dc}
$$

- 先验 prior: $P(c)$
- evidence: $P(x)$
- 类条件概率class-conditional probablity\likelihood: $P(x\vert c)$
  - class-conditional probability: $x$
  - likelihood: $\theta,P(x\vert c)(\theta)$
    - $P(D_c\vert\theta_c)=\prod_{x\in D_c}P(x\vert\theta_c)$
    - $LL(\theta_c)=logP(D_c\vert\theta_c)=\sum_{x\in D_c}logP(x\vert\theta_c)$

### k近邻学习

- lazy learning
- 最邻近分类器的泛化错误率不会超过贝叶斯最优分类器错误率的两倍

### 朴素贝叶斯分类器

- 属性条件独立性假设：$P(x\vert c)=\prod_{i=1}^dP(x_i\vert c)$
- $h_{nb}=\arg\max_{c\in Y}P(c)\prod_{i=1}^dP(x_i\vert c)$
  - $P(c)=\frac{\vert D_c\vert}{\vert D\vert}$
  - $P(x_i\vert c)=\frac{\vert D_{c,x_i}\vert}{\vert D_c\vert}$
- 拉普拉斯修正
  - $\hat P(c)=\frac{\vert D\vert+1}{\vert D\vert+N}$
    - N 为 D 中可能的类别
  - $\hat P(x_i\vert c)=\frac{\vert D_{c,x_i}\vert+1}{\vert D_c\vert+N_i}$
    - $N_i$ 为第 i 个属性可能取值数
- 连续属性
  - $p(x_i\vert c)\sim N(\mu_{c,i},\sigma^2_{c,i})$

### 半朴素贝叶斯分类器

#### 独依赖估计 (One-Dependent Estimator, ODE)

- $P(c\vert x)\propto P(c)\prod_{i=1}^dP(x_i\vert c,p(a_i))$

SPODE (super-parent ODE)

- 假设所有属性都依赖于同一属性: $p(a_i)=x_t$

AODE (Averaged One-Dependent Estimator)

- SPODE 的集成
- $P(c\vert x)\propto\sum_{i=1,\vert D_{x_i}\vert\ge m'}^dP(c,x_i)\prod_{j=1}^dP(x_j\vert c,x_i)$

TAN (Tree Augmented naive Bayes)

- 仅保留了强相关属性间的依赖性

- 基于最大带权生成树

- 算法

  - conditional mutual information:

    $I(x_i,x_j\vert y)=\sum_{x_i,x_j,c}log\frac{P(x_i,x_j\vert c)}{P(x_i\vert c)P(x_j\vert c)}$

    - 在已知类别情况下的相关性

  - 在以属性为节点，互信息为边建完全图上构造最大带权生成树，挑选根节点，边置为有向

  - 加入类别节点 y,增加 y 到每个属性的边

kDE (k-Dependent Estimator)

- 样本不足：高阶连个概率估计困难，需要的样本数指数级增加