[ { "title": "矩阵复习笔记", "url": "/posts/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/", "categories": "2021, 12", "tags": "notes, matrix theory, course, jnu", "date": "2021-12-28 13:00:00 +0800", "snippet": "修改时间：2021.12.281. 线性空间与线性变换（1）线性空间的定义： 以 $\\alpha, \\beta, \\gamma,\\dots\\text{为元素的非空集合}V, \\text{数域}F$ 定义两种运算：==加法== $\\forall \\alpha , \\beta \\in V, \\; \\alpha + \\beta \\in V$；==数乘== $\\forall k \\in F, \\alpha \\in V, k \\alpha \\in V$。 满足8条：加法交换律、加法结合律、数乘结合律、两个分配律，0元存在，1元存在，负元存在。称 $V$为数域$F$上的线性空间。（2）证明一组向量是线性空间的基，两步走： 证明这组向量线性无关； 证明线性空间任意向量可由这组向量表示。（3）如果\\(\\{E_{ij}, i=1,2,...,m;j=1,2,...,n\\}\\)是矩阵空间$R^{m \\times n}$的一组基，则$\\dim R^{m \\times n} = m \\times n$。 注：这里有前提条件，实际上$\\dim R^{m \\times n}$并不是总等于$m \\times n$，如2015年填空题第2题。（4）$\\alpha_1, \\alpha_2, …, \\alpha_n$是线性空间$V_n(F)$的一组基，对于$\\forall \\beta \\in V$,\\[\\beta = (\\alpha_1 \\; \\alpha_2 \\; ... \\; \\alpha_n)\\begin{bmatrix}x_1\\\\ x_2\\\\ ...\\\\ x_n\\end{bmatrix}= (\\alpha_1 \\; \\alpha_2 \\; ... \\; \\alpha_n)X\\]其中$X$称为向量$\\beta$在基$(\\alpha_1 \\; \\alpha_2 \\; … \\; \\alpha_n)$下对应的坐标。 $V_n(F)$中向量组${\\beta_1 \\; \\beta_2 \\; … \\; \\beta_m}$线性相关的充要条件是坐标向量组${X_1,X_2,…,X_m}$是线性相关组。（5）设$(\\alpha_1 \\; \\alpha_2 \\; … \\; \\alpha_n)$和$(\\beta_1 \\; \\beta_2 \\; … \\; \\beta_n)$是$n$维线性空间$V_n(F)$中的两组基，则有$C\\in F^{m \\times n}$\\((\\beta_1 \\; \\beta_2 \\; ... \\; \\beta_n) = (\\alpha_1 \\; \\alpha_2 \\; ... \\; \\alpha_n)C\\)其中$C$称为从基设$(\\alpha_1 \\; \\alpha_2 \\; … \\; \\alpha_n)$到$(\\beta_1 \\; \\beta_2 \\; … \\; \\beta_n)$的过渡矩阵。 重要推论：如果向量$\\alpha \\in V_n(F)$，$\\alpha$在两组基下对应坐标分别是$X$和$Y$，则有：\\[\\alpha = (\\alpha_1 \\; \\alpha_2 \\; ... \\; \\alpha_n)X\\]\\[\\alpha = (\\beta_1 \\; \\beta_2 \\; ... \\; \\beta_n)Y\\] 显然有：$\\color{red}{X = CY}$。（6）设$W$是线性空间$V_n(F)$的非空子集合，则$W$是$V_n(F)$的子空间的充要条件是： 若$\\alpha, \\beta \\in W$，则$\\alpha + \\beta \\in W$ 若$\\alpha \\in W, \\; k \\in F$，则$k \\alpha \\in W$也就是说只需要验证对加法和数乘封闭即可。（7）设$W_1, \\; W_2$是线性空间$V$的子空间，则： $W_1$与$W_2$的交空间为：\\(W_1 \\cap W_2 =\\{\\alpha \\vert \\alpha \\in W_1 \\; and \\; \\alpha \\in W_2 \\}\\) $W_1$与$W_2$的**和空间**为：\\(W_1 + W_2 =\\{\\alpha \\vert \\alpha = \\alpha_1 + \\alpha_2, \\; \\alpha_1 \\in W_1, \\; \\alpha_2 \\in W_2 \\}\\) 两个重要的维数公式 $\\dim (W_1 \\cap W_2) \\leqslant \\dim W_i \\leqslant \\dim(W_1 + W_2) \\leqslant \\dim V$ $\\dim W_1 + \\dim W_2 = \\dim (W_1 + W_2) + \\dim(W_1 \\cap W_2)$ 直和子空间：如果$W = W_1 + W_2$，并且$W_1 \\cap W_2 = {0}$，那么称$W$是$W_1$与$W_2$的直和子空间，表示为$W = W_1 \\oplus W_2$。 直和补子空间：对$n$维空间$V$的任何子空间$W$，设$\\alpha_1, …,\\alpha_r$为$W$的基，$r &amp;lt; n $，把它们扩充为$V$的基${\\alpha_1, …,\\alpha_r; \\beta_{r+1}, …, \\beta_n}, \\quad U = L{\\beta_{r+1}, …, \\beta_n }$，有$V = W \\oplus U$成立，则称$U$是$W$的直和补子空间。（8）若$(\\alpha_1 \\; \\alpha_2 \\; … \\; \\alpha_n)$是线性空间$V_n(F)$的一组基，则\\(V_n(F) = L \\{\\alpha_1, \\alpha_2, .., \\alpha_n\\}\\) 对一个矩阵$A \\in F^{m \\times n}$，可以得到两个与$A$相关的子空间：\\(N(A) = \\{X\\vert AX=0 \\} \\subseteq F^n\\)\\[R(A) = L\\{A_1, A_2, ...,A_n \\} \\subseteq F^m\\]其中$N(A)$称为矩阵$A$的零空间，$R(A)$称为矩阵$A$的列空间。（9）内积： 欧氏空间的内积：$(\\alpha, \\beta) = \\alpha^T \\beta ; \\quad (A, B) = tr(AB^T)$ 酉空间的内积：$(\\alpha, \\beta) = \\beta^H \\alpha ; \\quad (A, B) = tr(B^HA)$ 柯西不等式：$\\vert(\\alpha, \\beta)\\vert^2 \\leqslant (\\alpha, \\alpha)(\\beta, \\beta)$ 正交补子空间：设$U$为内积空间$V_n(F)$的一个子空间，定义$V_n(F)$上的一个子集$U^{\\perp} = {\\alpha \\;\\vert \\;\\alpha \\in V_n(F), \\; \\forall \\beta \\in U, \\; (\\alpha, \\beta)=0 }$称为$U$的正交补子空间，有$V_n(F) = U \\oplus U^{\\perp}$。（10）设$T$是线性空间$V_n(F)$上的线性变换，则满足$T(k_1 \\alpha_1 + k_2 \\alpha_2) = k_1 T(\\alpha_1) + k_2 T(\\alpha_2)$，则有： 像空间：$R(T) = {\\beta \\vert \\; \\exists \\alpha \\in V_n(F), s.t. \\; \\beta = T(\\alpha)}$是$V_n(F)$上的子空间，称为$T$的像空间；$\\dim R(T)$称为$T$的秩。 零空间：$N(T) = {\\alpha \\vert\\; T(\\alpha) = 0}$是$V_n(F)$上的子空间，称为$T$的零空间；$\\dim N(T)$称为$T$的零度。（11）设$T$为$V_n(F)$上的线性变换，${\\alpha_1, \\alpha_2, …, \\alpha_n}$是$V_n(F)$的基，若存在$n$阶方阵$A$，有：\\(T(\\alpha_1 \\; \\alpha_2 \\; ... \\; \\alpha_n) = (\\alpha_1 \\; \\alpha_2 \\; ... \\; \\alpha_n)A\\)称$A$为$T$在基${\\alpha_1, \\alpha_2, …, \\alpha_n}$下的矩阵。 设$\\alpha$与$T(\\alpha)$在基${\\alpha_1, \\alpha_2, … , \\alpha_n}$下的坐标分别是$X$与$Y$，则有：${\\color {red}{ Y = AX}}$。 设${\\alpha_1, \\alpha_2, … , \\alpha_n}$和${\\beta_1, \\beta_2, … , \\beta_n}$是$V_n(F)$的两组基，且有$(\\beta_1 \\; \\beta_2 \\; … \\; \\beta_n) = (\\alpha_1 \\; \\alpha_2 \\; … \\; \\alpha_n)C$；$T$在两组基下的变换矩阵分别是$A$与$B$，则${\\color{red} {B=C^{-1}AC}}$。 （12）设$T$是线性空间$V_n(F)$上的线性变换，$W$是$V_n(F)$的子空间，如果$\\forall \\alpha \\in W, \\; T(\\alpha) \\in W$，即值域$T(W) \\subseteq W$，则称$W$是$T$的不变子空间。 **重要例题** 设$T$是欧式空间$R^3$上的线性变换，对$R^3$中单位矢量$u$，$\\forall x \\in R^3$，$T(x) = x - (1-k)(x,u)u$，问：T的不变子空间的直和分解以及相应的矩阵分解。 答：对向量$u$有\\(T(u) = u - (1-k)(u,u)u= u - (1-k)u = ku\\)所以以$u$为基向量的空间是不变子空间，表示为$L{u}$；同理，对于$u$的正交补子空间$u^{\\perp}$，对于任意向量$X \\in u^{\\perp}$，有\\(T(X) = X - (1-k)(X,u)u = X-0=X\\)于是另一个不变子空间为$u^{\\perp}$；即$R^3 = L{u} \\oplus u^{\\perp}$。显然有$L{u}$是一维空间，特征值$k$对应的特征向量是$u_1 = u$；那么$u^{\\perp}$就是二维空间，特征值$1$对应两个线性无关的特征向量，可以找到两个单位正交特征向量$u_2, u_3$，所以相应的矩阵分解为\\(\\begin{bmatrix}k &amp;amp; &amp;amp; \\\\ &amp;amp; 1 &amp;amp; \\\\ &amp;amp; &amp;amp; 1\\end{bmatrix}\\) ，对应的特征向量组 ${u_1,u_2,u_3}$为标准正交基。 （13）正交变换（酉变换）：线性变换$T$不改变向量内积，即$(T(\\alpha), T(\\beta)) = (\\alpha, \\beta)$。 正交变换$T$关于任一标准正交基的矩阵$C$满足$C^TC = CC^T=I$；酉变换关于任一标准正交基的矩阵$U$满足$U^HU=UU^H=I$。 正交矩阵的行列式为$\\pm 1$；酉矩阵的行列式的模长为$1$。（14）常见的正交变换 $R^2$上绕原点逆时针旋转$\\theta$角的线性变换$T_{\\theta}$称为正交变换，在标准正交基下对应的变换矩阵\\(\\begin{bmatrix} \\cos \\theta &amp;amp; - \\sin \\theta \\\\ \\sin \\theta &amp;amp; \\cos \\theta \\end{bmatrix}\\)是正交矩阵。 空间$R^3$上绕过原点的直线$l$旋转$\\theta$角的变化$T_{L_{\\theta}}$为正交变换，在标准正交基下对应的变换矩阵\\(\\begin{bmatrix}1 &amp;amp; &amp;amp; \\\\ &amp;amp; \\cos \\theta &amp;amp; -\\sin \\theta \\\\ &amp;amp; \\sin \\theta &amp;amp; \\cos \\theta \\end{bmatrix}\\)是正交矩阵。2. Jordan标准形（1）若有$T(\\xi) = \\lambda \\xi$，称$\\lambda$为$T$的特征值，$\\xi$为$T$的特征向量。如果$A$是线性变换$T$对应的矩阵，那么，$\\lambda$和$\\xi$也是$A$的特征值和特征向量。（2）设$\\lambda_1, \\lambda_2, …, \\lambda_s$是$V_n(F)$上线性变换$T$的$s$个互异特征值，$V_{\\lambda_i}$是$\\lambda_i$的特征子空间，其中$i=1,2,…,s$，则： $V_{\\lambda_i}$是$T$的不变子空间； $\\lambda_i \\neq \\lambda_j$时，$V_{\\lambda_i} \\cap V_{\\lambda_j} = {0}$； 若$\\lambda_i$是$k_i$重（代数重数）的，$\\dim V_{\\lambda_i}$是几何重数，则有$\\dim V_{\\lambda_i} \\leqslant k_i$。（3）线性变换$T$有对角矩阵表示的充分必要条件是 $T$有$n$个线性无关的特征向量。 幂等矩阵：$A^2 = A$，$A$相似于对角矩阵\\(\\begin{bmatrix} I_r&amp;amp; \\\\ &amp;amp; 0\\end{bmatrix}\\)，其中$r$为矩阵$A$的秩。 乘方矩阵：$A^2 = I$，$A$相似于对角阵\\(\\begin{bmatrix}I_s &amp;amp; \\\\ &amp;amp; I_t\\end{bmatrix}\\)，其中$s+t=n$。（4）关于秩的不等式：\\[rank(A \\pm B) \\leqslant rank(A) + rank(B)\\]\\[rank(A) + rank(B) -n \\leqslant rank(A_{m \\times n}B_{n \\times m}) \\leqslant \\min(r(A), r(B))\\]\\[if \\;A_{m \\times n}B_{n \\times m}=0, \\quad rank(A) + rank(B) \\leqslant n\\]（5）形如\\(J(\\lambda) = \\begin{bmatrix}\\lambda &amp;amp; 1 &amp;amp; &amp;amp; \\\\ &amp;amp; \\lambda &amp;amp; 1 &amp;amp; \\\\ &amp;amp; &amp;amp; ... &amp;amp; 1\\\\ &amp;amp; &amp;amp; &amp;amp; \\lambda\\end{bmatrix}\\)，称为Jordan块。Jordan块呈上三角，主对角线是它的全部特征值，特点是主对角线上元素相等，紧邻上方元素$a_{i,i+1} = 1$，其余元素为0。（6）每个$n$阶方阵$A$都相似于一个Jordan矩阵，即存在可逆矩阵$P$，有：\\(P^{-1}AP = J_A = \\begin{bmatrix}J_1(\\lambda_1) &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; J_2(\\lambda_2) &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; ... &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; J_s(\\lambda_s)\\end{bmatrix}\\)其中$J_A$称为Jordan标准形。（7）Jordan标准形的求法： 求矩阵$A$的特征多项式$\\vert\\lambda I-A\\vert = (\\lambda - \\lambda_1)^{k_1}(\\lambda-\\lambda_2)^{k_2}…(\\lambda - \\lambda_s)^{k_s}$，其中$k_i$是特征值$\\lambda_i$的代数重数，决定了对角线上特征值$\\lambda_i$的个数； 对$\\lambda_i$，由$(A-\\lambda_i I)X=0$，求$A$的线性无关的特征向量$\\alpha_1,\\alpha_2, …,\\alpha_{t_i}$，其中$t_i$是特征值$\\lambda_i$的几何重数，决定了Jordan块的个数； 如果$k_i = t_i$，即代数重数等于几何重数，说明$\\lambda_i$对应的Jordan块是对角阵； 如果$t_i &amp;lt; k_i$，就选择合适的特征向量$\\alpha_j$，利用\\({\\color{red} {\\vert A-\\lambda_i I\\vert = \\alpha_j}}\\)求Jordan链，确定每一个小Jordan块的阶数。 将所有特征值$\\lambda_i$对应的Jordan块组合起来，形成Jordan矩阵$J_A$。（8）矩阵多项式可以表示为$g(A) = a_m A^m + a_{m-1}A^{m-1}+…+a_1A +a_0 I $，由于有$A = P J_AP^{-1}$，所以有：\\[g(A) = P \\begin{bmatrix}g(J_1(\\lambda_1)) &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; g(J_2(\\lambda_2)) &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; ... &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; g(J_s(\\lambda_s)) \\end{bmatrix} P^{-1}\\]而对于$g(J(\\lambda))$则有：\\[g(J(\\lambda)) = \\begin{bmatrix}g(\\lambda) &amp;amp; g&#39;(\\lambda) &amp;amp; ... &amp;amp; \\frac{g^{(r-1)}(\\lambda)}{(r-1)!} \\\\ &amp;amp; g(\\lambda) &amp;amp; ... &amp;amp; .\\\\ &amp;amp; &amp;amp; ... &amp;amp; .\\\\ &amp;amp; &amp;amp; &amp;amp; g(\\lambda)\\end{bmatrix}\\]对于**常用的幂指数形式**有：\\[J^k(\\lambda) = \\begin{bmatrix}\\lambda^k &amp;amp; \\frac{(\\lambda^k)&#39;}{1!} &amp;amp; \\frac{(\\lambda^k)&#39;&#39;}{2!} &amp;amp;... \\\\ &amp;amp; \\lambda^k&amp;amp; ... &amp;amp; .\\\\ &amp;amp; &amp;amp; ... &amp;amp; .\\\\ &amp;amp; &amp;amp; &amp;amp; \\lambda^k\\end{bmatrix}\\]（9）使$g(A)=0$的多项式$g(\\lambda)$称为$A$的化零多项式，特征多项式必是矩阵$A$的化零多项式。 注：化零多项式的根一定包含了所有的特征值，但不能说化零多项式的根一定是特征值。（10）对于最小多项式$m_T(\\lambda)$ $m_T(\\lambda)$最高项系数为1； $m_T(\\lambda)$是$T$的一个化零多项式； $m_T(\\lambda)$是化零多项式中次数最低的那一个。 最小多项式$m_T(\\lambda)$的根一定包含了所有的特征值$\\lambda_i$，子式$(\\lambda-\\lambda_i)^{r_i}$的幂$r_i$等于Jordan标准形中关于特征值$\\lambda_i$的Jordan块中的最高阶数。 比如矩阵$A$有一个代数重数为3的特征值2，该特征值对应两个Jordan块，分别是\\(\\begin{bmatrix}2 &amp;amp; 1 \\\\ &amp;amp; 2 \\end{bmatrix}\\)以及$[2]$， 说明其中其最高阶数为2，那么在最小多项式中对应的子式为$(\\lambda -2)^2$。 3. 矩阵的分解（1）等价标准形 对于$A \\in C^{m \\times n}$，存在可逆矩阵$P \\in C^{m \\times m}, Q \\in C^{n \\times n}$，使得\\[A = P \\begin{bmatrix}I_r &amp;amp; 0 \\\\ 0 &amp;amp; 0\\end{bmatrix}Q\\]其中$r$是矩阵$A$的秩。（2）相似标准形 存在可逆矩阵$P \\in C^{ n \\times n}$，有\\(A = P \\begin{bmatrix}\\lambda_1 &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; \\lambda_2 &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; ... &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; \\lambda_n\\end{bmatrix} P^{-1}\\)（3）LU分解 定义：$L$是下三角矩阵，$U$是上三角矩阵，$A=LU$。 求法： 对于$(A \\;\\vert\\; I_n)$，只用第$i$行乘数$k$加到第$j$行（$i &amp;lt; j$）型初等变换将$A$化为上三角形$U$，可以得到\\({\\color{red} (U\\;\\vert\\;P)}\\)； 可知$PA=U$，于是有$L=P^{-1}$，则$A=LU$。（4）LDV分解 定义：$L, V$分别是对角线元素为1的下三角矩阵和上三角矩阵，$D$为对角矩阵，$A=LDV$。 求法： **方法一：** 由LU分解得到$A = LU$； 通过每行除以对应的对角线上元素的值，将$U$的对角线元素化为1，得到$U=DV$； 有$A=LDV$。 **方法二：** 取矩阵$A$对角线第一个元素，得到矩阵$A_1=[a_{11}]$，则有$A_1 = L_1D_1V_1 = [1][a_{11}][1]$； 取包含对角线前两个元素的二阶矩阵\\(A_2 = \\begin{bmatrix}A_1 &amp;amp; \\alpha\\\\ \\beta &amp;amp; a_{22}\\end{bmatrix}\\)，则有矩阵$A_2 = L_2 D_2 V_2$，其中\\(L_2 = \\begin{bmatrix}L_1 &amp;amp; 0\\\\ x &amp;amp; 1\\end{bmatrix}\\)，\\(D_2 = \\begin{bmatrix}D_1 &amp;amp; 0\\\\ 0 &amp;amp; d_2\\end{bmatrix}\\)，\\(V_2 = \\begin{bmatrix}V_1 &amp;amp; y\\\\ 0 &amp;amp; 1\\end{bmatrix}\\)，求得未知量$x, d_2, y$； 以此类推，最终得到$A = L_n D_n V_n$。（5）满秩分解 定义：对于$rank(A)=r$的矩阵$A$，若存在秩为$r$的矩阵$B \\in F^{m \\times r}, \\; C \\in F^{r \\times n}$，有$A=BC$，称为矩阵$A$的满秩分解。 求法：方法较多，一般只用最简单的第3种。 用行初等变换把$A$化为Hermite标准形； 依Hermite标准形中向量$e_i$所在的列的位置第$j_i$列，相应地取出$A$的第$j_i$列$a_{ji}$，得到 $A$的列向量极大无关组${a_{j_1}, a_{j_2}, …, a_{j_r}}$，$B =(a_{j_1}, a_{j_2}, …, a_{j_r}) $; $A$的Hermite矩阵中的非零行构成矩阵$C$，得到满秩分解$A=BC$。 举个例子： 求矩阵\\(A=\\begin{bmatrix}1 &amp;amp; 1 &amp;amp; 2\\\\ 0 &amp;amp; 2 &amp;amp; 2\\\\ 1 &amp;amp; 0 &amp;amp; 1\\end{bmatrix}\\)的满秩分解。 答： 用行初等变换化$A$为Hermite标准形：\\(A = \\begin{bmatrix}1 &amp;amp; 1 &amp;amp; 2\\\\ 0 &amp;amp; 2 &amp;amp; 2\\\\ 1 &amp;amp; 0 &amp;amp; 1\\end{bmatrix} \\rightarrow \\begin{bmatrix}1 &amp;amp; 1 &amp;amp; 2\\\\ 0 &amp;amp; 2 &amp;amp; 2\\\\ 0 &amp;amp; -1 &amp;amp; -1\\end{bmatrix} \\rightarrow \\begin{bmatrix}1 &amp;amp; 0 &amp;amp; 1\\\\ 0 &amp;amp; 1 &amp;amp; 1\\\\ 0 &amp;amp;0 &amp;amp;0 \\end{bmatrix}\\)可知$rank(A)=2$，$A$的前两列线性无关，取出构成$B$；取出$A$的Hermite标准形的前两行作为$C$，有： \\[B = \\begin{bmatrix} 1&amp;amp;1 \\\\ 0&amp;amp;2 \\\\ 1&amp;amp;0 \\end{bmatrix}, C = \\begin{bmatrix}1 &amp;amp; 0 &amp;amp; 1\\\\ 0 &amp;amp; 1 &amp;amp; 1\\end{bmatrix}, A=BC\\]（6）谱分解 定义：矩阵$A$互异的特征值${\\lambda_1, \\lambda_2, …, \\lambda_s}$称为矩阵$A$的谱。可相似对角化是可以谱分解的充要条件。 求法： 通过求特征值和特征向量得到\\(A = P\\begin{bmatrix}\\lambda_1 &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; ... &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; \\lambda_1 &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; \\lambda_2&amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; ... &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; \\lambda_2 &amp;amp; &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; ... &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; \\lambda_s &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; ... &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; \\lambda_s\\end{bmatrix} P^{-1}\\)； 对角阵\\(\\Lambda = \\lambda_1 \\begin{bmatrix}I_{r_1} &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; 0 &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; ... &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; 0\\end{bmatrix} + \\lambda_2\\begin{bmatrix}0 &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; I_{r_2} &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; 0 &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; 0\\end{bmatrix} + ... + \\lambda_s\\begin{bmatrix}0 &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; 0 &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; 0 &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; I_{r_s}\\end{bmatrix}\\)，令\\(Q_i = \\begin{bmatrix}0 &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp;... &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; I_{r_i} &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; ...\\end{bmatrix}\\)； 得到$A = \\sum_{i=1}^s =\\lambda_i P_i$，其中$P_i = P Q_i P^{-1}$。（7）Schur分解 定义：对可逆矩阵$A$，存在酉矩阵$U$和主对角线上元素都为正的上三角矩阵$R$，使$A=UR$。 求法： 取矩阵$A = (A_1, A_2, …, A_n)$的列向量，进行施密特正交化，得到$u_1,u_2, …,u_n$，有$U=(u_1,u_2,…,u_n)$； 再由$R = U^H A$得到$R$，于是$A=UR$。（8）几种特殊矩阵： **正规矩阵**： $A^HA = AA^H$ （**正规矩阵酉相似于对角阵**） **酉矩阵**： $A^HA = AA^H=I$ **Hermite矩阵**： $A^H = A$ （9）奇异值分解（SVD分解） **奇异值** ：对$rank(A)=r$的矩阵$A$，矩阵$A^HA$的非零特征值有$\\lambda_1 \\geqslant \\lambda_2 \\geqslant … \\geqslant \\lambda_r &amp;gt;0$，则称正数$\\sigma_i = \\sqrt{\\lambda_i}$为矩阵$A$的奇异值。 定义：对$rank(A)=r$的矩阵$A \\in C^{m \\times n}$，奇异值有$\\sigma_1 \\geqslant \\sigma_2 \\geqslant … \\geqslant \\sigma_r &amp;gt; 0$，则存在酉矩阵$U \\in C^{m \\times m}, \\; V \\in C^{n \\times n}$，分块矩阵\\(\\Sigma = \\begin{bmatrix}\\Delta &amp;amp; 0\\\\ 0 &amp;amp; 0\\end{bmatrix}\\)，有$A = U \\Sigma V^H$，其中\\(\\Delta = \\begin{bmatrix}\\sigma_1 &amp;amp; &amp;amp; &amp;amp; \\\\ &amp;amp; \\sigma_2 &amp;amp; &amp;amp; \\\\ &amp;amp; &amp;amp; ... &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; \\sigma_r\\end{bmatrix}\\)。 求解： 由特征多项式$\\vert\\lambda I - A^HA\\vert = 0$求得特征值$\\lambda_1 \\geqslant \\lambda_2 \\geqslant .. \\geqslant \\lambda_n$，（**务必按照从大到小排列**），以及每个特征值对应的特征向量$\\alpha_1, \\alpha_2, …, \\alpha_n$； 对特征向量进行施密特正交化和单位化（一般只需要单位化），得到单位正交向量组$v_1, v_2, ..,v_n$，则$V=(v_1, v_2, …,v_n)$； 对于非零特征值$\\lambda_1, …, \\lambda_r$对应奇异值$\\sigma_1, … , \\sigma_r$，于是有${\\color{red} {u_i = \\frac{1}{\\sigma_i}Av_i}}$，这样得到了$r$个列向量，剩余的设为$\\beta$，通过正交的特性$u_i^T \\beta = 0$即可求得， 于是得到$A=U \\Sigma V^H$（10）极分解 定义：对于$rank(A)=r$的矩阵$A \\in C^{n \\times n}$，可以被分解为$A=PQ$，其中$P$为半正定矩阵，$Q$为酉矩阵。 求法： 对$A$进行奇异值分解，得到$A=U \\Sigma V^H$; 可以得到$A = (U \\Sigma U^H)(UV^H)$，于是$P=U \\Sigma U^H, \\; Q=UV^H$，$A=PQ$。4. 矩阵的广义逆（1）设$A \\in C^{m \\times n}, B \\in C^{n \\times m}$，若有$BA=I_n$，则称$B$是$A$的一个**左逆**。 等价条件： $A$的零空间$N(A)={0}$ $m \\geqslant n, \\; rank(A)= n$，即$A$是列满秩的 $A^H A$可逆（2）设$A \\in C^{m \\times n}, C \\in C^{n \\times m}$，有$AC = I_m$，则称$C$是$A$的一个**右逆**。 等价条件： $A$的列空间$R(A)=C^m$ $m \\leqslant n, \\; rank(A)=m$，即$A$是行满秩的 $AA^H$可逆（3）对于$A \\in C^{m \\times n}, \\; G \\in C^{n \\times m}$，有$AGA=A$，称$G$是$A$的一个**减号广义逆**。 求法： 对$rank(A)=r$的矩阵$A$，有矩阵\\(\\begin{bmatrix}A &amp;amp; I_m\\\\ I_n &amp;amp; 0 \\end{bmatrix}\\)进行初等变换，对$A$行变换时$I_m$保持同步，对$A$列变换时，$I_n$保持同步，将$A$化为最简形，得到\\(\\begin{bmatrix} I_r&amp;amp; 0 &amp;amp; P \\\\ 0 &amp;amp; 0 &amp;amp; \\\\ \\\\ Q &amp;amp; &amp;amp; 0 &amp;amp; \\end{bmatrix}\\)； 有$G = Q\\begin{bmatrix}I_r &amp;amp; U\\ V &amp;amp; W\\end{bmatrix}P$，其中$U,V,W$是满足固定阶次的任意矩阵。（4）加号广义逆（M-P逆） 定义：对于矩阵$A \\in C^{m \\times n}, \\; G \\in C^{n \\times m}$，满足4条 $AGA=A$ $GAG=G$ $(AG)^H = AG$ $(GA)^H=GA$ 称$G$为$A$的M-P逆。 求法： **方法一** ： 对矩阵$A$进行**满秩分解**，得到$A=BC$; 则$\\color{red}{A^+ = C^H(CC^H)^{-1}(B^HB)^{-1}B^H}$，也就是等于C的右逆 x B的左逆。 **方法二** ： 对矩阵$A$进行**奇异值分解**，得到$A = U \\begin{bmatrix}\\Delta &amp;amp;0 \\ 0 &amp;amp; 0\\end{bmatrix}V^H$； 则\\(\\color{red}{A^+ = V \\begin{bmatrix}\\Delta^{-1} &amp;amp; 0\\\\ 0 &amp;amp; 0\\end{bmatrix}U^H}\\)。 性质 $rank(A) = rank(A^+)$ $rank(A^+A) = rank(AA^+)=rank(A)$（5）投影变换 定义：$C^n = L \\oplus M, \\quad x=y+z, \\quad y \\in L, z \\in M$，投影变换$\\sigma$就是把$C^n$映射成子空间$L$，称$\\sigma$是从$C^n$沿子空间$M$到子空间$L$的投影变换，在一组基下对应的矩阵称为投影矩阵，子空间$L$称为投影子空间。显然有，子空间$L$就是$\\sigma$的像空间$R(\\sigma)$，$M$就是$\\sigma$的核空间$N(\\sigma)$，于是$C^n = R(\\sigma) \\oplus N(\\sigma)$。 $\\sigma$是投影变换的充要条件是$\\sigma$关于某组基下的矩阵$A$是==幂等矩阵==，即$A^2=A$。 求法 找出像空间$L$的一组基$y_1,y_2,…,y_r$，得到矩阵$B = (y_1 \\; y_2 \\; … \\; y_r)$；找出$M$的一组基$z_{r+1}, …., z_n$，得到矩阵$C=(z_{r+1} \\; … \\; z_n)$； 于是有投影矩阵$\\color{red}{A = (B \\vert 0)(B\\vert C)^{-1}}$。（6）正交投影变换 定义：若$C^n = R(\\sigma) \\oplus N(\\sigma)$，$R(\\sigma)$的正交补空间是$R(\\sigma)^{\\perp} = N(\\sigma)$，称$\\sigma$是正交投影变换，其在标准正交基下对应的矩阵称为正交投影矩阵。 $\\sigma$是正交投影变换的充要条件是$A$是==幂等Hermite矩阵==，即$A^2=A, \\;A^H=A$。 求法\\[A = (B\\vert0)(B\\vert C)^{-1} = (B\\vert0)((B\\vert C)^H(B\\vert C))^{-1}(B\\vert C)^H = {\\color{red} {B(B^HB)^{-1}B^H}}\\]（7）最佳最小二乘解 $A \\in C^{m \\times n}, \\; b \\in C^m$，则${\\color{red} {x_0=A^+b}}$是线性方程组$Ax=b$的最佳最小二乘解。 $A \\in C^{m \\times n}, \\; B \\in C^{m \\times k}$，则${\\color{red}{X_0 = A^+B}}$是$AX=B$的最佳最小二乘解。5. 矩阵分析（1）向量范数满足正定性、齐次性和三角不等式，定义了范数的内积空间称为赋范空间。（2）重要的向量范数： 对于复向量$x = (x_1 \\;\\; x_2 \\;\\; … \\;\\; x_n)$，有： **2-范数：** ${\\color{red}{\\vert x\\vert| = \\sqrt{\\vert x_1\\vert^2 + \\vert x_2\\vert^2 + … + \\vert x_n\\vert^2}}} $ **1-范数：** ${\\color{red}{\\Vert x\\Vert_1 = \\vert x_1\\vert + \\vert x_2\\vert + … + \\vert x_n\\vert}}$ **∞范数：** ${\\color{red}{\\Vert x\\Vert_{\\infty} = \\underset{i}{\\max} \\vert x_i\\vert}}$ 有限维线性空间的任意两种向量范数都是==等价的==。（3）矩阵范数满足正定性、齐次性、三角不等式以及相容性。（4）重要的矩阵范数和诱导范数 **F范数**： ${\\color{red}{\\Vert A\\Vert_F = [tr(A^HA)]^{\\frac{1}{2}}}}$ **列和范数**：** ${\\color{red}{\\Vert A\\Vert_1 = \\underset{j}{\\max}(\\sum_{i=1}^n \\vert a_{ij}\\vert)}}$，即每一列各元素模相加其中的最大值** **谱范数**： ${\\color{red}{\\Vert A\\Vert_2 = \\sqrt{\\lambda_1}}}$，其中$\\lambda_1$是$A^HA$的最大特征值 **行和范数**： ${\\color{red}{\\Vert A\\Vert_{\\infty} = \\underset{i}{\\max}(\\sum_{j=1}^n \\vert a_{ij}\\vert)}}$，即每一行各元素模相加其中的最大值 （5）向量收敛和矩阵收敛必须其中的每一个元素都收敛。 向量按分量收敛的充要条件是它按任意一个向量范数收敛。 当$k \\rightarrow \\infty$时，$\\Vert A^{(k)}-A\\Vert \\rightarrow 0$，称矩阵序列按矩阵范数收敛于$A$（6）谱半径 定义：$\\lambda_1, \\lambda_2, …, \\lambda_n$是矩阵$A \\in C^{n \\times n}$的全部特征值，称${\\color{red}{\\rho(A)=\\underset{i}{\\max}\\vert \\lambda_i\\vert}}$为$A$的**谱半径**。 ==$A^k \\rightarrow 0(k \\rightarrow \\infty)$的充要条件是$\\rho(A) &amp;lt; 1$==。 $A$的谱半径是$A$的任意一种矩阵范数的下确界。（7）矩阵幂级数 若复变量$z$的幂级数$\\sum_{k=0}^{\\infty}a_kz^k$的收敛半径为$R$，而方阵$A \\in C^{n \\times n}$的谱半径为$\\rho(A)$，则 当${\\color{red} {\\rho(A) &amp;lt; R}}$时，矩阵幂级数$\\sum_{k=0}^{\\infty}a_kA^k$收敛； 当${\\color{red} {\\rho(A) &amp;gt; R}}$时，矩阵幂级数$\\sum_{k=0}^{\\infty}a_k A^k$发散 当求解$A$的特征值比较困难时，由于$A$的每个范数都是谱半径$\\rho(A)$的上界，只需要找到一种特殊的矩阵范数$   A   $，使得$   A   &amp;lt; R$，就能说明矩阵幂级数收敛。（**优先考虑行和、列和范数**） （8）常用的幂级数收敛域是整个复平面的幂级数\\[e^A = \\sum_{k=0}^{\\infty} \\frac{1}{k!}A^k\\]\\[\\cos A = \\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}A^{2k}\\]\\[\\sin A = \\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k+1)!}A^{2k+1}\\] **收敛域为复平面$ z &amp;lt; 1$的幂级数** \\[(I-A)^{-1} = \\sum_{k=0}^{\\infty}A^k, \\quad \\rho(A) &amp;lt; 1\\]\\[\\ln(I+A) = \\sum_{k=1}^{\\infty}\\frac{(-1)^{k-1}}{k}A^k, \\quad \\rho(A) &amp;lt; 1\\]（9）矩阵函数的两种求法 **方法一：Jordan标准形法** 求矩阵$A$的Jordan标准形$J_A$，得到${\\color{red} {A = PJ_AP^{-1}}}$ 设解析函数为$f(z)$，则对每一个Jordan块有\\(f(J_i)= \\begin{bmatrix}f(\\lambda_i) &amp;amp; \\frac{f&#39;(\\lambda_i)}{1!} &amp;amp; \\frac{f&#39;&#39;(\\lambda_i)}{2!} &amp;amp; ... \\\\ &amp;amp; f(\\lambda_i) &amp;amp; \\frac{f&#39;(\\lambda_i)}{1!} &amp;amp; ...\\\\ &amp;amp; &amp;amp; ... &amp;amp; \\\\ &amp;amp; &amp;amp; &amp;amp; f(\\lambda_i)\\end{bmatrix}\\)，得到$f(J_A)$ 最后得到$f(A) = Pf(J_A)P^{-1}$ 这种方法的难点在于需要求Jordan链，过程中可以会遇到麻烦。==如果不同特征值个数较多，建议使用第一种；而如果特征值比较单一，并且 代数重数 - 几何重数 &amp;gt; 2，建议使用第二种==。 **方法二：最小多项式法** 先计算$A$的Jordan标准形，由此得到最小多项式$m_A(\\lambda)=(\\lambda -\\lambda_1)^{n_1}(\\lambda-\\lambda_2)^{n_2}…(\\lambda-\\lambda_s)^{n_s}$，其中幂次和有$\\sum_{i=1}^s n_i =m$； 得到$g(\\lambda)=c_0+c_1\\lambda+…+c_{m-1}\\lambda^{m-1}$，并令$g^{(j)}(\\lambda_i)=f^{(j)}(\\lambda_i)$，解得系数$c_0,c_1,…,c_{m-1}$； 最后得到$f(A) = c_0I + c_1A+…+c_{m-1}A^{m-1}$ 当不同特征值的个数比较多或者最小多项式幂次较高时，计算起来比较复杂，建议使用第一种。 （10）两个知识点： 重要的导数 $\\color{red}{\\frac{d A^{-1}(t)}{dt} = - A^{-1}(t) \\big( \\frac{d}{dt}A(t)\\big)A^{-1}(t)}$ 矩阵指数函数的行列式 $\\vert e^A\\vert = e^{trA}$（11）矩阵函数应用 一阶常系数齐次微分方程组： \\(\\begin{cases}\\dot{x}(t) = Ax(t)\\\\x(t_0) = C_{n \\times 1}\\end{cases}\\)解为：${\\color{red}{x(t) = e^{A(t-t_0)}x(t_0)}}$ 一阶线性常系数非齐次线性方程组：\\(\\begin{cases}\\dot{x}(t) = Ax(t) + f(t)\\\\x(t_0) = C\\end{cases}\\)解为：${\\color{red} {x(t) = e^{A(t-t_0)}x(t_0) + \\int_{t_0}^t e^{A(t-\\tau)}f(\\tau)d \\tau}}$6. K积（1）对于矩阵$A=(a_{ij}) \\in C^{m \\times n}, \\;B=(b_{ij}) \\in C^{s \\times t}$，则K积为：\\(A \\otimes B = \\begin{bmatrix}a_{11}B &amp;amp; a_{12}B &amp;amp; ... &amp;amp; a_{1n}B\\\\ a_{21}B &amp;amp; a_{22}B &amp;amp; ... &amp;amp; a_{2n}B\\\\ ... &amp;amp; ... &amp;amp; &amp;amp; ...\\\\ a_{n1}B &amp;amp; a_{n2}B &amp;amp; ... &amp;amp; a_{nn}B\\end{bmatrix}\\) K积不具有交换律，即$A \\otimes B \\neq B \\otimes A$（2）重要性质 $I \\otimes I = I$ $(A \\otimes B)(C \\otimes D) = (AC) \\otimes (BD)$ $(A \\otimes B)^k = A^k \\otimes B^k$ $(A \\otimes B) = (I_m \\otimes B)(A \\otimes I_n)$ $(A \\otimes B)^{-1} = A^{-1} \\otimes B^{-1}$ $\\vert A \\otimes B\\vert = \\vert B \\otimes A\\vert = \\vert A\\vert^n\\vert B\\vert^m$ （**这里的`n`表示`B`的阶数，`m`表示`A`的阶数**） $rank(A \\otimes B) = rank(A)rank(B)$（3）K和：设$A \\in F^{m \\times m}, \\;B \\in F^{n \\times n}$，$A \\oplus B = A \\otimes I_n + I_m \\otimes B$（4）若$A$的特征值是$\\lambda_i$，相应的特征向量是$x_i$；$B$的特征值是$\\mu_i$，相应的特征向量为$y_i$；则： $A \\otimes B$的特征值是$\\color{red}{\\lambda_i \\mu_i}$，对应的特征向量是$\\color{red}{x_i \\otimes y_i}$ $A \\oplus B$的特征值是$\\color{red}{\\lambda_i + \\mu_i}$，对应的特征向量是$\\color{red}{x_i \\otimes y_i}$（5）设$f(z)$是解析函数，$A \\in F^{n \\times n}$，$f(A)$存在，则 $f(I_m \\otimes A) = I_m \\otimes f(A)$ $f(A \\otimes I_m) = f(A) \\otimes I_m$（6）设矩阵$A \\in F^{m \\times n}, \\quad A=(A_1, A_2,…,A_n)$，则\\(Vec(A) = \\begin{bmatrix}A_1\\\\ A_2\\\\ ...\\\\ A_n\\end{bmatrix} \\in F^{nm}\\) ${\\color{red}{Vec(ABC) = (C^T \\otimes A)Vec(B)}}$ $Vec(AX) = (I_s \\otimes A)Vec(X)$ $Vec(XC) = (C^T \\otimes I_k) Vec(X)$（7）求解矩阵方程$AX+XB=D$，将两边同时取向量化算子，得到${\\color{red}{(I_m \\otimes A + B^T \\otimes I_n)Vec(X) = Vec(D)}}$，最后通过常规的求非齐次线性方程组的方法求解。（8）求微分方程：$\\begin{cases}\\dot{X}(t) = AX(t) + X(t)B\\X(0) = C\\end{cases}$ 用向量化算子作用在方程两边，得到$Vec(\\dot{X}(t)) = (I_n \\otimes A + B^T \\otimes I_m)Vec(X(t))$和$Vec(X(0)) = Vec(C)$ 令$Y(t) = Vec(X(t)), \\quad C_1 = Vec(C), \\quad G = I_n \\otimes A + B^T \\otimes I_m$，通过求解普通微分方程的方法得到$Y(t) = e^{Gt}C_1$； 将$Y(t), \\; G, \\; C_1$带入化简求得$X(t)$。" }, { "title": "Video", "url": "/posts/video/", "categories": "2021, 12", "tags": "notes, video, tracking", "date": "2021-12-09 15:00:00 +0800", "snippet": "VideoVideo = 2D + TimeA video is a sequence of images4D tensor: $T\\times3\\times H\\times W$Training on Clips Raw video: Long, high FPS Training: Train model to classify short clips with low FPS Testing: Run model on different clips, average predictions.Video classification:Single-Frame CNNSimple idea: train normal 2D CNN to classify video frame independently.(Average predicted probs at test-time)Often a very strong baseline for video classification.Late Fusion with FC layers Intuition: Get high-level appearance of each frame, and combine them. Clip features: TDH’W’ with pooling The same intuition. Clip features: D Problem: Hard to compare low-level motion between frames. Early Fusion Intuition: Compare frames with very first conv layer, after that normal 2D CNN. Input: $T\\times3\\times H\\times W$ Reshape: $3T\\times H\\times W$. That means it concats all fo the frames along the cahnnel dimension, then fuse all of this temporal information using a single two-dimensional convolution operations. First 2D convolution collapses all temporal information: Input: $3T\\times H\\times W$, Output: $D\\times H\\times W$. Rest of the network is standard 2D CNN. Problem: One layer of temporal processing may not be enough.3D CNN Intuition: Use 3D versions of convolution and pooling to slowly fuse temporal information over the course of the network. Each layer in the network is a 4D tensor: $D\\times T\\times H\\times W$. Use 3D conv and 3D pooling operations.Early Fusion vs Late Fusion vs 3D CNN space information and temporal information are interchangeable.next level:different between processing motion and processing visual appearance measuring motion: optical flow flow field and distortion field for every pixel in the first frame, what is the vector displacement telling where that pixel is going to move in the second frame. next level: because all of operations we’ve seen are very localearly fusion doesn’t work well, 3D convolution is only looking at a tiny receptive field in time, optical flow is only looking between adjcent frames to compute the motion between a pair of frames.modeling long-term temporal structure" }, { "title": "Mac OS VSCode配置stdc++.h头文件", "url": "/posts/VSCode%E9%85%8D%E7%BD%AEstdc++%E5%A4%B4%E6%96%87%E4%BB%B6/", "categories": "2021, 11", "tags": "notes, vscode, mac", "date": "2021-11-28 22:00:00 +0800", "snippet": "VSCode 配置万能头文件stdc++.hmac自带的clang就没办法直接include bits/stdc++.h，苦恼了一下午，终于找到了解决方法。解决方法就是在“includePath”中添加自己写的stdc++.h 首先输入gcc -v -E -x c++ -找到地址 /Library/Developer/CommandLineTools/usr/include/c++/v1 在访达中输入shift+command+g复制上述地址，并写文件stdc++.h，常用的头文件都可以加在endif前 // C++ includes used for precompiling -*- C++ -*- // Copyright (C) 2003-2015 Free Software Foundation, Inc.//// This file is part of the GNU ISO C++ Library. This library is free// software; you can redistribute it and/or modify it under the// terms of the GNU General Public License as published by the// Free Software Foundation; either version 3, or (at your option)// any later version. // This library is distributed in the hope that it will be useful,// but WITHOUT ANY WARRANTY; without even the implied warranty of// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the// GNU General Public License for more details. // Under Section 7 of GPL version 3, you are granted additional// permissions described in the GCC Runtime Library Exception, version// 3.1, as published by the Free Software Foundation. // You should have received a copy of the GNU General Public License and// a copy of the GCC Runtime Library Exception along with this program;// see the files COPYING3 and COPYING.RUNTIME respectively. If not, see// &amp;lt;http://www.gnu.org/licenses/&amp;gt;. /** @file stdc++.h * This is an implementation file for a precompiled header. */ // 17.4.1.2 Headers // C#ifndef _GLIBCXX_NO_ASSERT#include &amp;lt;cassert&amp;gt;#endif#include &amp;lt;cctype&amp;gt;#include &amp;lt;cerrno&amp;gt;#include &amp;lt;cfloat&amp;gt;#include &amp;lt;ciso646&amp;gt;#include &amp;lt;climits&amp;gt;#include &amp;lt;clocale&amp;gt;#include &amp;lt;cmath&amp;gt;#include &amp;lt;csetjmp&amp;gt;#include &amp;lt;csignal&amp;gt;#include &amp;lt;cstdarg&amp;gt;#include &amp;lt;cstddef&amp;gt;#include &amp;lt;cstdio&amp;gt;#include &amp;lt;cstdlib&amp;gt;#include &amp;lt;cstring&amp;gt;#include &amp;lt;ctime&amp;gt; #if __cplusplus &amp;gt;= 201103L#include &amp;lt;ccomplex&amp;gt;#include &amp;lt;cfenv&amp;gt;#include &amp;lt;cinttypes&amp;gt;#include &amp;lt;cstdalign&amp;gt;#include &amp;lt;cstdbool&amp;gt;#include &amp;lt;cstdint&amp;gt;#include &amp;lt;ctgmath&amp;gt;#include &amp;lt;cwchar&amp;gt;#include &amp;lt;cwctype&amp;gt;#endif // C++#include &amp;lt;algorithm&amp;gt;#include &amp;lt;bitset&amp;gt;#include &amp;lt;complex&amp;gt;#include &amp;lt;deque&amp;gt;#include &amp;lt;exception&amp;gt;#include &amp;lt;fstream&amp;gt;#include &amp;lt;functional&amp;gt;#include &amp;lt;iomanip&amp;gt;#include &amp;lt;ios&amp;gt;#include &amp;lt;iosfwd&amp;gt;#include &amp;lt;iostream&amp;gt;#include &amp;lt;istream&amp;gt;#include &amp;lt;iterator&amp;gt;#include &amp;lt;limits&amp;gt;#include &amp;lt;list&amp;gt;#include &amp;lt;locale&amp;gt;#include &amp;lt;map&amp;gt;#include &amp;lt;memory&amp;gt;#include &amp;lt;new&amp;gt;#include &amp;lt;numeric&amp;gt;#include &amp;lt;ostream&amp;gt;#include &amp;lt;queue&amp;gt;#include &amp;lt;set&amp;gt;#include &amp;lt;sstream&amp;gt;#include &amp;lt;stack&amp;gt;#include &amp;lt;stdexcept&amp;gt;#include &amp;lt;streambuf&amp;gt;#include &amp;lt;string&amp;gt;#include &amp;lt;typeinfo&amp;gt;#include &amp;lt;utility&amp;gt;#include &amp;lt;valarray&amp;gt;#include &amp;lt;vector&amp;gt; #if __cplusplus &amp;gt;= 201103L#include &amp;lt;array&amp;gt;#include &amp;lt;atomic&amp;gt;#include &amp;lt;chrono&amp;gt;#include &amp;lt;condition_variable&amp;gt;#include &amp;lt;forward_list&amp;gt;#include &amp;lt;future&amp;gt;#include &amp;lt;initializer_list&amp;gt;#include &amp;lt;mutex&amp;gt;#include &amp;lt;random&amp;gt;#include &amp;lt;ratio&amp;gt;#include &amp;lt;regex&amp;gt;#include &amp;lt;scoped_allocator&amp;gt;#include &amp;lt;system_error&amp;gt;#include &amp;lt;thread&amp;gt;#include &amp;lt;tuple&amp;gt;#include &amp;lt;typeindex&amp;gt;#include &amp;lt;type_traits&amp;gt;#include &amp;lt;unordered_map&amp;gt;#include &amp;lt;unordered_set&amp;gt;#endif 最后在$project/.vscode/目录下的c_cpp_properties.json文件中加上一条”includePath”，我的json文件： { &quot;configurations&quot;: [ { &quot;name&quot;: &quot;Mac&quot;, &quot;includePath&quot;: [ &quot;${workspaceFolder}/**&quot;, &quot;/Library/Developer/CommandLineTools/usr/include/c++/v1&quot; ], &quot;defines&quot;: [], &quot;macFrameworkPath&quot;: [ &quot;/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks&quot; ], &quot;compilerPath&quot;: &quot;/usr/bin/clang&quot;, &quot;cStandard&quot;: &quot;c17&quot;, &quot;cppStandard&quot;: &quot;c++98&quot;, &quot;intelliSenseMode&quot;: &quot;macos-clang-arm64&quot; } ], &quot;version&quot;: 4}" }, { "title": "Generative Learning algorithms", "url": "/posts/GDA_NB/", "categories": "2021, 10", "tags": "notes, ml", "date": "2021-10-27 22:30:00 +0800", "snippet": "Generative Learning algorithmsdiscriminative learning algorithms: Algorithms that try to learn $p(y\\vert x)$ directly (such as logistic regression) or algorithms that try to learn mappings directly from the space of inputs $\\mathcal{X}$ to the labels {0,1} (such as perceptron).generative learning algorithms: instead try to model $p(x\\vert y)$ (and $p(y)$). For instance, if $y$ indicates whether an example is a dog (0) and an elephant (1), then $p(x\\vert y=0)$ models the distribution of dogs’ features, and $p(x\\vert y=1)$ models the distribution of elephants’ features.After modeling $p(y)$ (called the class prior) and $p(x\\vert y)$ (called class-conditional probability / likelihood), our algorithm can then use Bayes rule to derive the posterior distribution on $y$ given $x$:\\(p(y\\vert x)=\\frac{p(x\\vert y)p(y)}{p(x)}\\)Here, the denominator is given by $p(x)=p(x\\vert y=1)p(y=1)+p(x\\vert y=0)p(y=0)$. Actually, if were calculating $p(y\\vert x)$ in order to make a prediction, then we don’t actually need to calculate the denominator, since\\[\\begin{align}\\arg\\max_y p(y\\vert x)&amp;amp;=\\arg\\max_y\\frac{p(x\\vert y)p(y)}{p(x)}\\\\&amp;amp;=\\arg\\max_y p(x\\vert y)p(y)\\\\\\end{align}\\]1. Gaussian discriminant analysisGDA. We assume that $p(x\\vert y)$ is distributed according to a multivariate normal distribution.1.1 The multivariate normal distributionIn d-dimensions, also called the multivariate Gaussian distribution, is parameterized by a mean vector $\\mu\\in\\mathbb{R}^d$ and a covariance matrix $\\Sigma\\in\\mathbb{R}^{d\\times d}$, where $\\Sigma\\ge 0$ is symmetric and positive semi-definite对称半正定 . Also written “$\\mathcal{N}(\\mu,\\Sigma)$”, its density is given by:\\[p(x;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{d/2}\\vert \\Sigma\\vert^\\frac12}\\exp(-\\frac12(x-\\mu)^T\\Sigma^{-1}(x-\\mu))\\]For a random variable $X\\in\\mathcal{N}(\\mu,\\Sigma)$, the mean is $\\mu$ , the covariate matrix is $\\Sigma$.1.2 The GDA modelThe classification problem in which the input features $x$ are continuous-valued random variables, we can then use GDA model, which model $p(x\\vert y)$ using a multivariate normal distribution. The model is:\\[\\begin{align}y&amp;amp;\\sim\\text{Bernoulli}(\\phi)\\\\x\\vert y=0&amp;amp;\\sim\\mathcal{N}(\\mu_0,\\Sigma)\\\\x\\vert y=1&amp;amp;\\sim\\mathcal{N}(\\mu_1,\\Sigma)\\end{align}\\]也就是\\[\\begin{align}p(y)&amp;amp;=\\phi^y(1-\\phi)^{1-y}\\\\p(x\\vert y=0)&amp;amp;=\\frac{1}{(2\\pi)^{d/2}\\vert \\Sigma\\vert^\\frac12}\\exp(-\\frac12(x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0))\\\\p(x\\vert y=1)&amp;amp;=\\frac{1}{(2\\pi)^{d/2}\\vert \\Sigma\\vert^\\frac12}\\exp(-\\frac12(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1))\\end{align}\\]连乘操作容易造成下溢，通常使用对数似然\\[\\begin{align}\\ell&amp;amp;=\\log\\prod_{i=1}^np(x^{(i)},y^{(i)};\\phi,\\mu_0,\\mu_1,\\Sigma)\\\\&amp;amp;=\\log\\prod_{i=1}^np(x^{(i)}\\vert y^{(i)};\\mu_0,\\mu_1,\\Sigma)p(y^{(i)};\\phi)\\\\&amp;amp;=\\sum_{i=1}^n-\\frac d2\\log{2\\pi}-\\frac12\\log{\\vert \\Sigma\\vert}-\\frac12(x-\\mu)^T\\Sigma^{-1}(x-\\mu)+y\\log\\phi+(1-y)\\log{(1-\\phi)}\\end{align}\\]最大化对数似然估计，求矩阵偏导=0，得：\\[\\begin{align}\\phi&amp;amp;=\\frac1n\\sum_{i=1}^n1\\lbrace{y^{(i)}=1}\\rbrace\\\\\\mu_0&amp;amp;=\\frac{\\sum_{i=1}^n1\\lbrace{y^{(i)}=0}\\rbrace^{(i)}}{\\sum_{i=1}^n1\\lbrace{y^{(i)}=0}\\rbrace}\\\\\\mu_1&amp;amp;=\\frac{\\sum_{i=1}^n1\\lbrace{y^{(i)}=1} \\rbrace x^{(i)}}{\\sum_{i=1}^n1\\lbrace{y^{(i)}=1}\\rbrace}\\\\\\Sigma&amp;amp;=\\frac1n\\sum_{i=1}^n(x^{(i)}-\\mu_{y^{(i)}})(x^{(i)}-\\mu_{y^{(i)}})^T\\end{align}\\]参数已经得到，判断新样本 x, 求得$p(y=0\\vert x)、p(y=1\\vert x)$ 取概率值大的那一类。GDA 的分界面$(1-\\phi)\\exp((x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0))=\\phi\\exp((x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1))$化简可得$2x^T\\Sigma^{-1}(\\mu_1-\\mu_0)=\\mu_1^T\\Sigma^{-1}\\mu_1-\\mu_0^T\\Sigma^{-1}\\mu_0+\\log\\phi-\\log(1-\\phi)$1.3 GDA and logistic regression可以将 $p(y\\vert x)$ 表示成\\[p(y=1\\vert x;\\phi,\\Sigma,\\mu_0,\\mu_1)=\\frac{1}{1+\\exp(-\\theta^Tx)}\\]where $\\theta$ is some appropriate function of $\\phi,\\Sigma,\\mu_0,\\mu_1$.We just argued that if $p(x\\vert y)$ is multivariate gaussian (with shared $\\Sigma$), then $p(y\\vert x)$ necessarily follows a logistic function. The converse is false. This shows that GDA makes stronger modeling assumption, and is more data efficient when assumption are correct.2. Naive Bayes离散特征Naive Bayes assumption: $x_i’s$ are conditionally independent given $y$. 简单来说就是，给定条件 $y$ , $x_i$ 之间互相独立\\[\\begin{align}p(x_1,\\dots,x_n\\vert y)&amp;amp;=p(x_1\\vert y)p(x_2\\vert y,x_1)\\cdots p(x_n\\vert y,x_1,x_2,\\dots,x_{n-1})\\\\&amp;amp;=p(x_1\\vert y)p(x_2\\vert y)\\cdots p(x_n\\vert y)\\\\&amp;amp;=\\prod_{j=1}^np(x_j\\vert y)\\\\\\end{align}\\]Model is parameterized by $\\phi_{j\\vert y=1}=p(x_j=1\\vert y=1),\\phi_{j\\vert y=0}=p(x_j=1\\vert y=0)$ and $\\phi_y=p(y=1)$. As usual, given a training set $\\lbrace(x^{(i)},y^{(i)});i=1,\\dots,n\\rbrace$ , where each $x^{(i)}$ is a vector, and each $y^{(i)}$ is in ${0,1}$.We assume that each vector x is in the set $\\lbrace0,1\\rbrace^d$ for some integer $d$ specifying the number of “feature” in the model. The joint likelihood of the data:\\[\\mathcal{L}(\\phi_y,\\phi_{j\\vert y=0},\\phi_{j\\vert y=1})=\\prod_{i=1}^np(x^{(i)},y^{(i)})\\]log-likelihood function is\\[\\begin{align}\\ell&amp;amp;=\\sum_{i=1}^n\\log p(x^{(i)},y^{(i)})\\\\&amp;amp;=\\sum_{i=1}^n\\log\\Bigl(\\phi_{y^{(i)}}\\prod_{j=1}^d\\phi_{x_j^{^{(i)}}\\vert y^{(i)}}\\Bigr)\\\\&amp;amp;=\\sum_{i=1}^n\\log\\phi_{y^{(i)}}+\\sum_{i=1}^n\\sum_{j=1}^d\\log\\phi_{x_j^{(i)}\\vert y^{(i)}}\\end{align}\\]maximizing this w.r.t. $\\phi_y,\\phi_{j\\vert y=0},\\phi_{j\\vert y=1}$ gives the maximum likelihood estimates:\\[\\begin{align}\\phi_{j\\vert y=1}&amp;amp;=\\frac{\\sum_{i=1}^n1\\lbrace x_j^{(i)}=1\\land y^{(i)}=1\\rbrace}{\\sum_{i=1}^n1\\lbrace y^{(i)}=1\\rbrace}\\\\\\phi_{j\\vert y=1}&amp;amp;=\\frac{\\sum_{i=1}^n1\\lbrace x_j^{(i)}=1\\land y^{(i)}=0\\rbrace}{\\sum_{i=1}^n1\\lbrace y^{(i)}=0\\rbrace}\\\\\\phi_y&amp;amp;=\\frac{\\sum_{i=1}^n1\\lbrace y^{(i)}=1\\rbrace}{n}\\end{align}\\]For interpretation,$\\phi_y$ can be interpreted as the probability of seeing the label $y$. The value of $\\phi_{j\\vert y=1}$ is just the fraction of the spam (y=1) emails in which word j does appear.To make a prediction on a anew example with features $x$, we then simply calculate\\[\\begin{align}p(y=1\\vert x)&amp;amp;=\\frac{p(x\\vert y=1)p(y=1)}{p(x)}\\\\&amp;amp;=\\frac{\\Bigl(\\prod_{j=1}^np(x_j\\vert y=1)\\Bigr)p(y=1)}{\\Bigl(\\prod_{j=1}^np(x_j\\vert y=1)\\Bigr)p(y=1)+\\Bigl(\\prod_{j=1}^np(x_j\\vert y=0)\\Bigr)p(y=0)}\\\\\\end{align}\\]and pick whichever class has the higher posterior probability.Laplace smoothingmultinomial random variable $z$ taking values in ${1,\\dots,k}$.\\[\\phi_j=\\frac{1+\\sum_{i=1}^n1\\lbrace z^{(i)}=j\\rbrace}{k+n}\\] " }, { "title": "Review of Probability Theory", "url": "/posts/Prob/", "categories": "2021, 10", "tags": "note, prob", "date": "2021-10-27 15:00:00 +0800", "snippet": "Multiple random variables基本性质$X_1,X_2,\\dots,X_n$的联合分布函数$F_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n)=P(X\\le x_1,X_2\\le x_2,\\dots,X_n\\le x_n)$联合概率密度函数$f_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n)=\\frac{\\partial^nF_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n)}{\\partial x_1,\\dots,\\partial x_n}$$X_1$的边缘密度函数$f_{X_1}(X_1)=\\int_{-\\infty}^{\\infty}\\cdots\\int_{-\\infty}^{\\infty}f_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n)dx_2\\dots dx_n$$X_1$ 的条件概率密度函数$f_{X_1\\vert X_2,\\dots,X_n}(x_1\\vert x_2,\\dots,x_n)=\\frac{f_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n)}{f_{X_2,\\dots,X_n}(x_2,\\dots,x_n)}$Independence. In practice, non-independence of samples does come up often, and it has the effect od reducing the “effective size” of the training set.Random Vector假设有 n 个随机变量，通常用 random vector 表示更方便， $X=[X_1,X_2,\\dots,X_n]^T$期望、方差、协变量略协方差矩阵 covariance matrix. For a given random vector $X:\\Omega\\rightarrow\\mathbb{R}^n$, its covariance matrix $\\Sigma$ is the nxn square matrix, and its entries are given by $\\Sigma_{ij}=Cov[X_i,X_j]$.\\(\\begin{align}\\Sigma&amp;amp;=\\begin{bmatrix}Cov[X_1,X_1]&amp;amp;\\cdots&amp;amp;Cov[X_1,X_n]&amp;amp;\\\\\\vdots&amp;amp;\\ddots&amp;amp;\\vdots&amp;amp;\\\\Cov[X_n,X_1]&amp;amp;\\cdots&amp;amp;Cov[X_n,X_n]&amp;amp;\\end{bmatrix}\\\\&amp;amp;=\\begin{bmatrix}E[X_1^2]-E[X_1]E[X_1]&amp;amp;\\cdots&amp;amp;E[X_1X_n]-E[X_1]E[X_n]&amp;amp;\\\\\\vdots&amp;amp;\\ddots&amp;amp;\\vdots&amp;amp;\\\\E[X_nX_1]-E[X_n]E[X_1]&amp;amp;\\cdots&amp;amp;E[X_n^2]-E[X_n]E[X_n]&amp;amp;\\end{bmatrix}\\\\&amp;amp;=\\begin{bmatrix}E[X_1^2]&amp;amp;\\cdots&amp;amp;E[X_1X_n]&amp;amp;\\\\\\vdots&amp;amp;\\ddots&amp;amp;\\vdots&amp;amp;\\\\E[X_nX_1]&amp;amp;\\cdots&amp;amp;E[X_n^2]&amp;amp;\\end{bmatrix}-\\begin{bmatrix}E[X_1]E[X_1]&amp;amp;\\cdots&amp;amp;E[X_1]E[X_n]&amp;amp;\\\\\\vdots&amp;amp;\\ddots&amp;amp;\\vdots&amp;amp;\\\\E[X_n]E[X_1]&amp;amp;\\cdots&amp;amp;E[X_n]E[X_n]&amp;amp;\\end{bmatrix}\\\\&amp;amp;=E[XX^T]-E[X]E[X]^T=\\dots=E[(X-E[X])(X-E[X])^T]\\end{align}\\)The multivariate Gaussian distribution\\[f_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{d/2}\\vert \\Sigma\\vert^\\frac12}\\exp(-\\frac12(x-\\mu)^T\\Sigma^{-1}(x-\\mu))\\]" }, { "title": "Proof of Jacobi&#39;s formula", "url": "/posts/JacobiFormula/", "categories": "2021, 10", "tags": "matrix theory", "date": "2021-10-03 15:00:00 +0800", "snippet": "proof555,本来手打了一遍，因为更新pages功能，把本地markdown文件删了，git版本回退也没整回来" }, { "title": "Notes on backpropagation", "url": "/posts/Backpropagation/", "categories": "2021, 09", "tags": "notes, nn", "date": "2021-09-03 19:00:00 +0800", "snippet": "Some common loss function backpropagation1. SVMhinge loss\\[L_i=\\sum_{j\\neq y_i}\\max(0,\\omega_j^Tx_i-\\omega_{y_i}^Tx_i+\\Delta)\\]SVM full loss:\\[L=\\frac1N\\sum_i\\sum_{j\\neq y_i}[\\max(0,f(x_i,W)_j-f(x_i;W)_{y_i}+\\Delta)]+\\lambda\\sum_k\\sum_lW_{k,l}^2 \\\\\\]where $W_{k,l}^2$ is all square elements of $W$.its backpropagation process is below:\\[\\begin{align}\\text{dW}_j&amp;amp;=\\frac{X_i}{N} \\\\\\text{dW}_{y_i}&amp;amp;=\\frac{-X_i}{N} \\\\\\text{dW}&amp;amp;=\\text{dW}+2\\lambda W\\end{align}\\]2. Cross entropy error with logistic activationIn a classification task with two classes, it is standard to use a neural network architecture with a single logistic output unit and the cross-entropy loss function. The output predication is between zero and one, and is interpreted as a probability.We consider a network with a single hidden layer of logistic units, multiple logistic output units.The cross entropy error:\\[E=-\\sum_{i=1}^{nclass}(t_i\\log(y_i)+(1-t_i)\\log(1-y_i))\\]where $\\text{t}$ is the target vector, $\\text{y}$ is the output vector. Outputs are computed by applying the logistic function to the weighted sums of the hidden layer activations, $s$,\\[\\begin{align}y_i=\\frac1{1+e^{-s_i}} \\\\s_i=\\sum_{j=1}h_j\\omega_{ji} \\\\\\end{align}\\]Compute the derivative of the error w.r.t each weight connecting the hidden units to the output units using chain rule.\\[\\frac{\\partial E}{\\partial\\omega_{ji}}=\\frac{\\partial E}{\\partial y_i}\\frac{\\partial y_i}{\\partial s_i}\\frac{\\partial s_i}{\\partial \\omega_{ji}}\\]Examining each factor in turn:\\[\\begin{equation}\\begin{aligned}\\frac{\\partial E}{\\partial y_i}&amp;amp;=\\frac{-t_i}{y_i}+\\frac{1-t_i}{1-y_i} \\\\&amp;amp;=\\frac{y_i-t_i}{y_i(1-y_i)} \\\\\\frac{\\partial y_i}{\\partial s_i}&amp;amp;=y_i(1-y_i) \\\\\\frac{\\partial s_i}{\\partial \\omega_{ji}}&amp;amp;=h_j \\\\\\end{aligned}\\end{equation}\\]Combining things back together:\\[\\frac{\\partial E}{\\partial s_i}=y_i-t_i\\]and\\[\\frac{\\partial E}{\\partial \\omega_{ji}}=(y_i-t_i)h_j\\]3. Softmax and cross-entropy errorWhen a classification task has more than two classes, it is standard to useThe softmax activation of $i$-th output unit is:\\[y_i=\\frac{e^{s_i}}{\\sum_{c}^{nclass}e^{s_c}}\\]and the cross entropy error function for multi-class output is:\\[E=-\\sum_i^{nclass}t_i\\log(y_i)\\]$t$ is the target vector. Thus, computing the gradient yields:\\[\\begin{align}\\frac{\\partial E}{\\partial y_i}&amp;amp;=-\\frac{t_i}{y_i} \\\\\\frac{\\partial y_i}{\\partial s_k}&amp;amp;=\\begin{cases}\\frac{e^{s_i}}{\\sum_{c}^{nclass}e^{s_c}}-(\\frac{e^{s_i}}{\\sum_{c}^{nclass}e^{s_c}})^2 &amp;amp; i=k \\\\-\\frac{e^{s_i}e^{s_k}}{(\\sum_{c}^{nclass}e^{s_c})^2} &amp;amp; i\\neq k \\\\\\end{cases}\\\\ &amp;amp;=\\begin{cases}y_i(1-y_i) &amp;amp; i=k\\\\-y_iy_k &amp;amp; i\\neq k\\end{cases}\\end{align}\\]\\[\\begin{aligned}\\frac{\\partial E}{\\partial s_i}&amp;amp;=\\sum_k^{nclass}\\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial s_i} \\\\&amp;amp;=\\frac{\\partial E}{\\partial y_i}\\frac{\\partial y_i}{\\partial s_i}-\\sum_{k\\neq i}\\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial s_i} \\\\&amp;amp;= -t_i(1-y_i)+\\sum_{k\\neq i}t_ky_i \\\\&amp;amp;= -t_i+y_i\\sum_kt_k \\\\&amp;amp;=y_i-t_i \\\\\\end{aligned}\\]" }, { "title": "ML Distance", "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Distance/", "categories": "2021, 08", "tags": "Machine Learning, notes", "date": "2021-08-27 19:00:00 +0800", "snippet": "距离 正定性 对称性 三角不等式有序距离 闵科夫斯基距离: $l=(\\sum_{i=1}^n\\vert x_i-y_i\\vert^p)^\\frac1p$ 切比雪夫距离: $l_{\\infty}=\\max_{i=1}^n\\vert x_i-y_i\\vert$ 欧几里得距离: $l_2=\\sqrt{\\sum_{i=1}^n\\vert x_i-y_i\\vert^2}$ 曼哈顿距离: $l_1=\\sum_{i=1}^n\\vert x_i-y_i\\vert$ 加权闵科夫斯基距离: $l=(\\sum_{i=1}^n\\omega_i\\vert x_i-y_i\\vert^p)^\\frac1p$ 马氏距离: $d(\\vec{x},\\vec{y})=\\sqrt{(\\vec{x}-\\vec{y})^TS^{-1}(\\vec{x}-\\vec{y})}$ $S$ 协方差矩阵 $\\mathrm{dist}_{mah}^2(x_i,x_j)=(x_i-x_j)^TM(x_i-x_j)=\\Vert x_i-x_j\\Vert_M^2$, 度量矩阵 $M$ 为半正定矩阵。 $M=PP^T$ $\\Vert x_i-x_j\\Vert_M=\\Vert P^Tx_i-P^Tx_j\\Vert$ 余弦距离: $d(x,y)=\\frac{&amp;lt;x,y&amp;gt;}{\\vert x\\vert\\vert y\\vert}$离散距离簇 VDM (Value Difference Metric) $m_{u,a}$: 在属性 $u$ 上取值为 $a$ 的样本数 $m_{u,a,i}$: 第 $i$ 个样本簇在属性为 $a$ 的样本数 $\\text{VDM}p(a,b)=\\sum{i=1}^k\\vert \\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_u,b}\\vert$ $\\text{MinkovDM}p(a,b)={(\\sum\\vert x{iu}-x_{ju}\\vert^p+\\sum\\text{VDM}p(x{iu},x_{ju}))^{\\frac1p}}$ 字符串 海明距离 Lee 距离 Levenshtein (编辑距离)\\[\\text{lev}(a,b)=\\begin{cases}\\max(i,j)\\qquad \\text{if}\\ \\min(i,j)=0\\newline \\min\\begin{cases}\\text{lev}_{a,b}(i-1,j)+1 \\newline\\text{lev}_{a,b}(i,j-1)+1 \\newline\\text{lev}_{a,b}(i-1,j-1)+1_{a_i\\neq b_j} \\newline\\end{cases} \\qquad \\text{otherwise}\\end{cases}\\]非度量距离不满足三角不等式 (相似度度量无需满足三角不等式)两组点集的相似程度 Hausdorff 距离 $\\text{dist}_H(X,Z)=\\max(\\text{dist}_h(X,Z),\\text{dist}_h(Z,X))$ $\\text{dist}h(X,Z)=\\max{x\\in X}\\min_{z\\in Z}\\Vert x-z\\Vert_2$ NCANeighbourhood Component Analysis 近邻成分分析 近邻分类器中 $x_j$ 对 $x_i$ 分类结果影响概率为: $p_{ij}=\\frac{e^{-\\Vert x_i-x_j\\Vert_M^2}}{\\sum_le^{-\\Vert x_i-x_j\\Vert_m^2}}$ $x_i$ LOO 正确率: $p_i=\\sum_{j\\in \\Omega_i}p_{ij},\\Omega_i$ 为相同类别下标 训练集 LOO 正确率: $\\sum_{i=1}^mp_i=\\sum_{i=1}^m\\sum_{j\\in \\Omega_i}p_{ij}$ $\\min_p1-\\sum_{i=1}^m\\sum_{j\\in \\Omega_i}\\frac{\\exp(-\\Vert P^Tx_i-P^Tx_j\\Vert_2^2)}{\\sum_l\\exp(-\\Vert P^Tx_i-P^Tx_l\\Vert_2^2)}$领域知识必连约束 $\\mathcal{M}$,勿连约束 $\\mathcal{C}$\\[\\begin{equation}\\begin{aligned}\\min_{M}&amp;amp;\\sum_{(x_i,x_j)\\in\\mathcal{M}}\\Vert x_i-x_j\\Vert_M^2 \\notag \\\\s.t. &amp;amp;\\sum_{(x_i,x_k)\\in\\mathcal{C}}\\Vert x_i-x_k\\Vert_m \\geq1 \\notag\\\\&amp;amp;M\\succeq 0\\end{aligned}\\end{equation}\\]LMNNLarge Margin Nearest Neighbors $k$ 个目标邻居相近，入侵样本远离 目标邻居：最近的同类别样本 入侵样本：最近中的非同类样本 \\[\\begin{align}\\min\\ast M&amp;amp;\\sum_{i,j\\in N\\ast i}d(x_i,x_j)+\\sum_{i,j,l}\\xi_{ijl} \\notag \\\\s.t.&amp;amp;\\forall_{i,j\\in N_k,l,yl\\neq y_i}d(x_i,d_j)+1\\leq d(x_i,x_l)+\\xi_{ijl} \\notag \\\\&amp;amp;\\xi_{ijl}\\geq0 \\notag \\\\&amp;amp;M \\succeq0 \\notag \\\\\\end{align}\\]" }, { "title": "ML Dimension Reduction", "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_DimensionReduction/", "categories": "2021, 08", "tags": "notes, Machine Learning", "date": "2021-08-17 15:00:00 +0800", "snippet": "线性降维 维数灾难 curse of dimensionality 高维空间样本稀疏 计算内积难 MDSMultiple Dimensional Scaling, 多维放缩 样本间距离在低维空间保持 算法 由距离矩阵 $D$ 求内积矩阵: $b_{ij}=-\\frac12(D_{ij}^2-D_{i\\ast}^2-D_{\\ast j}^2+D_{\\ast\\ast}^2)$ 特征值分解:$B=V\\Lambda V^T$, 非零特征值构成 $\\Lambda_\\ast=diag\\lambda_1,\\lambda_2,\\cdots,\\lambda_d^\\ast$ 坐标$Z=\\Lambda_\\ast^{\\frac12}V_\\ast^T$, 可提前 $d$ 个最大特征值 PCA (Principal Component Analysis) 最近重构性：样本点到这个超平面距离足够近 最大可分性：样本点在这个超平面上的投影尽可能的分开\\[\\begin{equation}\\begin{aligned}\\max\\mathrm{tr}(W^TX&amp;amp;X^TW) \\\\s.t. \\ W^TW&amp;amp; =1 \\\\\\end{aligned}\\end{equation}\\] 算法 中心化 计算协方差矩阵 $XX^T$ 并特征值分解 取最大 $d$ 个特征向量为投影矩阵 PCA: 最佳描述特征 LDA: 最佳分类特征非线性降维核化线性降维 KPCA KLDA流行学习Isomap 只考虑局部距离 算法 最短路径算法求出任意两点距离 带入 MDS LLE (Locally Linear Embedding 局部线性嵌入) 只考虑领域内样本间的线性关系，在低维空间重构权值 算法 确定每个点的 $k$ 近邻 $Q_i$ 根据下式求出 $\\omega_{ij},j\\in Q_i$,且 $\\omega_{ij}=0$ if $j\\notin Q_i$\\[\\begin{equation}\\begin{aligned}\\min_{\\omega_1,\\omega_2,\\cdots,\\omega_m}\\sum_{i=1}^m\\Vert x_i&amp;amp; \\sum_{i\\in Q_i}\\omega_{ij}x_j\\Vert_2^2 \\\\s.t.\\sum_{j\\in Q_i}\\omega_{ij}&amp;amp; =1 \\\\\\end{aligned}\\end{equation}\\] 对 $M=(I-W)^T(I-W)$ 特征值分解，最小的 $d’$ 个特征值为投影 $Z$ " }, { "title": "ML Clustering", "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Clustering/", "categories": "2021, 08", "tags": "notes, Machine Learning", "date": "2021-08-04 15:00:00 +0800", "snippet": "性能度量 性能度量，有效性指标 validity index 外部指标：与某个参考模型比较 簇划分：$\\mathcal{C}=C_1,C_2,\\cdots,C_k$，参考模型簇划分 $\\mathcal{C^}={C_1^,C_2^,\\cdots,C_s^},\\lambda,\\lambda^*$ 分别为两者的簇标记向量，定义 $a=\\vert\\text{SS}\\vert,\\text{SS}={(x_i,x_j)\\ \\vert\\ \\lambda_i=\\lambda_j,\\lambda_i^=\\lambda^_j,i\\lt j}$ $b=\\vert\\text{SD}\\vert,\\text{SD}={(x_i,x_j)\\ \\vert\\ \\lambda_i=\\lambda_j,\\lambda_i^\\neq\\lambda^_j,i\\lt j}$ $c=\\vert\\text{DS}\\vert,\\text{DS}={(x_i,x_j)\\ \\vert\\ \\lambda_i\\neq\\lambda_j,\\lambda_i^=\\lambda^_j,i\\lt j}$ $d=\\vert\\text{DD}\\vert,\\text{DD}={(x_i,x_j)\\ \\vert\\ \\lambda_i\\neq\\lambda_j,\\lambda_i^\\neq\\lambda^_j,i\\lt j}$ $a+b+c+d=\\frac{m(m-1)}2$ JC (Jaccard Coefficent) $\\text{JC}=\\frac a{a+b+c}$ FMI (Fowlkes and Mallows Index) $\\text{FMI}=\\sqrt{\\frac a{a+b}\\cdot\\frac a{a+c}}$ RI (Rand Index) $\\text{RI}=\\frac{2(a+d)}{m(m-1)}$ 内部指标 簇划分: $\\mathcal{C}=C_1,C_2,\\cdots,C_k$ $\\text{avg}(C)=\\frac 2{\\vert C\\vert(\\vert C\\vert-1)}\\sum_{1\\leq i\\lt j\\leq\\vert C\\vert}\\text{dist}(x_i,x_j)$ 簇内样本平均距离 $\\text{diam}(C)=\\max_{1\\leq i\\lt j\\leq\\vert C\\vert}\\text{dist}(x_i,x_j)$ 簇内样本间最远距离 $d_{min}(C_i,C_j)=\\min_{x_i\\in C_i,x_j\\in C_j}\\text{dist}(x_i,x_j)$ $d_{cen}=\\text{dist}(\\mu_i,\\mu_j)$ 簇$C_i$与簇$C_j$中心点的距离 DBI (Davies Bouldin Index) $\\text{DBI}=\\frac1k\\sum_{i=1}^k\\max_{j\\neq i}(\\frac{\\text{avg}(C_i)+\\text{avg}(C_j)}{d_{cen}(C_i,C_j)})$ 越小越好 DI (Dunn Index) $\\text{DI}=\\min_{1\\leq i\\leq k}(\\min_{j\\neq i}(\\frac{d_{\\text{min}(C_i,C_j)}}{\\max_{1\\leq l\\leq k}\\text{diam}(C_l)}))$ 越大越好 原型聚类 SOM: self-organizing mapsk-meansLearning Vector Quantization 学习向量量化 利用样本监督信息 每次迭代，每个样本对其最近的原型向量根据标记一致性做推动/吸引 每个原型向量 $p_i$ 定义了与之相关的一个区域 $R_i$，形成了对样本空间的 Voronoi tessellation高斯混合聚类 高斯混合分布：$p_M(x)=\\sum_{i=1}^h\\alpha_i\\cdotp(x\\vert \\mu_i.\\sum_i),\\ \\sum_{i=1}^k\\alpha_i=1$ EM 算法求解 $\\text{LL(D)}=\\ln(\\prod\\limits_{j=1}^mp_M(x_j))=\\sum_{j=1}^m\\ln(\\sum_{i=1}^k\\alpha_ip(x_j\\vert \\mu_i,\\sum_i))$ $\\text{E:}\\gamma_{ji}=p_M(z_j=i\\vert x_j)$ $\\text{M}$ $\\mu_i’=\\frac{\\sum_{j=1}^m\\gamma_{ji}x_j}{\\sum_{j=1}^m\\gamma_{ji}}$ 新均值向量 $\\sum_i’=\\frac{\\sum_{j=1}^m\\gamma_{ji}(x_j-\\mu_i’)(x_j-\\mu_i’)^T}{\\sum_{j=1}^m\\gamma_{ji}}$ 新协方差矩阵 $\\alpha_i’=\\frac{\\sum_{j=1}^m\\gamma_{ji}}{m}$ 新混合系数 密度聚类假设聚类结构能通过样本分布的紧密程度确定DBSCAN $\\epsilon$-邻域：$N_\\epsilon(x_j)={x_i\\in D\\vert \\text{dist}(x_i,x_j)\\leq\\epsilon}$ 样本集中与$x_j$的距离不大于$\\epsilon$ 的样本 核心对象：$x_j,\\ \\vert N_\\epsilon(x_j)\\vert\\ge\\text{MinPts}$ directly density-reachable: $x_j\\in N_\\epsilon(x_i)$且$x_i$ 为核心对象，则$x_j$由$x_i$密度可达（无对称性） density-reachable density-connected: $\\exists x_k,x_i,x_j$均由$x_k$密度可达 簇 为满足下列性质的最大集合 connectivity: $x_i\\in C,x_j\\in C$ 则 $x_i,x_j$ 密度相连 maximality: $x_i\\in C,x_j$ 由 $x_i$ 密度可达，则 $x_j\\in C$ 算法： 找出所有核心对象 对每个核心对象求 $X=x’\\in D\\vert x’$ 由 $x$ 密度可达 DIANAtop-down层次聚类自底向上或者自顶向下AGNESagglomerative nesting 自底向上 起初每个样本点为一个簇 不断合并最近两个簇 name d single-linkage 单链接 min complete-linkage 全链接 max average-linkage 均链接 avg " }, { "title": "ML EnsembleLearning", "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_EnsembleLearning/", "categories": "2021, 08", "tags": "notes, Machine Learning", "date": "2021-08-01 09:00:00 +0800", "snippet": "集成学习 个体学习器 同质：基学习器，基学习算法 异质：组件学习器 准确性，多样性 学习器结合可能带来的好处 统计：学习任务假设空间大，多个假设在训练集上达到同等性能，使用单学习器可能因误选而导致泛化性能不佳 计算：降低陷入糟糕局部极小点的风险 表示：某些学习任务的真实假设可能不在当前算法所考虑的假设空间中，使用多学习器可能学得较好的近似 序列化方法 Boosting Train a weak learner $h_t$ from distribution $D_t$ Evaluate the error $\\epsilon_t$ of $h_t$ $D_{t+1}=\\mathrm{Adjust_Distribution}(D_t,\\epsilon_t)$ $\\color{Bittersweet}{\\mathrm{Adaboost}}$ 加性模型 (additive model): $H(x)=\\sum_{t=1}^T\\alpha_th_t(x)$ exponential loss function: $l_{exp}(H\\vert D)=E_{x\\sim D}(e^{-f(x)H(x)})$ 指数损失函数最小化，分类错误率也将最小化（与 0/1 损失函数一致） 分类器权重更新公式: $\\alpha_t=\\frac12\\ln \\frac{1-\\epsilon_t}{\\epsilon_t}$并行化方法Bagging 采样 T 次，训练 T 个学习器，分类简单投票，回归简单平均 out-of-bag estimate: $H^{oob}(x)$ 为未使用 x 训练的基学习器在 x 上的预测 $H^{oob}(x)=\\arg_{y\\in Y}\\max\\sum_{t=1}^T[h_t(x)=y][x\\notin D_t]$ $\\epsilon^{oob}=\\frac1{\\vert D\\vert}\\sum_{(x,y)\\in D}[H^{oob}(x)\\neq y]$ 随机森林 Bagging + 在随机选择的 k 个属性中选择最优属性 推荐值 $k=\\log_2 d$ 结合策略平均法 简单平均法: $H(x)=\\frac1T\\sum_{i=1}^Th_i(x)$ 加权平均法: $H(x)= \\sum_{i=1}^T\\omega_ih_i(x)$投票法 绝对多数投票法 可能拒绝预测 相对多数投票法 加权投票法 $h(x)$ 输出不同 hard voting: 类标记投票 soft voting: 类概率投票 基学习器类型不同，其概率值不能直接进行比较 学习法 Stacking 初级学习器（个体学习器） 次级学习器（元学习器） BMA (贝叶斯模型平均)多样性误差-分歧分解 $E=\\bar{E}-\\bar{A}$ $h_i$ 的分歧：$A(h_i\\vert x)=(h_i(x)-H(x))^2$ 集成的分歧: $\\bar{A}(h_i\\vert x)=\\sum_{i=1}^T\\omega_iA(h_i\\vert x)$ $E(h_i\\vert x)=(f(x)-h_i(x))^2$ $\\bar{E}(h\\vert x)=\\sum_{i=1}^T\\omega_iE(h_i\\vert x)$ 多样性度量 预测结果列联表 (contingency table), $m=a+b+c+d$   $h_i=+1$ $h_i=-1$ $h_i=+1$ a c $h_j=-1$ b d 指标   不合度量(Disagreement measure) $\\mathrm{dis}_{ij}=\\frac{b+c}m$ 相关系数 $\\rho_{ij}=\\frac{ad-bc}{\\sqrt{(a+b)(a+c)(c+d)(b+d)}}$ $Q$-statistic $Q_{ij}=\\frac{ad-bc}{ad+bc}$ $\\kappa$-statistic $\\kappa=\\frac{p1-p2}{1-p2}$ $\\kappa$ 图 $\\kappa$-平均误差图 取得一致的概率：$p1=\\frac{a+d}m$ 偶然取得一致的概率：$p2=\\frac{(a+b)(a+c)(c+d)(b+d)}{m^2}$多样性增强 数据样本扰动 Bootstrap 对不稳定基学习器（决策树、神经网络）有效 输入属性扰动 random subspace算法 输出表述扰动 Flipping Output Output Smearing ECOC 算法参数扰动 负相关法 不同增强机制同时使用 " }, { "title": "ML Bayes", "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Bayes/", "categories": "2021, 07", "tags": "notes, Machine Learning", "date": "2021-07-25 15:00:00 +0800", "snippet": "Bayesian decision theory   定义 最小化分类错误率 loss $\\lambda_{ij}$ $[i=j]$ Expected loss $R(c_i\\vert x)=\\sum_{j=1}^N\\lambda_{ij}P(c_j\\vert x)$ $1-P(c\\vert x)$ Bayes optimal classifier $h^\\ast(x)=\\arg\\min_{c\\in Y}R(c\\vert x)$ $\\arg\\max_{c\\in Y}P(c\\vert x)$ Decision loss $R(h)=E_x(R(h(x)\\vert x))$ $P(h^\\ast(x)\\vert x)$ Bayes risk $1-R(h^\\ast)$ $1-P(h^\\ast(x)\\vert x)$ 贝叶斯定理\\[P(c\\vert x)=\\frac{P(x,c)}{P(x)}=\\frac{P(c)P(x\\vert c)}{\\int p(c)P(x\\vert c)dc}\\] 先验 prior: $P(c)$ evidence: $P(x)$ 类条件概率class-conditional probablity\\likelihood: $P(x\\vert c)$ class-conditional probability: $x$ likelihood: $\\theta,P(x\\vert c)(\\theta)$ $P(D_c\\vert\\theta_c)=\\prod_{x\\in D_c}P(x\\vert\\theta_c)$ $LL(\\theta_c)=logP(D_c\\vert\\theta_c)=\\sum_{x\\in D_c}logP(x\\vert\\theta_c)$ k近邻学习 lazy learning 最邻近分类器的泛化错误率不会超过贝叶斯最优分类器错误率的两倍朴素贝叶斯分类器 属性条件独立性假设：$P(x\\vert c)=\\prod_{i=1}^dP(x_i\\vert c)$ $h_{nb}=\\arg\\max_{c\\in Y}P(c)\\prod_{i=1}^dP(x_i\\vert c)$ $P(c)=\\frac{\\vert D_c\\vert}{\\vert D\\vert}$ $P(x_i\\vert c)=\\frac{\\vert D_{c,x_i}\\vert}{\\vert D_c\\vert}$ 拉普拉斯修正 $\\hat P(c)=\\frac{\\vert D\\vert+1}{\\vert D\\vert+N}$ N 为 D 中可能的类别 $\\hat P(x_i\\vert c)=\\frac{\\vert D_{c,x_i}\\vert+1}{\\vert D_c\\vert+N_i}$ $N_i$ 为第 i 个属性可能取值数 连续属性 $p(x_i\\vert c)\\sim N(\\mu_{c,i},\\sigma^2_{c,i})$ 半朴素贝叶斯分类器独依赖估计 (One-Dependent Estimator, ODE) $P(c\\vert x)\\propto P(c)\\prod_{i=1}^dP(x_i\\vert c,p(a_i))$SPODE (super-parent ODE) 假设所有属性都依赖于同一属性: $p(a_i)=x_t$AODE (Averaged One-Dependent Estimator) SPODE 的集成 $P(c\\vert x)\\propto\\sum_{i=1,\\vert D_{x_i}\\vert\\ge m’}^dP(c,x_i)\\prod_{j=1}^dP(x_j\\vert c,x_i)$TAN (Tree Augmented naive Bayes) 仅保留了强相关属性间的依赖性 基于最大带权生成树 算法 conditional mutual information: $I(x_i,x_j\\vert y)=\\sum_{x_i,x_j,c}log\\frac{P(x_i,x_j\\vert c)}{P(x_i\\vert c)P(x_j\\vert c)}$ 在已知类别情况下的相关性 在以属性为节点，互信息为边建完全图上构造最大带权生成树，挑选根节点，边置为有向 加入类别节点 y,增加 y 到每个属性的边 kDE (k-Dependent Estimator) 样本不足：高阶连个概率估计困难，需要的样本数指数级增加" }, { "title": "ML SVM", "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_SVM/", "categories": "2021, 07", "tags": "notes, Machine Learning", "date": "2021-07-20 20:00:00 +0800", "snippet": "SVM 基本型 划分超平面: $\\omega^Tx+b=0$ 点到超平面的距离: $\\frac{\\vert \\omega^Tx+b\\vert}{\\Vert \\omega\\Vert}$\\(\\begin{cases}\\omega^Tx+b \\ge y_i,&amp;amp; y_i=+1 \\\\\\omega^Tx+b \\le y_i,&amp;amp; y_i=-1 \\\\\\end{cases}\\) 支持向量（support vector）：使上式成立的样本点 间隔（margin）：两个异类支持向量到超平面的距离$\\frac{2}{\\Vert\\omega\\Vert}$ SVM基本型(Support Vector Machine) \\[\\begin{equation}\\begin{aligned}&amp;amp;\\min\\limits_{\\omega,b}\\frac12\\Vert\\omega\\Vert^2 \\\\&amp;amp;s.t. y_i(\\omega^Tx_i+b)\\ge1,\\qquad i=1,2,\\cdots,n \\\\\\end{aligned}\\end{equation}\\] 凸优化求解：复杂度与样本维度（等于权值 $\\omega$ 的维度）有关 对偶问题 复杂度与样本数量（等于拉格朗日算子 $\\alpha$ 的数量）有关 解的稀疏性：最终模型仅与支持向量有关 KKT条件导出 对偶问题的转化 step1：拉格朗日函数：$L(\\omega,b,\\alpha)$ step2： 对 $\\omega$ 和 $b$ 求导并令导数为0 $\\omega=\\sum_{i=1}^m{\\alpha_iy_ix_i}$ $\\sum_{i=1}^m\\alpha_iy_i=0$ step3：回代：\\[\\begin{equation}\\begin{split}\\max\\limits_{\\alpha}&amp;amp;\\sum_{i=0}^m\\alpha_i-\\frac12\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\\\s.t. &amp;amp;\\sum_{i=1}^m\\alpha_iy_i=0 \\\\&amp;amp;\\alpha_i\\ge0\\end{split}\\end{equation}\\]求解对偶问题 SMO(Sequential Minimal Optimization) 选取一对需要更新的变量 $\\alpha_i$ 和 $\\alpha_j$ 先选违背KKT条件最大的，再选使目标函数增长最快的 实际中启发式：选取两变量所对应样本之间间隔最大 固定其他参数，更新 $\\alpha_i$ 和 $\\alpha_j$ 核函数 $f(x)=\\omega^T\\phi(x)$ 核函数：$\\kappa(x_i,x_j)=\\langle\\phi(x_i),\\phi(x_j)\\rangle$ $\\kappa$ 为核函数 $\\iff$ 核矩阵 $\\Kappa$ 是半正定的 $\\kappa_1,\\kappa_2$ 为核函数，则以下为核函数 $\\gamma\\kappa_1+\\gamma\\kappa_2$ $\\kappa_1\\otimes\\kappa_2(x,z)=\\kappa_1(x,z)\\kappa_2(x,z)$ $\\kappa(x,z)=g(x)\\kappa_1(x,z)g(z)$ 常用核函数 $\\kappa(x_i,x_j)$ 线性核 $x_i^Tx_j$ 多项式核 $(x_i^Tx_j)$ 高斯核 $e^{-\\frac{\\Vert x_i-x_j\\Vert^2}{2\\sigma^2}}$ 拉普拉斯核 $e^{-\\frac{\\Vert x_i-x_j\\Vert}{\\sigma}}$ Sigmoid 核 $tanh(\\beta x_i^Tx_j+\\theta)$ 支持向量展式（利用对偶问题）： $f(x)=\\omega^T\\phi(x)+b=\\sum_{i=1}^m\\alpha_iy_i\\kappa(x,z)+b$ 软间隔 优化目标：$\\min_{\\omega,b}\\frac12\\Vert\\omega\\Vert^2+C\\sum_{i=1}^m\\xi_i$ 松弛变量 $\\xi_i=l(y_i(\\omega^Tx_i+b)-1)$ 原问题\\[\\begin{equation}\\begin{split}\\min\\limits_{\\omega,b}&amp;amp;\\frac12\\Vert\\omega\\Vert^2+C\\sum_{i=1}^m\\xi_i \\\\s.t. &amp;amp;y_i(\\omega^Tx_i+b)\\ge1-\\xi_i \\\\&amp;amp;\\xi \\ge0\\\\\\end{split}\\end{equation}\\] 对偶问题（损失函数为 hinge）\\[\\begin{equation}\\begin{aligned}\\max\\limits_\\alpha&amp;amp;\\sum_{i=1}^m\\alpha_i-\\frac12\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_j\\phi(x_i)^T\\phi(x_j) \\\\s.t. &amp;amp;\\sum_{i=1}^m\\alpha_iy_i=0 \\\\&amp;amp;C\\ge\\alpha_i\\ge0 \\\\\\end{aligned}\\end{equation}\\] 损失函数 $l(z)$ Remark 0/1 1, z&amp;lt;0 不易求解 hinge max(0,1-z) 保持稀疏性 exp $e^{-z}$   log $log(1+e^{-z})$ 几率回归模型，无稀疏性 一般形式：$\\min_f\\Omega(f)+C\\sum_{i=1}^ml(f(x_i),y_i)$ 结构风险：$\\Omega(f)$ 经验风险：$\\sum_{i=1}^ml(f(x_i),y_i)$ ，模型与训练数据契合程度 支持向量回归 SVR $\\min_{\\omega,b}\\frac12\\Vert\\omega\\Vert^2+C\\sum_{i=1}^ml_\\epsilon(f(x_i)-y_i)$ 落入中间 2$\\epsilon$ 间隔带的样本不计算损失\\(\\begin{cases}0,&amp;amp;\\vert z\\vert\\le\\epsilon \\\\\\vert z\\vert-\\epsilon,&amp;amp; otherwise\\end{cases}\\) 核方法 表示定理：对于任意的单调递增函数 $\\Omega$ 和任意非负损失函数 $l$，优化问题\\(\\min\\limits_{h\\in\\mathbb{H}}F(h)=\\Omega(\\Vert h\\Vert_\\mathbb{H})+l(h(x_1),h(x_2,\\cdots,h(x_m))\\)的解总可以写成 $h^\\ast(x)=\\sum_{i=1}^m\\alpha_i\\kappa(x,x_i)$ KLDA KPCA" }, { "title": "ML DecisionTree", "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_DecisionTree/", "categories": "2021, 05", "tags": "notes, Machine Learning", "date": "2021-05-23 15:00:00 +0800", "snippet": "决策树基本算法 当前节点包含样本全部同类：标记为某类 当前样本属性值为空/取值相同：标记为最多一类 属性划分选择 为属性每个值分配一个结点继续执行算法 若其属性值上为空则标记为当前最多一类 递归过程，分治划分选择 指标名称 指标 辅助函数 例子 remark 信息增益(Info gain) $\\mathrm{Gain}(D,a)=\\mathrm{Ent}(D)-\\sum_{v=1}^V\\frac{\\vert D^v\\vert}{\\vert D\\vert}\\mathrm{Ent}(D^v)$ 信息熵$\\mathrm{Ent}(D)=-\\sum_{k=1}^{\\vert y\\vert}p_klog_2p_k$ ID3 对可取值数目较多的属性有偏好 Gain ratio $\\mathrm{Gain_ratio}(D,a)=\\frac{\\mathrm{Gain}(D,a)}{\\mathrm{IV}(a)}$ 固有值$\\mathrm{IV}(a)=-\\sum_{v=1}^V\\frac{\\vert D^v\\vert}{\\vert D\\vert}log_2\\frac{\\vert D^v\\vert}{\\vert D\\vert}$ C4.5 从候选划分中找出信息增益高于平均水平的属性，再从中选择增益率最高的 Gini ratio $\\mathrm{Gini_index}(D,a)=\\sum_{v=1}^V\\frac{\\vert D^v\\vert}{\\vert D\\vert}\\mathrm{Gini}(D^v)$ $\\mathrm{Gini}(D)=1-\\sum_{k=1}^{\\vert Y\\vert}p_k^2$ CART Gini指数为随机抽取两个样本类别标记不一致的概率，越小纯度越高 剪枝处理 方法 指标 过拟合 欠拟合 训练时间 预剪枝 precision 降低过拟合风险 有欠拟合风险 较小 后剪枝 precision 降低过拟合风险 欠拟合风险小 较长 连续与缺失值 连续属性离散化(二分法) $T_a=\\frac{a^i+a^{i+1}}2 \\vert\\ 1\\le i\\le n-1$ 缺失值 $\\tilde{D}:D$在属性 $a$ 上没有缺失值的样本子集 $\\tilde{D}^v:D$中属性 $a$ 上取值为 $a^v$ 的样本子集 $\\tilde{D}_k:\\tilde{D}$ 类别中为 $k$ 的样本子集 $\\omega_x:$ 每个样本的权重 $\\rho=\\frac{\\sum_{x\\in\\tilde{D}}\\omega_x}{\\sum_{x\\in D}\\omega_x}$ 对属性 $a$ ，$\\rho$ 表示无缺失样本所占比例 $\\tilde{p_k}=\\frac{\\sum_{x\\in\\tilde{D}k}\\omega_x}{\\sum{x\\in D}\\omega_x}$ 对属性 $a$，$\\tilde{p_k}$ 表示无缺失值样本中第 $k$ 类的占比 $\\tilde{r_v}=\\frac{\\sum_{x\\in\\tilde{D}^v}\\omega_x}{\\sum_{x\\in D}\\omega_x}$ 其中 $\\tilde{r_v}$ 表示无缺失值样本中在属性 $a$ 上取值 $a^v$ 的占比 $\\mathrm{Ent}(\\tilde{D})=-\\sum_{k=1}^{\\vert Y\\vert}\\tilde{p_k}log_2\\tilde{p_k}$ $\\mathrm{Gain}(D,a)=\\rho\\ast\\mathrm{Gain}(\\tilde{D},a)=\\rho\\ast\\Bigl(\\mathrm{Ent}(\\tilde{D})-\\sum_{v=1}^V\\tilde{r_v}\\mathrm{Ent}(\\tilde{D^v})\\Bigr)$ 多变量决策树 每个非叶节点用 $\\sum_{i=1}^d\\omega_ia_i=t$ 的线性分类器，是对属性的线性组合进行测试。 特征预处理" }, { "title": "ML LinearModel", "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_LinearModel/", "categories": "2021, 05", "tags": "notes, Machine Learning", "date": "2021-05-17 15:00:00 +0800", "snippet": "多元线性回归 $f(x)=\\omega^T\\omega+b$ 决策平面：$f(x;\\omega)=0$ 有向距离: $\\gamma=\\frac{f(x;\\omega)}{\\vert\\omega\\vert}$ 最小二乘法(使均方误差最小) $\\hat\\omega^*=arg\\ min_{\\hat\\omega}(y-X\\hat\\omega)^T(y-X\\hat\\omega)=(X^TX)^{-1}X^Ty$ 广义线性模型：$y=g^{-1}(\\omega^Tx+b$)，其中单调可微函数 $g(\\cdot)$ 称为联系函数对数几率回归 单位跃阶函数 (Heaviside/unit-step function)：理想但不连续\\[y=\\begin{cases}0, &amp;amp; z&amp;lt;0 \\\\0.5, &amp;amp; z=0 \\\\1, &amp;amp; z&amp;gt;0\\end{cases}\\] 对数几率函数 (logistic/Sigmoid function) $g=ln\\frac{y}{1-y}$ 几率：$\\frac y{1-y}$ ，反映了 $x$ 作为正例的相对可调性 $g^{-1}=S(x)=\\frac 1{1+e^{-x}}$ $S(x)’=s(x)(1-S(x))$ 对数几率回归：用线性模型逼近真实标记的几率 $ln \\frac{p1}{p0}=\\hat x\\beta=(x,1)(\\omega;b)$ 二分类：$y_ia+(1-y_i)b=a^{y_i}b^{1-y_i}$ Maxmimum likelihood method $l(\\beta)=\\sum_{i=1}^mlmp(y_i\\vert x_i;\\beta_i)=\\sum_{i=1}^my_iln(g(\\hat x_i\\beta)+(1-y_i)ln(1-g(\\hat x_i\\beta)))$ $l’=\\sum_{i=1}^m(y_i-g(\\hat x\\beta))\\hat x_i^T=X^T(Y-g(\\beta^TX))$ $l’’=\\sum_{i=1}^m\\hat x_i\\hat x_i^Tp_1(x_i^T;\\beta)(1-p_1(\\hat x_i;\\beta))$ 交叉熵做损失函数梯度下降 梯度下降: $\\theta_{i+1}=\\theta_i-\\alpha\\frac{\\partial L}{\\partial\\theta}$Softmax回归 $p(y=c\\vert x)=softmax(\\omega^T_cx)=\\frac{exp\\left(\\omega_c^Tx\\right)}{\\sum^C_{c’=1}exp(\\omega^T_{c’}x)}=\\frac{exp(W^Tx)}{1_C^Texp(W^Tx)}$LDA 给定训练集数据，设法将样例投影到一条直线上，使得同类样例投影点尽可能接近，异类投影点尽可能远 协方差矩阵: $\\sum=\\frac 1{n-1}(X-\\mu I)(X-\\mu I)^T$ $\\sum_{ij}=\\sigma(x_i,x_j)$ 投影后: $\\omega^T\\sum\\omega$   两类 一般 within-class scatter matrix $S_\\omega=\\sum_0+\\sum_1$ $S_m=\\sum_{i=1}^N \\sum_i$ Between-calss scatter matrix $S_b=(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T$ $S_b=\\sum_{i=1}^Nm_i(\\mu_i-\\mu)(\\mu_i-\\mu)^T$ 全局散度矩阵 $S_t=S_b+S_\\omega$ $\\sum_{i=1}^m(x_i-\\mu)(x_i-\\mu)^T$ 优化目标 $max_\\omega\\frac{\\omega^TS_b\\omega}{\\omega^TS_w\\omega}$ $max_W\\frac{tr(W^TS_bW)}{tr(W^TS_wW)}$ 闭式解 $w=S_w^{-1}(\\mu_0-\\mu_1)$ $S_w^{-1}S_b$前$k$大广义特征向量 ​" }, { "title": "ML Introduction", "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Introduction/", "categories": "2021, 05", "tags": "notes, Machine Learning", "date": "2021-05-14 15:00:00 +0800", "snippet": "机器学习 机器学习要素 模型 学习准则 优化算法 数据集：$D=x_1,x_2,…,x_m$ 通常假设全体样本服从一个未知分布$\\mathcal{D}$,且采样$i.i.d$ 归纳偏好 No Free Lunch Theorem Occam’s Razor Ugly Ducking Theorem all vectors are assumed to be column vectors $N$ number of input, $p$ number of features 训练集$X_{N_\\times p}$ $i$-th row $x_i^T$:length $p$ $j$-th column $x_j$ :length N input vector: $X_{p\\times 1}$ $X^T=(X_1,X_2,\\cdots,X_p),X_i $ is a scalar $y_{N_\\times 1}$ $Y \\in R$ $Y_{N_\\times 1}$, each row has one 1 $(X_1,X_2)$: 行并列 $(X_1^T,X_2^T)$: 列并列 偏差-方差分解： $E(f;D)=bias^2(x)+var(x)+c^2=(\\overline f(x)-y)^2+E_D((f(x;D)-\\overline f(x))^2)+E_D((y_D-y)^2)$ 评估方法 留出法 cross validation 将数据集分层采样划分为 $k$ 个大小相似的互斥子集，每次用 $k-1$ 个子集的并集作为训练集，余下的子集作为行为测试集，最终返回 $k$ 个测试结果的均值。 $k$ 最常用的取值是 10 LOO(leave-one-out): k=1. bootstrapping: 样本不被选到的概率=$\\lim_{m \\to \\infty}(1-\\frac {1}{m})^m=\\frac{1}{e}=0.368.$模型评估指标 T,F: 分类正确与否 P,N: 预测结果Regression 均方误差:$E(f;D)=\\frac{1}{m}\\sum_{i-1}^m(f(x_i)-y_i)^2$Classification 指标   accuracy $\\frac{TP+TN}{m}$ precision $\\frac{TP}{TP+FP}$ recall $\\frac{TP}{TP+FN}$ macro-P $\\frac{1}{n}\\sum _{i=1}^n P_i$ macro-R $\\frac{1}{n}\\sum _{i=1}^n R_i$ micro-P $\\frac{TP}{TP+FP}$ micro-R $\\frac{TP}{TP+FN}$ 平衡点(BEP) P-R曲线上 P=R 的点 F1 $\\frac{2PR}{P+R}$调和平均更重视较小值 $F_\\beta$ $\\frac{1}{F_\\beta}=\\frac{1}{1+\\beta ^2}(\\frac{1}{P}+\\frac{\\beta^2}{R}),\\beta&amp;gt;1,R $ counts more TPR R FPR $\\frac{FP}{TN+FP}$ ROC TPR-FPR 图 (Receiver Operating Characteristic) AUC (Area Under Curve) $\\frac{1}{2}\\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_{i+1}+y_i)$ FPR 1-TPR cost curve 横轴为正例概率代价，纵轴为归一化代价 Ranking Loss $l$=1-AUC Ranking One query $r\\to$ One permutation $\\pi$ One query $r\\to$ retrieved documents $R(r)$ precision: precision= $P$ =$\\frac{\\vert C(r)\\cap R(r)\\vert}{\\vert R(r)\\vert}$ precision$@k$ $\\vert R(r)\\vert=k$ $\\frac{\\sum_{i&amp;lt;k}l(\\pi(t))}k$ recall: $recall =\\frac{\\vert C(r)\\cap R(r)\\vert}{\\vert R(r)\\vert}$ recall$@k$,$\\vert R(r)\\vert=k$ AP (AveP, Average precision): AP $=\\frac{\\sum_{k=1}^{\\vert C(r)\\vert}P(@k)}{\\vert R(r)\\vert}$ mAP (Mean average precision): MAP = $=\\frac{\\sum_{q=1}^{Q}AP(q)}Q$ CG (Cumulative Gain): CG$@k=\\sum_{i=1}^k rel(i)$ DCG (Discounted cumulative gain): DCG$@k=\\sum_{i=1}^k rel(i)\\eta(i)$ 折扣因子$\\eta(i)$:$\\frac 1{log(i+1)}$ IDCG (Ideal DCG): $IDCG@k=max_\\pi DCG@k(\\pi)$ nDCG (Normalized DCG): $IDCG@k=\\frac{DCG_p}{IDCG_p}$ RR (reciprocal rank): $rank_i$第一个正确答案的排名 MRR (Mean reciprocal rank): $MRR=\\frac 1{\\vert Q\\vert}\\sum_{i=1}^{\\vert Q\\vert}\\frac 1{rank_i}$ Cascade Models: 用户在位置 k 需求满足的概率$PP(k)=\\prod_{i=1}^{k-1}(1-R(i))R(k)$ 该文档满足需求的概率: $R(t)=\\frac{2^{rel(t)}-1}{2^{l_{max}}}$ ERR (Expected reciprocal rank): 用户需求满足时停止位置的倒数的期望 $ERR=\\sum_{r=1}^n\\frac 1rPP_r$多分类 OvO：储存开销于测试时间偏大 OvR：训练时间偏大 MvM： Error Correcting Output Codes(ECOC) 编码：二元码，三元码 解码：将距离最小的编码所对应的类别作为预测结果 argmax: $y=arg max_{c=1}^C f_c(x;w_c)$类别不平等 欠采样 过采样 阈值移动：$\\frac{y}{1-y} &amp;gt; \\frac{m^+}{m^-}$假设检验 二项检验 二项分布：$P(\\hat \\epsilon;\\epsilon)=\\binom{m}{\\hat\\epsilon m}\\epsilon^{\\hat\\epsilon m}(1-\\epsilon)^{m-\\hat\\epsilon m}$ 假设：$\\epsilon \\le \\epsilon_0$ 临界值$\\bar\\epsilon=max\\ \\epsilon$ s.t. $\\sum_{i=\\epsilon_0 \\times m+1}^m \\binom{m}{i}\\epsilon^i(1-\\epsilon)^{m-i}$ t 检验 检验值：$\\tau_t=\\frac{\\sqrt{k}(\\mu-\\epsilon_0)}{\\sigma}$ 交叉验证 t 检验 差值：$\\Delta_i=\\epsilon_i^A-\\epsilon_i^B$ $\\tau_t=\\vert \\frac{\\sqrt{k}(\\mu-\\epsilon_0)}{\\sigma}\\vert$ 假设检验前提：测试错误率为泛化错误率独立采样 5$\\times$2交叉验证（为缓解不同训练轮次训练集的重叠问题，5次2折交叉验证） McNemar 检验 $e_{ij}:i$ 指示算法 $A$ 预测正确与否，$j$ 指示算法 $B$ 预测正确与否 假设：$e_{01}=e_{10}$ $\\vert e_{01}-e_{10}\\vert \\sim N(1,e_{01}+e_{10})$ 统计检验量：$\\tau=\\frac{(\\vert e_{01}-e_{10}\\vert -1)^2}{ e_{01}-e_{10}}\\sim \\chi^2(1)$ Friedman 检验：多个数据集对多个算法比较 根据测试性能赋予序值，求得平均序值 假设：算法性能全部相同 $\\tau_\\chi^2=\\frac{12N}{k(k+1)}(\\sum_{i=1}^kr_i^2-\\frac{k(k+1)^2}{4})$,当 k, N 较大时，服从$\\chi^2(k-1)$ 分布 $\\tau_F=\\frac{(N-1)\\tau_\\chi^2}{n(K-1)-\\tau_\\chi^2}\\sim F(k-1,(k-1)(N-1))$ Nemenyi 检验：算法性能全部相同假设被拒绝后进一步区分各算法 CD=$q_\\alpha \\sqrt{\\frac{k(k+1)}{6N}},q_\\alpha$ 为 Tukey 分布临界值 若两个算法平均序值之差超出临界值域 CD，则以相应置信度拒绝两个算法性能相同 " }, { "title": "oj输入输出", "url": "/posts/oj%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/", "categories": "2021, 05", "tags": "writing", "date": "2021-05-10 16:00:00 +0800", "snippet": "A+B输入描述：输入数据有多组，每组表示一组输入数据每行不定有n个整数，空格隔开输出描述：魅族输出求和结果代码#include &amp;lt;bits/stdc++.h&amp;gt;using namespace std;int main(){ int n; int sum=0; while(cin&amp;gt;&amp;gt;n){ sum+=n; if(cin.get()==&#39;\\n&#39;){ cout&amp;lt;&amp;lt;sum&amp;lt;&amp;lt;endl; sum=0; } } return 0;}cin、cin.get()、cin.getline()、getline()的区别cin»输入数据，在遇到结束符2(space、tab、enter)就结束，且不将结束符存入标量在。注意最后一个enter也在缓冲区中，例如：void main(){ char ch1,ch2; cin&amp;gt;&amp;gt;ch1; cout&amp;lt;&amp;lt;ch1&amp;lt;&amp;lt;endl; cin.get(ch2); cout&amp;lt;&amp;lt;int(ch2)&amp;lt;&amp;lt;endl; //输出10，因为enter的asii码为10}cin.get(字符数组名，接受长度，结束符)结束符意味着遇到该符号结束读取字符串，默认为enter，读取的字符个数最多为(长度-1)，因为最后一个为’\\0’。cin.get() 遇到结束符停止读取，但是并不会讲解舒服从缓冲区丢弃。cin.getline(字符数组名，接受长度，结束符)于cin.get()极为类似。不同的是cin.getline()当输入超长时，会引起cin函数的错误，后面的cin操作将不再执行。cin.get()每次读取一整行并把由Enter键生成的换行符留在输入队列中，然而cin.getline()每次读取一整行并把由Enter键生成的换行符抛弃getline(istream is,string str,结束符)geline()于前面的有很大差别，不会遇到tab、space、enter结束读取，会丢弃最后一个换行符。字符串第一种情况输入描述多个测试用例，每个测试用例一行每行通过空格隔开，有n个字符a c bbf ddddnowcoder输出描述对于每个输入阳历输出排序过的字符串，空格隔开a bb cdddd fnowcoder代码#include &amp;lt;bits/stdc++.h&amp;gt;using namespace std;int main(){ string s; vector&amp;lt;string&amp;gt; str; while(cin&amp;gt;&amp;gt;s){ str.push_back(s); if(cin.get()==&#39;\\n&#39;){ sort(str.begin(),str.end()); for(auto c:str) cout&amp;lt;&amp;lt;c&amp;lt;&amp;lt;&#39; &#39;; cout&amp;lt;&amp;lt;endl; str.clear(); } } return 0;}第二种情况输入描述多个测试用例，每个测试用例一行每行通过，隔开，有n个字符a,c,bbf,ddddnowcoder输出描述输出每组排序后的字符串，用，隔开，无结尾空格a,bb,cdddd,fnowcoder代码#include &amp;lt;bits/stdc++.h&amp;gt;using namespace std;int main(){ string line; while(getline(cin,line)){ istringstream is(line); string str; vector&amp;lt;string&amp;gt; s; while(getline(is,str,&#39;,&#39;)){ //while判断的实际上是is是否是有效的输入流 s.push_back(str); } sort(s.begin(),s.end()); for(int i=0;i&amp;lt;s.size()-1;i++){ cout&amp;lt;&amp;lt;c&amp;lt;&amp;lt;&quot;,&quot;; } cout&amp;lt;&amp;lt;s[i]&amp;lt;&amp;lt;endl; } return 0;}" }, { "title": "暑期实习笔试", "url": "/posts/%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AF%95/", "categories": "2021, 05", "tags": "writing", "date": "2021-05-08 15:00:00 +0800", "snippet": "网易开发岗题目描述给定多个区间，计算让这些区间互不重叠所需要移除区间的最少个数。起止相连不算重叠。代码int eraseOverlapIntervals(vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt;&amp;amp; intervals) { if (intervals.empty()) { return 0; } int n = intervals.size(); sort(intervals.begin(), intervals.end(), [](vector&amp;lt;int&amp;gt;&amp;amp; a, vector&amp;lt;int&amp;gt;&amp;amp; b) { return a[1] &amp;lt; b[1]; }); int removed = 0, prev = intervals[0][1]; for (int i = 1; i &amp;lt; n; ++i) { if (intervals[i][0] &amp;lt; prev) { ++removed; } else { prev = intervals[i][1]; } } return removed;}intervals.size()指的是行大小，intervals[i].size()指的是列大小。题目描述给定两个字符串S和T，求S中包含T所有字符的最短连续子字符串的长度，同时要求时间复杂度不超过O(n)代码其中chars 表示目前每个字符缺少的数量，flag 表示每个字符是否在 T 中存在。string minWindow(string S, string S){ vector&amp;lt;int&amp;gt; chars(128,0); vector&amp;lt;bool&amp;gt; flag(128,false); for(int i=0;i&amp;lt;T.size();i++){ flag[T[i]]=true; ++chars[T[i]]; } int cnt=0,l=0,min_l=0,min_size=S.size()+1; for(int r=0;r&amp;lt;S.size();r++){ if(flag[S[r]]){ if(--chars[S[r]]&amp;gt;=0){ ++cnt; } while(cnt == T.size()){ if(r-l+1&amp;lt;min.size()){ min_l=1; min_size=r-l+1; } if(flag[Sl] &amp;amp;&amp;amp; ++chars[S[l]]&amp;gt;0) --cnt; ++l; } } } return min_size &amp;gt; S.size()?&quot;&quot;:S.substr(min_l,min_size);}题目描述给定两个字符串S和T，求S中包含T所有字符的最短连续子字符串的长度，同时要求时间复杂度不超过O(n)代码其中chars 表示目前每个字符缺少的数量，flag 表示每个字符是否在 T 中存在。string minWindow(string S, string S){ vector&amp;lt;int&amp;gt; chars(128,0); vector&amp;lt;bool&amp;gt; flag(128,false); for(int i=0;i&amp;lt;T.size();i++){ flag[T[i]]=true; ++chars[T[i]]; } int cnt=0,l=0,min_l=0,min_size=S.size()+1; for(int r=0;r&amp;lt;S.size();r++){ if(flag[S[r]]){ if(--chars[S[r]]&amp;gt;=0){ ++cnt; } while(cnt == T.size()){ if(r-l+1&amp;lt;min.size()){ min_l=1; min_size=r-l+1; } if(flag[Sl] &amp;amp;&amp;amp; ++chars[S[l]]&amp;gt;0) --cnt; ++l; } } } return min_size &amp;gt; S.size()?&quot;&quot;:S.substr(min_l,min_size);}判断回文子字符串Palindromic Substring (中心扩散)int countSubstrings(string s){ it count=0; for(int i=0;i&amp;lt;s.length();i++){ count+=extendSubstrings(s,i,i;); //奇数长度 count+=extendSubstrings(s,i,i+1); //偶数长度 } return count;}int extendSubstrings(string s,int l,int r){ int count=0; while(l&amp;gt;=0&amp;amp;&amp;amp;r&amp;lt;s.length()&amp;amp;&amp;amp;s[l]==s[r]){ --l; ++r; ++count; } return count;}str.substr(pos,n) 取子字符串。remove(str.begin(),str.end(),val) 将字符串str中val的字符提取到字符串后部，并返回新的尾部，remove不是真的删除。str.erase(remove(str.begin(),str.end(),val),str.end()) 删除字符串str中指定val的字符。字节data算法岗题目描述输入字符串B和数字n，以逗号，隔开，定义特殊子串为B中顺序不变、长度为n的子串，求B一共有多少个子串。输入word,2输出6解释：wo、wr、wd、or、od、rd代码#include &amp;lt;bits/stdc++.h&amp;gt;using namespace std;int main(){ }面试常问 进程和线程的概念、区别和联系，线程通信方式。 HTTP2.0和1.0的区别。" }, { "title": "Anaconda3安装Pytorch", "url": "/posts/Anaconda%E5%AE%89%E8%A3%85Pytorch/", "categories": "2021, 05", "tags": "tutorial", "date": "2021-05-06 17:00:00 +0800", "snippet": "引言在Win10下，通过在Anaconda3安装Pytorch，版本号为1.8.1。需要使用NVIDIA显卡，安装CUDA 11.1 和cuDNN。具体包含Anaconda3下载安装，同时给了常见出错问题急解决方法。Anaconda3安装我是在 anaconda3 官方网站上下载的，然后从上往下下载最新的版本。下载好之后要注意安装问题，首先所针对用户，我选择的是针对该电脑的全部用户 Just Me For all users之后是环境变量勾选，我没有勾选，系统推荐也是不勾选，之后再系统环境变量中添加就行，官方给的理由是会干扰到其他软件，没添加的话就没办法在cmd中运行。安装完成之后可以在这个博客完成镜像源的切换，在C:\\Users\\User_name\\.condarc文件中更改，例如，清华源可以channels: - defaultsshow_channel_urls: truechannel_alias: https://mirrors.tuna.tsinghua.edu.cn/anacondadefault_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud我用清华源的时候出现各种问题，头大，删了anaconda重装后没配置.condarc文件，才顺利，源问题还是看其它blog吧。创建conda环境在对应anaconda目录下启动终端conda create -n py38 python=3.8然后conda activate py38进入环境。。CUDA下载链接，cuDNN下载链接将cuDNN下载后的压缩包解压，将bin、include、lib文件夹复制到CUDA对应目录。不会产生覆盖，因为是新文件。安装pytorch安装完CUDA和cuDNN后再切入py38环境，输入conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia -c conda-forge因为是11.1版本的CUDA，官方文档上说要加上conda-forge。如果网速不稳定，或者镜像源不稳定，安装时可能会出现CondaHTTPError，重新下载几次也不行，就用迅雷下载，再进行安装。根据镜像源提供的网址，将未下载成功的文件离线下载，之后移动到Anaconda3安装目录的pkgs文件夹里，替换文件。然后打开pkgs文件里一个url命名的txt文本文件，将之前复制的下载链接复制到该txt中。virtualenv虚拟环境，创建不同版本的python虚拟环境" }, { "title": "Jekyll搭建Gitpages", "url": "/posts/Jekyll%E6%90%AD%E5%BB%BAgitpages/", "categories": "2021, 05", "tags": "tutorial", "date": "2021-05-04 21:20:30 +0800", "snippet": "GitPages + Jekyll我使用的是Jekyll-theme-chirpy模板，直接拿来。因为有Git的基础，用起来还是比较方便的。另外，gitpages本身的主题比较少而且不是很好看，所以用了第三方的，第三方还是很多的。这个模板需要注意的就是，要另外创建新branch，注意./github/workflows/pages-deply.yml、tools/deploy.sh文件的branch名，还有基础配置文件_config.yml的更改。Markdown基础在写blog的时候，也学了一点markdown基础。这是加粗，斜体，链接百度，复选框 复选插入代码int x，插入代码块for(int x;x&amp;lt;n;x++){ printf(&quot;%d&quot;,x);}插入图片 一 二" } ]
