

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://localhost:4000/</id>
  <title>Jiyong Rao</title>
  <subtitle>A minimal, portfolio, sidebar, bootstrap Jekyll theme with responsive web design and focuses on text presentation.</subtitle>
  <updated>2021-10-04T14:55:53+08:00</updated>
  <author>
    <name>raojiyong</name>
    <uri>http://localhost:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://localhost:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator>
  <rights> © 2021 raojiyong </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>Notes on backpropagation</title>
    <link href="http://localhost:4000/posts/Backpropagation/" rel="alternate" type="text/html" title="Notes on backpropagation" />
    <published>2021-09-03T19:00:00+08:00</published>
  
    <updated>2021-09-08T14:29:14+08:00</updated>
  
    <id>http://localhost:4000/posts/Backpropagation/</id>
    <content src="http://localhost:4000/posts/Backpropagation/" />
    <author>
      <name>raojiyong</name>
    </author>

  
    
    <category term="2021" />
    
    <category term="09" />
    
  

  
    <summary>
      





      Some common loss function backpropagation

1. SVM

hinge loss

\[L_i=\sum_{j\neq y_i}\max(0,\omega_j^Tx_i-\omega_{y_i}^Tx_i+\Delta)\]

SVM full loss:

\[L=\frac1N\sum_i\sum_{j\neq y_i}[\max(0,f(x_i,W)_j-f(x_i;W)_{y_i}+\Delta)]+\lambda\sum_k\sum_lW_{k,l}^2 \\\]

where $W_{k,l}^2$ is all square elements of $W$.

its backpropagation process is below:

\[\begin{align}
\text{dW}_j&amp;amp;=\frac{X_i}{N}...
    </summary>
  

  </entry>

  
  <entry>
    <title>ML Distance</title>
    <link href="http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Distance/" rel="alternate" type="text/html" title="ML Distance" />
    <published>2021-08-27T19:00:00+08:00</published>
  
    <updated>2021-09-07T23:13:03+08:00</updated>
  
    <id>http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Distance/</id>
    <content src="http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Distance/" />
    <author>
      <name>raojiyong</name>
    </author>

  
    
    <category term="2021" />
    
    <category term="08" />
    
  

  
    <summary>
      





      距离


  正定性
  对称性
  三角不等式


有序距离


  闵科夫斯基距离: $l=(\sum_{i=1}^n\vert x_i-y_i\vert^p)^\frac1p$
    
      切比雪夫距离: $l_{\infty}=\max_{i=1}^n\vert x_i-y_i\vert$
      欧几里得距离: $l_2=\sqrt{\sum_{i=1}^n\vert x_i-y_i\vert^2}$
      曼哈顿距离: $l_1=\sum_{i=1}^n\vert x_i-y_i\vert$
      加权闵科夫斯基距离: $l=(\sum_{i=1}^n\omega_i\vert x_i-y_i\vert^p)^\frac1p$
    
  
  马氏距离: $d(\vec{x},\vec{y})=\sqrt{(\vec{x}-\vec{y})^...
    </summary>
  

  </entry>

  
  <entry>
    <title>ML Dimension Reduction</title>
    <link href="http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_DimensionReduction/" rel="alternate" type="text/html" title="ML Dimension Reduction" />
    <published>2021-08-17T15:00:00+08:00</published>
  
    <updated>2021-08-17T15:00:00+08:00</updated>
  
    <id>http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_DimensionReduction/</id>
    <content src="http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_DimensionReduction/" />
    <author>
      <name>raojiyong</name>
    </author>

  
    
    <category term="2021" />
    
    <category term="08" />
    
  

  
    <summary>
      





      线性降维


  维数灾难 curse of dimensionality
    
      高维空间样本稀疏
      计算内积难
    
  


MDS

Multiple Dimensional Scaling, 多维放缩


  样本间距离在低维空间保持
  算法
    
      由距离矩阵 $D$ 求内积矩阵: $b_{ij}=-\frac12(D_{ij}^2-D_{i\ast}^2-D_{\ast j}^2+D_{\ast\ast}^2)$
      特征值分解:$B=V\Lambda V^T$, 非零特征值构成 $\Lambda_\ast=diag\lambda_1,\lambda_2,\cdots,\lambda_d^\ast$
      坐标$Z=\Lambda_\ast^{\frac12}V_\ast^T$, 可提前 $d$ 个最大特征值
 ...
    </summary>
  

  </entry>

  
  <entry>
    <title>ML Clustering</title>
    <link href="http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Clustering/" rel="alternate" type="text/html" title="ML Clustering" />
    <published>2021-08-04T15:00:00+08:00</published>
  
    <updated>2021-08-04T15:00:00+08:00</updated>
  
    <id>http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Clustering/</id>
    <content src="http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Clustering/" />
    <author>
      <name>raojiyong</name>
    </author>

  
    
    <category term="2021" />
    
    <category term="08" />
    
  

  
    <summary>
      





      性能度量


  性能度量，有效性指标 validity index
  外部指标：与某个参考模型比较
    
      簇划分：$\mathcal{C}=C_1,C_2,\cdots,C_k$，参考模型簇划分 $\mathcal{C^}={C_1^,C_2^,\cdots,C_s^},\lambda,\lambda^*$ 分别为两者的簇标记向量，定义
        
          $a=\vert\text{SS}\vert,\text{SS}={(x_i,x_j)\ \vert\ \lambda_i=\lambda_j,\lambda_i^=\lambda^_j,i\lt j}$
          $b=\vert\text{SD}\vert,\text{SD}={(x_i,x_j)\ \vert\ \lambda_i=\lambda_j,\lambda_i^\ne...
    </summary>
  

  </entry>

  
  <entry>
    <title>ML EnsembleLearning</title>
    <link href="http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_EnsembleLearning/" rel="alternate" type="text/html" title="ML EnsembleLearning" />
    <published>2021-08-01T09:00:00+08:00</published>
  
    <updated>2021-08-02T16:48:20+08:00</updated>
  
    <id>http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_EnsembleLearning/</id>
    <content src="http://localhost:4000/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_EnsembleLearning/" />
    <author>
      <name>raojiyong</name>
    </author>

  
    
    <category term="2021" />
    
    <category term="08" />
    
  

  
    <summary>
      





      集成学习


  个体学习器
    
      同质：基学习器，基学习算法
      异质：组件学习器
      准确性，多样性
    
  
  学习器结合可能带来的好处
    
      统计：学习任务假设空间大，多个假设在训练集上达到同等性能，使用单学习器可能因误选而导致泛化性能不佳
      计算：降低陷入糟糕局部极小点的风险
      表示：某些学习任务的真实假设可能不在当前算法所考虑的假设空间中，使用多学习器可能学得较好的近似
    
  


序列化方法


  Boosting
    
      Train a weak learner $h_t$ from distribution $D_t$
      Evaluate the error $\epsilon_t$ of $h_t$
      $D_{t+1}=\mathrm{Adjus...
    </summary>
  

  </entry>

</feed>


