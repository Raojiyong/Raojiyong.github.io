[
  
  {
    "title": "Notes on backpropagation",
    "url": "/posts/Backpropagation/",
    "categories": "2021, 09",
    "tags": "notes, nn",
    "date": "2021-09-03 19:00:00 +0800",
    





    "snippet": "Some common loss function backpropagation1. SVMhinge loss\\[L_i=\\sum_{j\\neq y_i}\\max(0,\\omega_j^Tx_i-\\omega_{y_i}^Tx_i+\\Delta)\\]SVM full loss:\\[L=\\frac1N\\sum_i\\sum_{j\\neq y_i}[\\max(0,f(x_i,W)_j-f(x_i;W)_{y_i}+\\Delta)]+\\lambda\\sum_k\\sum_lW_{k,l}^2 \\\\\\]where $W_{k,l}^2$ is all square elements of $W$.its backpropagation process is below:\\[\\begin{align}\\text{dW}_j&amp;amp;=\\frac{X_i}{N} \\\\\\text{dW}_{y_i}&amp;amp;=\\frac{-X_i}{N} \\\\\\text{dW}&amp;amp;=\\text{dW}+2\\lambda W\\end{align}\\]2.  Cross entropy error with logistic activationIn a classification task with two classes, it is standard to use a neural network architecture with a single logistic output unit and the cross-entropy loss function. The output predication is between zero and one, and is interpreted as a probability.We consider a network with a single hidden layer of logistic units, multiple logistic output units.The cross entropy error:\\[E=-\\sum_{i=1}^{nclass}(t_i\\log(y_i)+(1-t_i)\\log(1-y_i))\\]where $\\text{t}$ is the target vector, $\\text{y}$ is the output vector. Outputs are computed by applying the logistic function to the weighted sums of the hidden layer activations, $s$,\\[y_i=\\frac1{1+e^{-s_i}} \\\\s_i=\\sum_{j=1}h_j\\omega_{ji} \\\\\\]Compute the derivative of the error w.r.t each weight connecting the hidden units to the output units using chain rule.\\[\\frac{\\partial E}{\\partial\\omega_{ji}}=\\frac{\\partial E}{\\partial y_i}\\frac{\\partial y_i}{\\partial s_i}\\frac{\\partial s_i}{\\partial \\omega_{ji}}\\]Examining each factor in turn:\\[\\begin{equation}\\begin{aligned}\\frac{\\partial E}{\\partial y_i}&amp;amp;=\\frac{-t_i}{y_i}+\\frac{1-t_i}{1-y_i} \\\\&amp;amp;=\\frac{y_i-t_i}{y_i(1-y_i)} \\\\\\frac{\\partial y_i}{\\partial s_i}&amp;amp;=y_i(1-y_i) \\\\\\frac{\\partial s_i}{\\partial \\omega_{ji}}&amp;amp;=h_j \\\\\\end{aligned}\\end{equation}\\]Combining things back together:\\[\\frac{\\partial E}{\\partial s_i}=y_i-t_i\\]and\\[\\frac{\\partial E}{\\partial \\omega_{ji}}=(y_i-t_i)h_j\\]3. Softmax and cross-entropy errorWhen a classification task has more than two classes, it is standard to useThe softmax activation of $i$-th output unit is:\\[y_i=\\frac{e^{s_i}}{\\sum_{c}^{nclass}e^{s_c}}\\]and the cross entropy error function for multi-class output is:\\[E=-\\sum_i^{nclass}t_i\\log(y_i)E=-\\sum_i^{nclass}t_i\\log(y_i)\\]$t$ is the target vector. Thus, computing the gradient yields:\\[\\begin{align}\\frac{\\partial E}{\\partial y_i}&amp;amp;=-\\frac{t_i}{y_i} \\\\\\frac{\\partial y_i}{\\partial s_k}&amp;amp;=\\begin{cases}\\frac{e^{s_i}}{\\sum_{c}^{nclass}e^{s_c}}-(\\frac{e^{s_i}}{\\sum_{c}^{nclass}e^{s_c}})^2 &amp;amp; i=k \\\\-\\frac{e^{s_i}e^{s_k}}{(\\sum_{c}^{nclass}e^{s_c})^2} &amp;amp; i\\neq k \\\\\\end{cases}\\\\ &amp;amp;=\\begin{cases}y_i(1-y_i) &amp;amp; i=k\\\\-y_iy_k &amp;amp; i\\neq k\\end{cases}\\end{align}\\]\\[\\begin{aligned}\\frac{\\partial E}{\\partial s_i}&amp;amp;=\\sum_k^{nclass}\\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial s_i} \\\\&amp;amp;=\\frac{\\partial E}{\\partial y_i}\\frac{\\partial y_i}{\\partial s_i}-\\sum_{k\\neq i}\\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial s_i} \\\\&amp;amp;= -t_i(1-y_i)+\\sum_{k\\neq i}t_ky_i \\\\&amp;amp;= -t_i+y_i\\sum_kt_k \\\\&amp;amp;=y_i-t_i \\\\\\end{aligned}\\]"
  },
  
  {
    "title": "ML Distance",
    "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Distance/",
    "categories": "2021, 08",
    "tags": "machine learning, notes",
    "date": "2021-08-27 19:00:00 +0800",
    





    "snippet": "距离  正定性  对称性  三角不等式有序距离  闵科夫斯基距离: $l=(\\sum_{i=1}^n\\vert x_i-y_i\\vert^p)^\\frac1p$          切比雪夫距离: $l_{\\infty}=\\max_{i=1}^n\\vert x_i-y_i\\vert$      欧几里得距离: $l_2=\\sqrt{\\sum_{i=1}^n\\vert x_i-y_i\\vert^2}$      曼哈顿距离: $l_1=\\sum_{i=1}^n\\vert x_i-y_i\\vert$      加权闵科夫斯基距离: $l=(\\sum_{i=1}^n\\omega_i\\vert x_i-y_i\\vert^p)^\\frac1p$        马氏距离: $d(\\vec{x},\\vec{y})=\\sqrt{(\\vec{x}-\\vec{y})^TS^{-1}(\\vec{x}-\\vec{y})}$          $S$ 协方差矩阵      $\\mathrm{dist}_{mah}^2(x_i,x_j)=(x_i-x_j)^TM(x_i-x_j)=\\Vert x_i-x_j\\Vert_M^2$, 度量矩阵 $M$ 为半正定矩阵。      $M=PP^T$                  $\\Vert x_i-x_j\\Vert_M=\\Vert P^Tx_i-P^Tx_j\\Vert$                      余弦距离: $d(x,y)=\\frac{&amp;lt;x,y&amp;gt;}{\\vert x\\vert\\vert y\\vert}$离散距离簇  VDM (Value Difference Metric)          $m_{u,a}$: 在属性 $u$ 上取值为 $a$ 的样本数      $m_{u,a,i}$: 第 $i$ 个样本簇在属性为 $a$ 的样本数      $\\text{VDM}p(a,b)=\\sum{i=1}^k\\vert \\frac{m_{u,a,i}}{m_{u,a}}-\\frac{m_{u,b,i}}{m_u,b}\\vert$      $\\text{MinkovDM}p(a,b)={(\\sum\\vert x{iu}-x_{ju}\\vert^p+\\sum\\text{VDM}p(x{iu},x_{ju}))^{\\frac1p}}$      字符串  海明距离  Lee 距离  Levenshtein (编辑距离)\\[\\text{lev}(a,b)=\\begin{cases}\\max(i,j)\\qquad \\text{if}\\ \\min(i,j)=0\\newline \\min\\begin{cases}\\text{lev}_{a,b}(i-1,j)+1 \\newline\\text{lev}_{a,b}(i,j-1)+1 \\newline\\text{lev}_{a,b}(i-1,j-1)+1_{a_i\\neq b_j} \\newline\\end{cases} \\qquad \\text{otherwise}\\end{cases}\\]非度量距离不满足三角不等式 (相似度度量无需满足三角不等式)两组点集的相似程度  Hausdorff 距离          $\\text{dist}_H(X,Z)=\\max(\\text{dist}_h(X,Z),\\text{dist}_h(Z,X))$      $\\text{dist}h(X,Z)=\\max{x\\in X}\\min_{z\\in Z}\\Vert x-z\\Vert_2$      NCANeighbourhood Component Analysis 近邻成分分析  近邻分类器中 $x_j$ 对 $x_i$ 分类结果影响概率为: $p_{ij}=\\frac{e^{-\\Vert x_i-x_j\\Vert_M^2}}{\\sum_le^{-\\Vert x_i-x_j\\Vert_m^2}}$  $x_i$ LOO 正确率: $p_i=\\sum_{j\\in \\Omega_i}p_{ij},\\Omega_i$ 为相同类别下标  训练集 LOO 正确率: $\\sum_{i=1}^mp_i=\\sum_{i=1}^m\\sum_{j\\in \\Omega_i}p_{ij}$  $\\min_p1-\\sum_{i=1}^m\\sum_{j\\in \\Omega_i}\\frac{\\exp(-\\Vert P^Tx_i-P^Tx_j\\Vert_2^2)}{\\sum_l\\exp(-\\Vert P^Tx_i-P^Tx_l\\Vert_2^2)}$领域知识必连约束 $\\mathcal{M}$,勿连约束 $\\mathcal{C}$\\[\\begin{equation}\\begin{aligned}\\min_{M}&amp;amp;\\sum_{(x_i,x_j)\\in\\mathcal{M}}\\Vert x_i-x_j\\Vert_M^2 \\notag \\\\s.t. &amp;amp;\\sum_{(x_i,x_k)\\in\\mathcal{C}}\\Vert x_i-x_k\\Vert_m \\geq1 \\notag\\\\&amp;amp;M\\succeq 0\\end{aligned}\\end{equation}\\]LMNNLarge Margin Nearest Neighbors      $k$ 个目标邻居相近，入侵样本远离                  目标邻居：最近的同类别样本                    入侵样本：最近中的非同类样本            \\[\\begin{align}\\min\\ast M&amp;amp;\\sum_{i,j\\in N\\ast i}d(x_i,x_j)+\\sum_{i,j,l}\\xi_{ijl} \\notag \\\\s.t.&amp;amp;\\forall_{i,j\\in N_k,l,yl\\neq y_i}d(x_i,d_j)+1\\leq d(x_i,x_l)+\\xi_{ijl} \\notag \\\\&amp;amp;\\xi_{ijl}\\geq0 \\notag \\\\&amp;amp;M \\succeq0 \\notag \\\\\\end{align}\\]"
  },
  
  {
    "title": "ML Dimension Reduction",
    "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_DimensionReduction/",
    "categories": "2021, 08",
    "tags": "notes, Machine Learning",
    "date": "2021-08-17 15:00:00 +0800",
    





    "snippet": "线性降维  维数灾难 curse of dimensionality          高维空间样本稀疏      计算内积难      MDSMultiple Dimensional Scaling, 多维放缩  样本间距离在低维空间保持  算法          由距离矩阵 $D$ 求内积矩阵: $b_{ij}=-\\frac12(D_{ij}^2-D_{i\\ast}^2-D_{\\ast j}^2+D_{\\ast\\ast}^2)$      特征值分解:$B=V\\Lambda V^T$, 非零特征值构成 $\\Lambda_\\ast=diag\\lambda_1,\\lambda_2,\\cdots,\\lambda_d^\\ast$      坐标$Z=\\Lambda_\\ast^{\\frac12}V_\\ast^T$, 可提前 $d$ 个最大特征值      PCA (Principal Component Analysis)  最近重构性：样本点到这个超平面距离足够近  最大可分性：样本点在这个超平面上的投影尽可能的分开\\[\\begin{equation}\\begin{aligned}\\max\\mathrm{tr}(W^TX&amp;amp;X^TW) \\\\s.t. \\ W^TW&amp;amp; =1 \\\\\\end{aligned}\\end{equation}\\]      算法          中心化      计算协方差矩阵 $XX^T$ 并特征值分解      取最大 $d$ 个特征向量为投影矩阵        PCA: 最佳描述特征  LDA: 最佳分类特征非线性降维核化线性降维  KPCA  KLDA流行学习Isomap  只考虑局部距离  算法          最短路径算法求出任意两点距离      带入 MDS      LLE (Locally Linear Embedding 局部线性嵌入)      只考虑领域内样本间的线性关系，在低维空间重构权值        算法                  确定每个点的 $k$ 近邻 $Q_i$                    根据下式求出 $\\omega_{ij},j\\in Q_i$,且 $\\omega_{ij}=0$ if $j\\notin Q_i$\\[\\begin{equation}\\begin{aligned}\\min_{\\omega_1,\\omega_2,\\cdots,\\omega_m}\\sum_{i=1}^m\\Vert x_i&amp;amp; \\sum_{i\\in Q_i}\\omega_{ij}x_j\\Vert_2^2 \\\\s.t.\\sum_{j\\in Q_i}\\omega_{ij}&amp;amp; =1 \\\\\\end{aligned}\\end{equation}\\]                    对 $M=(I-W)^T(I-W)$ 特征值分解，最小的 $d’$ 个特征值为投影 $Z$            "
  },
  
  {
    "title": "ML Clustering",
    "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Clustering/",
    "categories": "2021, 08",
    "tags": "notes, Machine Learning",
    "date": "2021-08-04 15:00:00 +0800",
    





    "snippet": "性能度量  性能度量，有效性指标 validity index  外部指标：与某个参考模型比较          簇划分：$\\mathcal{C}=C_1,C_2,\\cdots,C_k$，参考模型簇划分 $\\mathcal{C^}={C_1^,C_2^,\\cdots,C_s^},\\lambda,\\lambda^*$ 分别为两者的簇标记向量，定义                  $a=\\vert\\text{SS}\\vert,\\text{SS}={(x_i,x_j)\\ \\vert\\ \\lambda_i=\\lambda_j,\\lambda_i^=\\lambda^_j,i\\lt j}$          $b=\\vert\\text{SD}\\vert,\\text{SD}={(x_i,x_j)\\ \\vert\\ \\lambda_i=\\lambda_j,\\lambda_i^\\neq\\lambda^_j,i\\lt j}$          $c=\\vert\\text{DS}\\vert,\\text{DS}={(x_i,x_j)\\ \\vert\\ \\lambda_i\\neq\\lambda_j,\\lambda_i^=\\lambda^_j,i\\lt j}$          $d=\\vert\\text{DD}\\vert,\\text{DD}={(x_i,x_j)\\ \\vert\\ \\lambda_i\\neq\\lambda_j,\\lambda_i^\\neq\\lambda^_j,i\\lt j}$          $a+b+c+d=\\frac{m(m-1)}2$                    JC (Jaccard Coefficent)                  $\\text{JC}=\\frac a{a+b+c}$                    FMI (Fowlkes and Mallows Index)                  $\\text{FMI}=\\sqrt{\\frac a{a+b}\\cdot\\frac a{a+c}}$                    RI (Rand Index)                  $\\text{RI}=\\frac{2(a+d)}{m(m-1)}$                      内部指标          簇划分: $\\mathcal{C}=C_1,C_2,\\cdots,C_k$                  $\\text{avg}(C)=\\frac 2{\\vert C\\vert(\\vert C\\vert-1)}\\sum_{1\\leq i\\lt j\\leq\\vert C\\vert}\\text{dist}(x_i,x_j)$ 簇内样本平均距离          $\\text{diam}(C)=\\max_{1\\leq i\\lt j\\leq\\vert C\\vert}\\text{dist}(x_i,x_j)$ 簇内样本间最远距离          $d_{min}(C_i,C_j)=\\min_{x_i\\in C_i,x_j\\in C_j}\\text{dist}(x_i,x_j)$          $d_{cen}=\\text{dist}(\\mu_i,\\mu_j)$ 簇$C_i$与簇$C_j$中心点的距离                    DBI (Davies Bouldin Index)                  $\\text{DBI}=\\frac1k\\sum_{i=1}^k\\max_{j\\neq i}(\\frac{\\text{avg}(C_i)+\\text{avg}(C_j)}{d_{cen}(C_i,C_j)})$ 越小越好                    DI (Dunn Index)                  $\\text{DI}=\\min_{1\\leq i\\leq k}(\\min_{j\\neq i}(\\frac{d_{\\text{min}(C_i,C_j)}}{\\max_{1\\leq l\\leq k}\\text{diam}(C_l)}))$ 越大越好                    原型聚类  SOM: self-organizing mapsk-meansLearning Vector Quantization 学习向量量化  利用样本监督信息  每次迭代，每个样本对其最近的原型向量根据标记一致性做推动/吸引  每个原型向量 $p_i$ 定义了与之相关的一个区域 $R_i$，形成了对样本空间的 Voronoi tessellation高斯混合聚类  高斯混合分布：$p_M(x)=\\sum_{i=1}^h\\alpha_i\\cdotp(x\\vert \\mu_i.\\sum_i),\\ \\sum_{i=1}^k\\alpha_i=1$  EM 算法求解          $\\text{LL(D)}=\\ln(\\prod\\limits_{j=1}^mp_M(x_j))=\\sum_{j=1}^m\\ln(\\sum_{i=1}^k\\alpha_ip(x_j\\vert \\mu_i,\\sum_i))$      $\\text{E:}\\gamma_{ji}=p_M(z_j=i\\vert x_j)$      $\\text{M}$                  $\\mu_i’=\\frac{\\sum_{j=1}^m\\gamma_{ji}x_j}{\\sum_{j=1}^m\\gamma_{ji}}$ 新均值向量          $\\sum_i’=\\frac{\\sum_{j=1}^m\\gamma_{ji}(x_j-\\mu_i’)(x_j-\\mu_i’)^T}{\\sum_{j=1}^m\\gamma_{ji}}$ 新协方差矩阵          $\\alpha_i’=\\frac{\\sum_{j=1}^m\\gamma_{ji}}{m}$ 新混合系数                    密度聚类假设聚类结构能通过样本分布的紧密程度确定DBSCAN  $\\epsilon$-邻域：$N_\\epsilon(x_j)={x_i\\in D\\vert \\text{dist}(x_i,x_j)\\leq\\epsilon}$ 样本集中与$x_j$的距离不大于$\\epsilon$ 的样本  核心对象：$x_j,\\ \\vert N_\\epsilon(x_j)\\vert\\ge\\text{MinPts}$  directly density-reachable: $x_j\\in N_\\epsilon(x_i)$且$x_i$ 为核心对象，则$x_j$由$x_i$密度可达（无对称性）  density-reachable  density-connected: $\\exists x_k,x_i,x_j$均由$x_k$密度可达  簇 为满足下列性质的最大集合          connectivity: $x_i\\in C,x_j\\in C$ 则 $x_i,x_j$ 密度相连      maximality: $x_i\\in C,x_j$ 由 $x_i$ 密度可达，则 $x_j\\in C$        算法：          找出所有核心对象      对每个核心对象求 $X=x’\\in D\\vert x’$ 由 $x$ 密度可达      DIANAtop-down层次聚类自底向上或者自顶向下AGNESagglomerative nesting  自底向上  起初每个样本点为一个簇  不断合并最近两个簇            name      d                  single-linkage 单链接      min              complete-linkage 全链接      max              average-linkage 均链接      avg      "
  },
  
  {
    "title": "ML EnsembleLearning",
    "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_EnsembleLearning/",
    "categories": "2021, 08",
    "tags": "notes, Machine Learning",
    "date": "2021-08-01 09:00:00 +0800",
    





    "snippet": "集成学习  个体学习器          同质：基学习器，基学习算法      异质：组件学习器      准确性，多样性        学习器结合可能带来的好处          统计：学习任务假设空间大，多个假设在训练集上达到同等性能，使用单学习器可能因误选而导致泛化性能不佳      计算：降低陷入糟糕局部极小点的风险      表示：某些学习任务的真实假设可能不在当前算法所考虑的假设空间中，使用多学习器可能学得较好的近似      序列化方法  Boosting          Train a weak learner $h_t$ from distribution $D_t$      Evaluate the error $\\epsilon_t$ of $h_t$      $D_{t+1}=\\mathrm{Adjust_Distribution}(D_t,\\epsilon_t)$      $\\color{Bittersweet}{\\mathrm{Adaboost}}$  加性模型 (additive model): $H(x)=\\sum_{t=1}^T\\alpha_th_t(x)$  exponential loss function: $l_{exp}(H\\vert D)=E_{x\\sim D}(e^{-f(x)H(x)})$          指数损失函数最小化，分类错误率也将最小化（与 0/1 损失函数一致）        分类器权重更新公式: $\\alpha_t=\\frac12\\ln \\frac{1-\\epsilon_t}{\\epsilon_t}$并行化方法Bagging  采样 T 次，训练 T 个学习器，分类简单投票，回归简单平均  out-of-bag estimate: $H^{oob}(x)$ 为未使用 x 训练的基学习器在 x 上的预测          $H^{oob}(x)=\\arg_{y\\in Y}\\max\\sum_{t=1}^T[h_t(x)=y][x\\notin D_t]$      $\\epsilon^{oob}=\\frac1{\\vert D\\vert}\\sum_{(x,y)\\in D}[H^{oob}(x)\\neq y]$      随机森林  Bagging + 在随机选择的 k 个属性中选择最优属性          推荐值 $k=\\log_2 d$      结合策略平均法  简单平均法: $H(x)=\\frac1T\\sum_{i=1}^Th_i(x)$  加权平均法: $H(x)= \\sum_{i=1}^T\\omega_ih_i(x)$投票法  绝对多数投票法          可能拒绝预测        相对多数投票法  加权投票法  $h(x)$ 输出不同          hard voting: 类标记投票      soft voting: 类概率投票      基学习器类型不同，其概率值不能直接进行比较      学习法  Stacking          初级学习器（个体学习器）      次级学习器（元学习器）        BMA (贝叶斯模型平均)多样性误差-分歧分解  $E=\\bar{E}-\\bar{A}$          $h_i$ 的分歧：$A(h_i\\vert x)=(h_i(x)-H(x))^2$      集成的分歧: $\\bar{A}(h_i\\vert x)=\\sum_{i=1}^T\\omega_iA(h_i\\vert x)$      $E(h_i\\vert x)=(f(x)-h_i(x))^2$      $\\bar{E}(h\\vert x)=\\sum_{i=1}^T\\omega_iE(h_i\\vert x)$      多样性度量  预测结果列联表 (contingency table), $m=a+b+c+d$                   $h_i=+1$      $h_i=-1$                  $h_i=+1$      a      c              $h_j=-1$      b      d                  指标                         不合度量(Disagreement measure)      $\\mathrm{dis}_{ij}=\\frac{b+c}m$              相关系数      $\\rho_{ij}=\\frac{ad-bc}{\\sqrt{(a+b)(a+c)(c+d)(b+d)}}$              $Q$-statistic      $Q_{ij}=\\frac{ad-bc}{ad+bc}$              $\\kappa$-statistic      $\\kappa=\\frac{p1-p2}{1-p2}$              $\\kappa$ 图      $\\kappa$-平均误差图        取得一致的概率：$p1=\\frac{a+d}m$  偶然取得一致的概率：$p2=\\frac{(a+b)(a+c)(c+d)(b+d)}{m^2}$多样性增强  数据样本扰动          Bootstrap      对不稳定基学习器（决策树、神经网络）有效        输入属性扰动          random subspace算法        输出表述扰动          Flipping Output      Output Smearing      ECOC        算法参数扰动          负相关法      不同增强机制同时使用      "
  },
  
  {
    "title": "ML Bayes",
    "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Bayes/",
    "categories": "2021, 07",
    "tags": "notes, Machine Learning",
    "date": "2021-07-25 15:00:00 +0800",
    





    "snippet": "Bayesian decision theory                   定义      最小化分类错误率                  loss      $\\lambda_{ij}$      $[i=j]$              Expected loss      $R(c_i\\vert x)=\\sum_{j=1}^N\\lambda_{ij}P(c_j\\vert x)$      $1-P(c\\vert x)$              Bayes optimal classifier      $h^\\ast(x)=\\arg\\min_{c\\in Y}R(c\\vert x)$      $\\arg\\max_{c\\in Y}P(c\\vert x)$              Decision loss      $R(h)=E_x(R(h(x)\\vert x))$      $P(h^\\ast(x)\\vert x)$              Bayes risk      $1-R(h^\\ast)$      $1-P(h^\\ast(x)\\vert x)$      贝叶斯定理\\[P(c\\vert x)=\\frac{P(x,c)}{P(x)}=\\frac{P(c)P(x\\vert c)}{\\int p(c)P(x\\vert c)dc}\\]  先验 prior: $P(c)$  evidence: $P(x)$  类条件概率class-conditional probablity\\likelihood: $P(x\\vert c)$          class-conditional probability: $x$      likelihood: $\\theta,P(x\\vert c)(\\theta)$                  $P(D_c\\vert\\theta_c)=\\prod_{x\\in D_c}P(x\\vert\\theta_c)$          $LL(\\theta_c)=logP(D_c\\vert\\theta_c)=\\sum_{x\\in D_c}logP(x\\vert\\theta_c)$                    k近邻学习  lazy learning  最邻近分类器的泛化错误率不会超过贝叶斯最优分类器错误率的两倍朴素贝叶斯分类器  属性条件独立性假设：$P(x\\vert c)=\\prod_{i=1}^dP(x_i\\vert c)$  $h_{nb}=\\arg\\max_{c\\in Y}P(c)\\prod_{i=1}^dP(x_i\\vert c)$          $P(c)=\\frac{\\vert D_c\\vert}{\\vert D\\vert}$      $P(x_i\\vert c)=\\frac{\\vert D_{c,x_i}\\vert}{\\vert D_c\\vert}$        拉普拉斯修正          $\\hat P(c)=\\frac{\\vert D\\vert+1}{\\vert D\\vert+N}$                  N 为 D 中可能的类别                    $\\hat P(x_i\\vert c)=\\frac{\\vert D_{c,x_i}\\vert+1}{\\vert D_c\\vert+N_i}$                  $N_i$ 为第 i 个属性可能取值数                      连续属性          $p(x_i\\vert c)\\sim N(\\mu_{c,i},\\sigma^2_{c,i})$      半朴素贝叶斯分类器独依赖估计 (One-Dependent Estimator, ODE)  $P(c\\vert x)\\propto P(c)\\prod_{i=1}^dP(x_i\\vert c,p(a_i))$SPODE (super-parent ODE)  假设所有属性都依赖于同一属性: $p(a_i)=x_t$AODE (Averaged One-Dependent Estimator)  SPODE 的集成  $P(c\\vert x)\\propto\\sum_{i=1,\\vert D_{x_i}\\vert\\ge m’}^dP(c,x_i)\\prod_{j=1}^dP(x_j\\vert c,x_i)$TAN (Tree Augmented naive Bayes)      仅保留了强相关属性间的依赖性        基于最大带权生成树        算法                  conditional mutual information:        $I(x_i,x_j\\vert y)=\\sum_{x_i,x_j,c}log\\frac{P(x_i,x_j\\vert c)}{P(x_i\\vert c)P(x_j\\vert c)}$                  在已知类别情况下的相关性                            在以属性为节点，互信息为边建完全图上构造最大带权生成树，挑选根节点，边置为有向                    加入类别节点 y,增加 y 到每个属性的边            kDE (k-Dependent Estimator)  样本不足：高阶连个概率估计困难，需要的样本数指数级增加"
  },
  
  {
    "title": "ML SVM",
    "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_SVM/",
    "categories": "2021, 07",
    "tags": "notes, Machine Learning",
    "date": "2021-07-20 20:00:00 +0800",
    





    "snippet": "SVM 基本型      划分超平面:  $\\omega^Tx+b=0$                  点到超平面的距离: $\\frac{\\vert \\omega^Tx+b\\vert}{\\Vert \\omega\\Vert}$\\(\\begin{cases}\\omega^Tx+b \\ge y_i,&amp;amp; y_i=+1 \\\\\\omega^Tx+b \\le y_i,&amp;amp; y_i=-1 \\\\\\end{cases}\\)                    支持向量（support vector）：使上式成立的样本点                    间隔（margin）：两个异类支持向量到超平面的距离$\\frac{2}{\\Vert\\omega\\Vert}$                    SVM基本型(Support Vector Machine)          \\[\\begin{equation}\\begin{aligned}&amp;amp;\\min\\limits_{\\omega,b}\\frac12\\Vert\\omega\\Vert^2 \\\\&amp;amp;s.t. y_i(\\omega^Tx_i+b)\\ge1,\\qquad i=1,2,\\cdots,n \\\\\\end{aligned}\\end{equation}\\]          凸优化求解：复杂度与样本维度（等于权值 $\\omega$ 的维度）有关      对偶问题  复杂度与样本数量（等于拉格朗日算子 $\\alpha$ 的数量）有关  解的稀疏性：最终模型仅与支持向量有关          KKT条件导出      对偶问题的转化  step1：拉格朗日函数：$L(\\omega,b,\\alpha)$      step2： 对 $\\omega$ 和 $b$ 求导并令导数为0          $\\omega=\\sum_{i=1}^m{\\alpha_iy_ix_i}$      $\\sum_{i=1}^m\\alpha_iy_i=0$        step3：回代：\\[\\begin{equation}\\begin{split}\\max\\limits_{\\alpha}&amp;amp;\\sum_{i=0}^m\\alpha_i-\\frac12\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\\\s.t. &amp;amp;\\sum_{i=1}^m\\alpha_iy_i=0 \\\\&amp;amp;\\alpha_i\\ge0\\end{split}\\end{equation}\\]求解对偶问题  SMO(Sequential Minimal Optimization)          选取一对需要更新的变量 $\\alpha_i$ 和 $\\alpha_j$                  先选违背KKT条件最大的，再选使目标函数增长最快的          实际中启发式：选取两变量所对应样本之间间隔最大                    固定其他参数，更新 $\\alpha_i$ 和 $\\alpha_j$      核函数  $f(x)=\\omega^T\\phi(x)$  核函数：$\\kappa(x_i,x_j)=\\langle\\phi(x_i),\\phi(x_j)\\rangle$          $\\kappa$ 为核函数 $\\iff$ 核矩阵 $\\Kappa$ 是半正定的        $\\kappa_1,\\kappa_2$ 为核函数，则以下为核函数          $\\gamma\\kappa_1+\\gamma\\kappa_2$      $\\kappa_1\\otimes\\kappa_2(x,z)=\\kappa_1(x,z)\\kappa_2(x,z)$      $\\kappa(x,z)=g(x)\\kappa_1(x,z)g(z)$                  常用核函数      $\\kappa(x_i,x_j)$                  线性核      $x_i^Tx_j$              多项式核      $(x_i^Tx_j)$              高斯核      $e^{-\\frac{\\Vert x_i-x_j\\Vert^2}{2\\sigma^2}}$              拉普拉斯核      $e^{-\\frac{\\Vert x_i-x_j\\Vert}{\\sigma}}$              Sigmoid 核      $tanh(\\beta x_i^Tx_j+\\theta)$            支持向量展式（利用对偶问题）：    $f(x)=\\omega^T\\phi(x)+b=\\sum_{i=1}^m\\alpha_iy_i\\kappa(x,z)+b$  软间隔  优化目标：$\\min_{\\omega,b}\\frac12\\Vert\\omega\\Vert^2+C\\sum_{i=1}^m\\xi_i$          松弛变量 $\\xi_i=l(y_i(\\omega^Tx_i+b)-1)$        原问题\\[\\begin{equation}\\begin{split}\\min\\limits_{\\omega,b}&amp;amp;\\frac12\\Vert\\omega\\Vert^2+C\\sum_{i=1}^m\\xi_i \\\\s.t. &amp;amp;y_i(\\omega^Tx_i+b)\\ge1-\\xi_i \\\\&amp;amp;\\xi \\ge0\\\\\\end{split}\\end{equation}\\]  对偶问题（损失函数为 hinge）\\[\\begin{equation}\\begin{aligned}\\max\\limits_\\alpha&amp;amp;\\sum_{i=1}^m\\alpha_i-\\frac12\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_j\\phi(x_i)^T\\phi(x_j) \\\\s.t. &amp;amp;\\sum_{i=1}^m\\alpha_iy_i=0 \\\\&amp;amp;C\\ge\\alpha_i\\ge0 \\\\\\end{aligned}\\end{equation}\\]            损失函数      $l(z)$      Remark                  0/1      1, z&amp;lt;0      不易求解              hinge      max(0,1-z)      保持稀疏性              exp      $e^{-z}$                     log      $log(1+e^{-z})$      几率回归模型，无稀疏性        一般形式：$\\min_f\\Omega(f)+C\\sum_{i=1}^ml(f(x_i),y_i)$          结构风险：$\\Omega(f)$      经验风险：$\\sum_{i=1}^ml(f(x_i),y_i)$ ，模型与训练数据契合程度      支持向量回归 SVR      $\\min_{\\omega,b}\\frac12\\Vert\\omega\\Vert^2+C\\sum_{i=1}^ml_\\epsilon(f(x_i)-y_i)$          落入中间 2$\\epsilon$ 间隔带的样本不计算损失\\(\\begin{cases}0,&amp;amp;\\vert z\\vert\\le\\epsilon \\\\\\vert z\\vert-\\epsilon,&amp;amp; otherwise\\end{cases}\\)      核方法      表示定理：对于任意的单调递增函数 $\\Omega$ 和任意非负损失函数 $l$，优化问题\\(\\min\\limits_{h\\in\\mathbb{H}}F(h)=\\Omega(\\Vert h\\Vert_\\mathbb{H})+l(h(x_1),h(x_2,\\cdots,h(x_m))\\)的解总可以写成 $h^\\ast(x)=\\sum_{i=1}^m\\alpha_i\\kappa(x,x_i)$    KLDA  KPCA"
  },
  
  {
    "title": "ML DecisionTree",
    "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_DecisionTree/",
    "categories": "2021, 05",
    "tags": "notes, Machine Learning",
    "date": "2021-05-23 15:00:00 +0800",
    





    "snippet": "决策树基本算法  当前节点包含样本全部同类：标记为某类  当前样本属性值为空/取值相同：标记为最多一类  属性划分选择  为属性每个值分配一个结点继续执行算法          若其属性值上为空则标记为当前最多一类      递归过程，分治划分选择            指标名称      指标      辅助函数      例子      remark                  信息增益(Info gain)      $\\mathrm{Gain}(D,a)=\\mathrm{Ent}(D)-\\sum_{v=1}^V\\frac{\\vert D^v\\vert}{\\vert D\\vert}\\mathrm{Ent}(D^v)$      信息熵$\\mathrm{Ent}(D)=-\\sum_{k=1}^{\\vert y\\vert}p_klog_2p_k$      ID3      对可取值数目较多的属性有偏好              Gain ratio      $\\mathrm{Gain_ratio}(D,a)=\\frac{\\mathrm{Gain}(D,a)}{\\mathrm{IV}(a)}$      固有值$\\mathrm{IV}(a)=-\\sum_{v=1}^V\\frac{\\vert D^v\\vert}{\\vert D\\vert}log_2\\frac{\\vert D^v\\vert}{\\vert D\\vert}$      C4.5      从候选划分中找出信息增益高于平均水平的属性，再从中选择增益率最高的              Gini ratio      $\\mathrm{Gini_index}(D,a)=\\sum_{v=1}^V\\frac{\\vert D^v\\vert}{\\vert D\\vert}\\mathrm{Gini}(D^v)$      $\\mathrm{Gini}(D)=1-\\sum_{k=1}^{\\vert Y\\vert}p_k^2$      CART      Gini指数为随机抽取两个样本类别标记不一致的概率，越小纯度越高      剪枝处理            方法      指标      过拟合      欠拟合      训练时间                  预剪枝      precision      降低过拟合风险      有欠拟合风险      较小              后剪枝      precision      降低过拟合风险      欠拟合风险小      较长      连续与缺失值  连续属性离散化(二分法)          $T_a=\\frac{a^i+a^{i+1}}2 \\vert\\ 1\\le i\\le n-1$        缺失值          $\\tilde{D}:D$在属性 $a$ 上没有缺失值的样本子集      $\\tilde{D}^v:D$中属性 $a$ 上取值为 $a^v$ 的样本子集      $\\tilde{D}_k:\\tilde{D}$ 类别中为 $k$ 的样本子集      $\\omega_x:$ 每个样本的权重      $\\rho=\\frac{\\sum_{x\\in\\tilde{D}}\\omega_x}{\\sum_{x\\in D}\\omega_x}$ 对属性 $a$ ，$\\rho$ 表示无缺失样本所占比例      $\\tilde{p_k}=\\frac{\\sum_{x\\in\\tilde{D}k}\\omega_x}{\\sum{x\\in D}\\omega_x}$ 对属性 $a$，$\\tilde{p_k}$ 表示无缺失值样本中第 $k$ 类的占比      $\\tilde{r_v}=\\frac{\\sum_{x\\in\\tilde{D}^v}\\omega_x}{\\sum_{x\\in D}\\omega_x}$ 其中 $\\tilde{r_v}$ 表示无缺失值样本中在属性 $a$ 上取值 $a^v$ 的占比      $\\mathrm{Ent}(\\tilde{D})=-\\sum_{k=1}^{\\vert Y\\vert}\\tilde{p_k}log_2\\tilde{p_k}$      $\\mathrm{Gain}(D,a)=\\rho\\ast\\mathrm{Gain}(\\tilde{D},a)=\\rho\\ast\\Bigl(\\mathrm{Ent}(\\tilde{D})-\\sum_{v=1}^V\\tilde{r_v}\\mathrm{Ent}(\\tilde{D^v})\\Bigr)$      多变量决策树  每个非叶节点用 $\\sum_{i=1}^d\\omega_ia_i=t$ 的线性分类器，是对属性的线性组合进行测试。  特征预处理"
  },
  
  {
    "title": "ML LinearModel",
    "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_LinearModel/",
    "categories": "2021, 05",
    "tags": "notes, Machine Learning",
    "date": "2021-05-17 15:00:00 +0800",
    





    "snippet": "多元线性回归  $f(x)=\\omega^T\\omega+b$  决策平面：$f(x;\\omega)=0$          有向距离: $\\gamma=\\frac{f(x;\\omega)}{\\vert\\omega\\vert}$        最小二乘法(使均方误差最小)          $\\hat\\omega^*=arg\\ min_{\\hat\\omega}(y-X\\hat\\omega)^T(y-X\\hat\\omega)=(X^TX)^{-1}X^Ty$        广义线性模型：$y=g^{-1}(\\omega^Tx+b$)，其中单调可微函数 $g(\\cdot)$ 称为联系函数对数几率回归  单位跃阶函数 (Heaviside/unit-step function)：理想但不连续\\[y=\\begin{cases}0, &amp;amp;  z&amp;lt;0 \\\\0.5, &amp;amp; z=0 \\\\1, &amp;amp; z&amp;gt;0\\end{cases}\\]  对数几率函数 (logistic/Sigmoid function)          $g=ln\\frac{y}{1-y}$                  几率：$\\frac y{1-y}$ ，反映了 $x$ 作为正例的相对可调性                    $g^{-1}=S(x)=\\frac 1{1+e^{-x}}$                  $S(x)’=s(x)(1-S(x))$                      对数几率回归：用线性模型逼近真实标记的几率          $ln \\frac{p1}{p0}=\\hat x\\beta=(x,1)(\\omega;b)$                  二分类：$y_ia+(1-y_i)b=a^{y_i}b^{1-y_i}$                    Maxmimum likelihood method                  $l(\\beta)=\\sum_{i=1}^mlmp(y_i\\vert x_i;\\beta_i)=\\sum_{i=1}^my_iln(g(\\hat x_i\\beta)+(1-y_i)ln(1-g(\\hat x_i\\beta)))$          $l’=\\sum_{i=1}^m(y_i-g(\\hat x\\beta))\\hat x_i^T=X^T(Y-g(\\beta^TX))$          $l’’=\\sum_{i=1}^m\\hat x_i\\hat x_i^Tp_1(x_i^T;\\beta)(1-p_1(\\hat x_i;\\beta))$                    交叉熵做损失函数梯度下降        梯度下降: $\\theta_{i+1}=\\theta_i-\\alpha\\frac{\\partial L}{\\partial\\theta}$Softmax回归  $p(y=c\\vert x)=softmax(\\omega^T_cx)=\\frac{exp\\left(\\omega_c^Tx\\right)}{\\sum^C_{c’=1}exp(\\omega^T_{c’}x)}=\\frac{exp(W^Tx)}{1_C^Texp(W^Tx)}$LDA  给定训练集数据，设法将样例投影到一条直线上，使得同类样例投影点尽可能接近，异类投影点尽可能远  协方差矩阵: $\\sum=\\frac 1{n-1}(X-\\mu I)(X-\\mu I)^T$          $\\sum_{ij}=\\sigma(x_i,x_j)$      投影后: $\\omega^T\\sum\\omega$                         两类      一般                  within-class scatter matrix      $S_\\omega=\\sum_0+\\sum_1$      $S_m=\\sum_{i=1}^N \\sum_i$              Between-calss scatter matrix      $S_b=(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T$      $S_b=\\sum_{i=1}^Nm_i(\\mu_i-\\mu)(\\mu_i-\\mu)^T$              全局散度矩阵      $S_t=S_b+S_\\omega$      $\\sum_{i=1}^m(x_i-\\mu)(x_i-\\mu)^T$              优化目标      $max_\\omega\\frac{\\omega^TS_b\\omega}{\\omega^TS_w\\omega}$      $max_W\\frac{tr(W^TS_bW)}{tr(W^TS_wW)}$              闭式解      $w=S_w^{-1}(\\mu_0-\\mu_1)$      $S_w^{-1}S_b$前$k$大广义特征向量      ​"
  },
  
  {
    "title": "ML Introduction",
    "url": "/posts/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Introduction/",
    "categories": "2021, 05",
    "tags": "notes, Machine Learning",
    "date": "2021-05-14 15:00:00 +0800",
    





    "snippet": "机器学习  机器学习要素          模型      学习准则      优化算法        数据集：$D=x_1,x_2,…,x_m$          通常假设全体样本服从一个未知分布$\\mathcal{D}$,且采样$i.i.d$        归纳偏好          No Free Lunch Theorem      Occam’s Razor      Ugly Ducking Theorem            all vectors are assumed to be column vectors        $N$ number of input, $p$ number of features        训练集$X_{N_\\times p}$          $i$-th row $x_i^T$:length $p$      $j$-th column $x_j$ :length N      input vector: $X_{p\\times 1}$      $X^T=(X_1,X_2,\\cdots,X_p),X_i $ is a scalar            $y_{N_\\times 1}$          $Y \\in R$      $Y_{N_\\times 1}$, each row has one 1            $(X_1,X_2)$: 行并列        $(X_1^T,X_2^T)$: 列并列        偏差-方差分解：    $E(f;D)=bias^2(x)+var(x)+c^2=(\\overline f(x)-y)^2+E_D((f(x;D)-\\overline f(x))^2)+E_D((y_D-y)^2)$  评估方法  留出法  cross validation          将数据集分层采样划分为 $k$ 个大小相似的互斥子集，每次用 $k-1$ 个子集的并集作为训练集，余下的子集作为行为测试集，最终返回 $k$ 个测试结果的均值。      $k$ 最常用的取值是 10      LOO(leave-one-out): k=1.        bootstrapping: 样本不被选到的概率=$\\lim_{m \\to \\infty}(1-\\frac {1}{m})^m=\\frac{1}{e}=0.368.$模型评估指标  T,F: 分类正确与否  P,N: 预测结果Regression  均方误差:$E(f;D)=\\frac{1}{m}\\sum_{i-1}^m(f(x_i)-y_i)^2$Classification            指标                         accuracy      $\\frac{TP+TN}{m}$              precision      $\\frac{TP}{TP+FP}$              recall      $\\frac{TP}{TP+FN}$              macro-P      $\\frac{1}{n}\\sum _{i=1}^n P_i$              macro-R      $\\frac{1}{n}\\sum _{i=1}^n R_i$              micro-P      $\\frac{TP}{TP+FP}$              micro-R      $\\frac{TP}{TP+FN}$              平衡点(BEP)      P-R曲线上 P=R 的点              F1      $\\frac{2PR}{P+R}$调和平均更重视较小值              $F_\\beta$      $\\frac{1}{F_\\beta}=\\frac{1}{1+\\beta ^2}(\\frac{1}{P}+\\frac{\\beta^2}{R}),\\beta&amp;gt;1,R $ counts more              TPR      R              FPR      $\\frac{FP}{TN+FP}$              ROC      TPR-FPR 图 (Receiver Operating Characteristic)              AUC      (Area Under Curve) $\\frac{1}{2}\\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_{i+1}+y_i)$              FPR      1-TPR              cost curve      横轴为正例概率代价，纵轴为归一化代价              Ranking Loss      $l$=1-AUC      Ranking  One query  $r\\to$ One permutation $\\pi$  One query  $r\\to$  retrieved documents $R(r)$  precision: precision= $P$ =$\\frac{\\vert C(r)\\cap R(r)\\vert}{\\vert R(r)\\vert}$          precision$@k$                  $\\vert R(r)\\vert=k$          $\\frac{\\sum_{i&amp;lt;k}l(\\pi(t))}k$                      recall: $recall =\\frac{\\vert C(r)\\cap R(r)\\vert}{\\vert R(r)\\vert}$          recall$@k$,$\\vert R(r)\\vert=k$        AP (AveP, Average precision): AP $=\\frac{\\sum_{k=1}^{\\vert C(r)\\vert}P(@k)}{\\vert R(r)\\vert}$  mAP (Mean average precision): MAP = $=\\frac{\\sum_{q=1}^{Q}AP(q)}Q$  CG (Cumulative Gain): CG$@k=\\sum_{i=1}^k rel(i)$  DCG (Discounted cumulative gain): DCG$@k=\\sum_{i=1}^k rel(i)\\eta(i)$          折扣因子$\\eta(i)$:$\\frac 1{log(i+1)}$        IDCG (Ideal DCG): $IDCG@k=max_\\pi DCG@k(\\pi)$  nDCG (Normalized DCG): $IDCG@k=\\frac{DCG_p}{IDCG_p}$  RR (reciprocal rank): $rank_i$第一个正确答案的排名  MRR (Mean reciprocal rank): $MRR=\\frac 1{\\vert Q\\vert}\\sum_{i=1}^{\\vert Q\\vert}\\frac 1{rank_i}$  Cascade Models: 用户在位置 k 需求满足的概率$PP(k)=\\prod_{i=1}^{k-1}(1-R(i))R(k)$          该文档满足需求的概率: $R(t)=\\frac{2^{rel(t)}-1}{2^{l_{max}}}$        ERR (Expected reciprocal rank): 用户需求满足时停止位置的倒数的期望 $ERR=\\sum_{r=1}^n\\frac 1rPP_r$多分类  OvO：储存开销于测试时间偏大  OvR：训练时间偏大  MvM：          Error Correcting Output Codes(ECOC)                  编码：二元码，三元码          解码：将距离最小的编码所对应的类别作为预测结果                      argmax: $y=arg max_{c=1}^C f_c(x;w_c)$类别不平等  欠采样  过采样  阈值移动：$\\frac{y}{1-y} &amp;gt; \\frac{m^+}{m^-}$假设检验  二项检验          二项分布：$P(\\hat \\epsilon;\\epsilon)=\\binom{m}{\\hat\\epsilon m}\\epsilon^{\\hat\\epsilon m}(1-\\epsilon)^{m-\\hat\\epsilon m}$      假设：$\\epsilon \\le \\epsilon_0$      临界值$\\bar\\epsilon=max\\  \\epsilon$  s.t. $\\sum_{i=\\epsilon_0 \\times m+1}^m \\binom{m}{i}\\epsilon^i(1-\\epsilon)^{m-i}$        t 检验          检验值：$\\tau_t=\\frac{\\sqrt{k}(\\mu-\\epsilon_0)}{\\sigma}$        交叉验证 t 检验          差值：$\\Delta_i=\\epsilon_i^A-\\epsilon_i^B$      $\\tau_t=\\vert \\frac{\\sqrt{k}(\\mu-\\epsilon_0)}{\\sigma}\\vert$      假设检验前提：测试错误率为泛化错误率独立采样                  5$\\times$2交叉验证（为缓解不同训练轮次训练集的重叠问题，5次2折交叉验证）                      McNemar 检验          $e_{ij}:i$ 指示算法 $A$ 预测正确与否，$j$ 指示算法 $B$ 预测正确与否      假设：$e_{01}=e_{10}$      $\\vert e_{01}-e_{10}\\vert \\sim N(1,e_{01}+e_{10})$      统计检验量：$\\tau=\\frac{(\\vert e_{01}-e_{10}\\vert -1)^2}{ e_{01}-e_{10}}\\sim \\chi^2(1)$        Friedman 检验：多个数据集对多个算法比较          根据测试性能赋予序值，求得平均序值      假设：算法性能全部相同      $\\tau_\\chi^2=\\frac{12N}{k(k+1)}(\\sum_{i=1}^kr_i^2-\\frac{k(k+1)^2}{4})$,当 k, N 较大时，服从$\\chi^2(k-1)$ 分布      $\\tau_F=\\frac{(N-1)\\tau_\\chi^2}{n(K-1)-\\tau_\\chi^2}\\sim F(k-1,(k-1)(N-1))$        Nemenyi 检验：算法性能全部相同假设被拒绝后进一步区分各算法          CD=$q_\\alpha \\sqrt{\\frac{k(k+1)}{6N}},q_\\alpha$ 为 Tukey 分布临界值      若两个算法平均序值之差超出临界值域 CD，则以相应置信度拒绝两个算法性能相同      "
  },
  
  {
    "title": "oj输入输出",
    "url": "/posts/oj%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/",
    "categories": "2021, 05",
    "tags": "writing",
    "date": "2021-05-10 16:00:00 +0800",
    





    "snippet": "A+B输入描述：输入数据有多组，每组表示一组输入数据每行不定有n个整数，空格隔开输出描述：魅族输出求和结果代码#include &amp;lt;bits/stdc++.h&amp;gt;using namespace std;int main(){	int n;	int sum=0;	while(cin&amp;gt;&amp;gt;n){		sum+=n;		if(cin.get()==&#39;\\n&#39;){			cout&amp;lt;&amp;lt;sum&amp;lt;&amp;lt;endl;			sum=0;		}	}	return 0;}cin、cin.get()、cin.getline()、getline()的区别cin»输入数据，在遇到结束符2(space、tab、enter)就结束，且不将结束符存入标量在。注意最后一个enter也在缓冲区中，例如：void main(){	char ch1,ch2;	cin&amp;gt;&amp;gt;ch1;	cout&amp;lt;&amp;lt;ch1&amp;lt;&amp;lt;endl;	cin.get(ch2);	cout&amp;lt;&amp;lt;int(ch2)&amp;lt;&amp;lt;endl;  //输出10，因为enter的asii码为10}cin.get(字符数组名，接受长度，结束符)结束符意味着遇到该符号结束读取字符串，默认为enter，读取的字符个数最多为(长度-1)，因为最后一个为’\\0’。cin.get() 遇到结束符停止读取，但是并不会讲解舒服从缓冲区丢弃。cin.getline(字符数组名，接受长度，结束符)于cin.get()极为类似。不同的是cin.getline()当输入超长时，会引起cin函数的错误，后面的cin操作将不再执行。cin.get()每次读取一整行并把由Enter键生成的换行符留在输入队列中，然而cin.getline()每次读取一整行并把由Enter键生成的换行符抛弃getline(istream is,string str,结束符)geline()于前面的有很大差别，不会遇到tab、space、enter结束读取，会丢弃最后一个换行符。字符串第一种情况输入描述多个测试用例，每个测试用例一行每行通过空格隔开，有n个字符a c bbf ddddnowcoder输出描述对于每个输入阳历输出排序过的字符串，空格隔开a bb cdddd fnowcoder代码#include &amp;lt;bits/stdc++.h&amp;gt;using namespace std;int main(){	string s;	vector&amp;lt;string&amp;gt; str;	while(cin&amp;gt;&amp;gt;s){		str.push_back(s);		if(cin.get()==&#39;\\n&#39;){			sort(str.begin(),str.end());			for(auto c:str)				cout&amp;lt;&amp;lt;c&amp;lt;&amp;lt;&#39; &#39;;			cout&amp;lt;&amp;lt;endl;			str.clear();		}	}	return 0;}第二种情况输入描述多个测试用例，每个测试用例一行每行通过，隔开，有n个字符a,c,bbf,ddddnowcoder输出描述输出每组排序后的字符串，用，隔开，无结尾空格a,bb,cdddd,fnowcoder代码#include &amp;lt;bits/stdc++.h&amp;gt;using namespace std;int main(){	string line;	while(getline(cin,line)){		istringstream is(line);		string str;		vector&amp;lt;string&amp;gt; s;		while(getline(is,str,&#39;,&#39;)){  //while判断的实际上是is是否是有效的输入流			s.push_back(str);				}		sort(s.begin(),s.end());		for(int i=0;i&amp;lt;s.size()-1;i++){			cout&amp;lt;&amp;lt;c&amp;lt;&amp;lt;&quot;,&quot;;		}		cout&amp;lt;&amp;lt;s[i]&amp;lt;&amp;lt;endl;	}	return 0;}"
  },
  
  {
    "title": "暑期实习笔试",
    "url": "/posts/%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AF%95/",
    "categories": "2021, 05",
    "tags": "writing",
    "date": "2021-05-08 15:00:00 +0800",
    





    "snippet": "网易开发岗题目描述给定多个区间，计算让这些区间互不重叠所需要移除区间的最少个数。起止相连不算重叠。代码int eraseOverlapIntervals(vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt;&amp;amp; intervals) {	if (intervals.empty()) {		return 0;	}	int n = intervals.size();	sort(intervals.begin(), intervals.end(), [](vector&amp;lt;int&amp;gt;&amp;amp; a, vector&amp;lt;int&amp;gt;&amp;amp; b)		{			return a[1] &amp;lt; b[1];	});	int removed = 0, prev = intervals[0][1];	for (int i = 1; i &amp;lt; n; ++i) {		if (intervals[i][0] &amp;lt; prev) {			++removed;		} else {			prev = intervals[i][1];		}	}	return removed;}intervals.size()指的是行大小，intervals[i].size()指的是列大小。题目描述给定两个字符串S和T，求S中包含T所有字符的最短连续子字符串的长度，同时要求时间复杂度不超过O(n)代码其中chars 表示目前每个字符缺少的数量，flag 表示每个字符是否在 T 中存在。string minWindow(string S, string S){	vector&amp;lt;int&amp;gt; chars(128,0);	vector&amp;lt;bool&amp;gt; flag(128,false);		for(int i=0;i&amp;lt;T.size();i++){		flag[T[i]]=true;		++chars[T[i]];	}		int cnt=0,l=0,min_l=0,min_size=S.size()+1;	for(int r=0;r&amp;lt;S.size();r++){		if(flag[S[r]]){			if(--chars[S[r]]&amp;gt;=0){				++cnt;			}						while(cnt == T.size()){				if(r-l+1&amp;lt;min.size()){					min_l=1;					min_size=r-l+1;				}				if(flag[Sl] &amp;amp;&amp;amp; ++chars[S[l]]&amp;gt;0)					--cnt;				++l;			}						}	}	return min_size &amp;gt; S.size()?&quot;&quot;:S.substr(min_l,min_size);}题目描述给定两个字符串S和T，求S中包含T所有字符的最短连续子字符串的长度，同时要求时间复杂度不超过O(n)代码其中chars 表示目前每个字符缺少的数量，flag 表示每个字符是否在 T 中存在。string minWindow(string S, string S){	vector&amp;lt;int&amp;gt; chars(128,0);	vector&amp;lt;bool&amp;gt; flag(128,false);		for(int i=0;i&amp;lt;T.size();i++){		flag[T[i]]=true;		++chars[T[i]];	}		int cnt=0,l=0,min_l=0,min_size=S.size()+1;	for(int r=0;r&amp;lt;S.size();r++){		if(flag[S[r]]){			if(--chars[S[r]]&amp;gt;=0){				++cnt;			}						while(cnt == T.size()){				if(r-l+1&amp;lt;min.size()){					min_l=1;					min_size=r-l+1;				}				if(flag[Sl] &amp;amp;&amp;amp; ++chars[S[l]]&amp;gt;0)					--cnt;				++l;			}						}	}	return min_size &amp;gt; S.size()?&quot;&quot;:S.substr(min_l,min_size);}判断回文子字符串Palindromic Substring (中心扩散)int countSubstrings(string s){	it count=0;	for(int i=0;i&amp;lt;s.length();i++){		count+=extendSubstrings(s,i,i;);  //奇数长度		count+=extendSubstrings(s,i,i+1);  //偶数长度			}	return count;}int extendSubstrings(string s,int l,int r){	int count=0;	while(l&amp;gt;=0&amp;amp;&amp;amp;r&amp;lt;s.length()&amp;amp;&amp;amp;s[l]==s[r]){		--l;		++r;		++count;	}	return count;}str.substr(pos,n) 取子字符串。remove(str.begin(),str.end(),val) 将字符串str中val的字符提取到字符串后部，并返回新的尾部，remove不是真的删除。str.erase(remove(str.begin(),str.end(),val),str.end()) 删除字符串str中指定val的字符。字节data算法岗题目描述输入字符串B和数字n，以逗号，隔开，定义特殊子串为B中顺序不变、长度为n的子串，求B一共有多少个子串。输入word,2输出6解释：wo、wr、wd、or、od、rd代码#include &amp;lt;bits/stdc++.h&amp;gt;using namespace std;int main(){	}面试常问  进程和线程的概念、区别和联系，线程通信方式。  HTTP2.0和1.0的区别。"
  },
  
  {
    "title": "Anaconda3安装Pytorch",
    "url": "/posts/Anaconda%E5%AE%89%E8%A3%85Pytorch/",
    "categories": "2021, 05",
    "tags": "tutorial",
    "date": "2021-05-06 17:00:00 +0800",
    





    "snippet": "引言在Win10下，通过在Anaconda3安装Pytorch，版本号为1.8.1。需要使用NVIDIA显卡，安装CUDA 11.1 和cuDNN。具体包含Anaconda3下载安装，同时给了常见出错问题急解决方法。Anaconda3安装我是在 anaconda3 官方网站上下载的，然后从上往下下载最新的版本。下载好之后要注意安装问题，首先所针对用户，我选择的是针对该电脑的全部用户  Just Me  For all users之后是环境变量勾选，我没有勾选，系统推荐也是不勾选，之后再系统环境变量中添加就行，官方给的理由是会干扰到其他软件，没添加的话就没办法在cmd中运行。安装完成之后可以在这个博客完成镜像源的切换，在C:\\Users\\User_name\\.condarc文件中更改，例如，清华源可以channels:  - defaultsshow_channel_urls: truechannel_alias: https://mirrors.tuna.tsinghua.edu.cn/anacondadefault_channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels:  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud我用清华源的时候出现各种问题，头大，删了anaconda重装后没配置.condarc文件，才顺利，源问题还是看其它blog吧。创建conda环境在对应anaconda目录下启动终端conda create -n py38 python=3.8然后conda activate py38进入环境。。CUDA下载链接，cuDNN下载链接将cuDNN下载后的压缩包解压，将bin、include、lib文件夹复制到CUDA对应目录。不会产生覆盖，因为是新文件。安装pytorch安装完CUDA和cuDNN后再切入py38环境，输入conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia -c conda-forge因为是11.1版本的CUDA，官方文档上说要加上conda-forge。如果网速不稳定，或者镜像源不稳定，安装时可能会出现CondaHTTPError，重新下载几次也不行，就用迅雷下载，再进行安装。根据镜像源提供的网址，将未下载成功的文件离线下载，之后移动到Anaconda3安装目录的pkgs文件夹里，替换文件。然后打开pkgs文件里一个url命名的txt文本文件，将之前复制的下载链接复制到该txt中。virtualenv虚拟环境，创建不同版本的python虚拟环境"
  },
  
  {
    "title": "Jekyll搭建Gitpages",
    "url": "/posts/Jekyll%E6%90%AD%E5%BB%BAgitpages/",
    "categories": "2021, 05",
    "tags": "tutorial",
    "date": "2021-05-04 21:20:30 +0800",
    





    "snippet": "GitPages + Jekyll我使用的是Jekyll-theme-chirpy模板，直接拿来。因为有Git的基础，用起来还是比较方便的。另外，gitpages本身的主题比较少而且不是很好看，所以用了第三方的，第三方还是很多的。这个模板需要注意的就是，要另外创建新branch，注意./github/workflows/pages-deply.yml、tools/deploy.sh文件的branch名，还有基础配置文件_config.yml的更改。Markdown基础在写blog的时候，也学了一点markdown基础。这是加粗，斜体，链接百度，复选框  复选插入代码int x，插入代码块for(int x;x&amp;lt;n;x++){	printf(&quot;%d&quot;,x);}插入图片  一  二"
  }
  
]

